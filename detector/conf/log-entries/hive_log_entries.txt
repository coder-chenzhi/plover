<org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions: org.apache.hadoop.hive.ql.exec.Task tasksForAddPartition(org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.ql.plan.AddPartitionDesc,org.apache.hadoop.hive.ql.exec.Task)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listMRoles(java.lang.String,org.apache.hadoop.hive.metastore.api.PrincipalType)>
<org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeHashAggregate: void doProcessBatch(org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch,boolean,boolean[])>
<org.apache.hadoop.hive.ql.exec.FunctionTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>
<org.apache.hadoop.hive.llap.log.LlapWrappedAppender: void append(org.apache.logging.log4j.core.LogEvent)>
<org.apache.hadoop.hive.llap.daemon.impl.AMReporter: void unregisterTask(java.lang.String,int,org.apache.hadoop.hive.llap.daemon.impl.QueryIdentifier,org.apache.tez.dag.records.TezTaskAttemptID)>
<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator: java.util.List generateSplitsFromPpd(org.apache.hadoop.hive.metastore.Metastore$SplitInfos)>
<org.apache.hadoop.hive.metastore.security.DBTokenStore: boolean addToken(org.apache.hadoop.hive.metastore.security.DelegationTokenIdentifier,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation)>
<org.apache.hadoop.hive.metastore.txn.AcidCompactionHistoryService: void run()>
<org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler: void cleanEmptyAbortedTxns()>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genFilterPlan(org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.exec.Operator,boolean)>
<org.apache.hadoop.hive.metastore.ObjectStore: void initializeHelper(java.util.Properties)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listTableNamesByFilter(java.lang.String,java.lang.String,java.lang.String,short)>
<org.apache.hadoop.hive.metastore.cache.SharedCache: void refreshTablesInCache(java.lang.String,java.lang.String,java.util.List)>
<org.apache.hadoop.hive.accumulo.mr.AccumuloIndexedOutputFormat$AccumuloRecordWriter: int printMutation(org.apache.hadoop.io.Text,org.apache.accumulo.core.data.Mutation)>
<org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader: void setDone()>
<org.apache.hadoop.hive.metastore.ObjectStore: org.apache.hadoop.hive.metastore.api.Table markPartitionForEvent(java.lang.String,java.lang.String,java.lang.String,java.util.Map,org.apache.hadoop.hive.metastore.api.PartitionEventType)>
<org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler$Shuffle: void verifyRequest(java.lang.String,org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.handler.codec.http.HttpRequest,org.jboss.netty.handler.codec.http.HttpResponse,java.net.URL)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: boolean checkIfTxnsCommitted(java.util.List,java.sql.Statement,java.lang.StringBuilder)>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genUDTFPlan(org.apache.hadoop.hive.ql.udf.generic.GenericUDTF,java.lang.String,java.util.ArrayList,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator,boolean)>
<org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: void initIOContext(long,boolean,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl: void unlockBuffer(org.apache.hadoop.hive.llap.cache.LlapDataBuffer,boolean)>
<org.apache.hadoop.hive.ql.exec.Operator: void initializeChildren(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path[] getMmDirectoryCandidatesRecursive(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int,org.apache.hadoop.fs.PathFilter)>
<org.apache.hadoop.hive.ql.optimizer.physical.NullScanTaskDispatcher: java.lang.Object dispatch(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,java.lang.Object[])>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.SessionHandle openSession(java.lang.String,java.lang.String,java.util.Map)>
<org.apache.hadoop.hive.metastore.MetaStoreDirectSql: java.util.List getDefaultConstraints(java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.metastore.cache.SharedCache$TableWrapper: void refreshPartitionColStats(java.util.List)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listAllMTableGrants(java.lang.String,org.apache.hadoop.hive.metastore.api.PrincipalType,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: void ensureAllTxnsValid(java.lang.String,java.lang.String,java.util.List,java.sql.Statement)>
<org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveIntersectRewriteRule: void onMatch(org.apache.calcite.plan.RelOptRuleCall)>
<org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask: void initializeOperators(java.util.Map)>
<org.apache.hadoop.hive.druid.DruidStorageHandler: void commitInsertTable(org.apache.hadoop.hive.metastore.api.Table,boolean)>
<org.apache.hadoop.hive.ql.plan.mapper.StatsSources: java.util.Map extractStatMapFromPlanMapper(org.apache.hadoop.hive.ql.plan.mapper.PlanMapper)>
<org.apache.hadoop.hive.ql.exec.FileSinkOperator: void initializeOp(org.apache.hadoop.conf.Configuration)>
<org.apache.hive.service.ServiceUtils: void cleanup(org.slf4j.Logger,java.io.Closeable[])>
<org.apache.hadoop.hive.ql.exec.tez.DagUtils: void addCredentials(org.apache.hadoop.hive.ql.plan.MapWork,org.apache.tez.dag.api.DAG)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listPrincipalMPartitionGrants(java.lang.String,org.apache.hadoop.hive.metastore.api.PrincipalType,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory: void updateStats(org.apache.hadoop.hive.ql.plan.Statistics,long,boolean,org.apache.hadoop.hive.ql.exec.Operator,boolean)>
<org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles: void generateActualTasks(org.apache.hadoop.hive.conf.HiveConf,java.util.List,long,long,org.apache.hadoop.hive.ql.exec.Task,org.apache.hadoop.hive.ql.exec.Task,org.apache.hadoop.hive.ql.exec.Task,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles$ConditionalResolverMergeFilesCtx,org.apache.hadoop.hive.ql.plan.MapWork,int)>
<org.apache.hadoop.hive.ql.metadata.Hive: void loadTable(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.hive.ql.plan.LoadTableDesc$LoadFileType,boolean,boolean,boolean,boolean,java.lang.Long,int,boolean)>
<org.apache.hadoop.hive.ql.util.HiveStrictManagedMigration: boolean migrateToExternalTable(org.apache.hadoop.hive.metastore.api.Table,org.apache.hadoop.hive.metastore.TableType)>
<org.apache.hadoop.hive.metastore.AggregateStatsCache: void evictOneNode()>
<org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction: void validateNoHavingReferenceToAlias(org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.parse.ASTNode)>
<org.apache.hadoop.hive.metastore.columnstats.merge.DoubleColumnStatsMerger: void merge(org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj,org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj)>
<org.apache.hadoop.hive.ql.exec.ReduceSinkOperator: int computeHashCode(java.lang.Object,int)>
<org.apache.hadoop.hive.metastore.ObjectStore: void removeUnusedColumnDescriptor(org.apache.hadoop.hive.metastore.model.MColumnDescriptor)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterMultiKeyOperator: void process(java.lang.Object,int)>
<org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader: boolean determineSplitIncludes(org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl$StripeData,boolean[],boolean[])>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List getSchemaVersionsByColumns(java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher: java.lang.Object dispatch(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,java.lang.Object[])>
<org.apache.hadoop.hive.metastore.ReplChangeManager: java.lang.String encodeFileUri(java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.accumulo.HiveAccumuloHelper: void setOutputFormatMockInstance(org.apache.hadoop.mapred.JobConf,java.lang.String)>
<org.apache.hadoop.hive.ql.parse.repl.load.message.CreateFunctionHandler: java.util.List handle(org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandler$Context)>
<org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$OriginalReaderPair: boolean nextFromCurrentFile(org.apache.hadoop.hive.ql.io.orc.OrcStruct)>
<org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat: org.apache.hadoop.mapred.RecordReader getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter)>
<org.apache.hadoop.hive.ql.Driver: void recordValidTxns(org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager)>
<org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler: void revokeTimedoutWorkers(long)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: org.apache.hadoop.hive.metastore.api.LockResponse checkLock(java.sql.Connection,long)>
<org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils: void createMRWorkForMergingFiles(org.apache.hadoop.hive.ql.exec.FileSinkOperator,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.exec.DependencyCollectionTask,java.util.List,org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.exec.Task,org.apache.hadoop.hive.ql.session.LineageState)>
<org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$LimitStatsRule: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.metastore.ObjectStore: int addMasterKey(java.lang.String)>
<org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader: org.apache.hadoop.hive.llap.io.api.impl.ColumnVectorBatch nextCvb()>
<org.apache.hadoop.hive.ql.io.CombineHiveRecordReader: org.apache.hadoop.hive.ql.plan.PartitionDesc extractSinglePartSpec(org.apache.hadoop.hive.ql.io.CombineHiveInputFormat$CombineHiveInputSplit)>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genPlan(org.apache.hadoop.hive.ql.parse.QB,boolean)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashTable: void expandAndRehash()>
<org.apache.hadoop.hive.ql.exec.Utilities: void mvFileToFinalPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean,org.slf4j.Logger,org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx,org.apache.hadoop.hive.ql.plan.FileSinkDesc,org.apache.hadoop.mapred.Reporter)>
<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void applyGroupAndPerms(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,boolean)>
<org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader: void logProcessOneSlice(int,java.lang.Object,org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl$StripeData)>
<org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl: void getOverlappingRanges(long,org.apache.hadoop.hive.common.io.DiskRangeList,java.util.concurrent.ConcurrentSkipListMap,org.apache.hadoop.hive.common.io.DataCache$DiskRangeListFactory,org.apache.hadoop.hive.common.io.DataCache$BooleanRef)>
<org.apache.hadoop.hive.io.HdfsUtils$HadoopFileStatus: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.hive.druid.serde.DruidSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>
<org.apache.hadoop.hive.ql.parse.ParseUtils: void processSetColsNode(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.CalcitePlanner$ASTSearcher)>
<org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler$Shuffle$3: void onRemoval(com.google.common.cache.RemovalNotification)>
<org.apache.hadoop.hive.ql.io.AcidUtils$OrcAcidVersion: int getAcidVersionFromMetaFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)>
<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: boolean prepareImport(boolean,boolean,boolean,boolean,boolean,java.lang.String,java.lang.String,java.lang.String,java.util.LinkedHashMap,java.lang.String,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext,org.apache.hadoop.hive.ql.parse.repl.load.UpdatedMetaDataTracker,org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager)>
<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: java.util.List generateSplitsInfo(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context)>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.GetInfoValue getInfo(org.apache.hive.service.cli.SessionHandle,org.apache.hive.service.cli.GetInfoType)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator: void closeOp(boolean)>
<org.apache.hadoop.hive.ql.exec.tez.TezTask: org.apache.tez.dag.api.DAG build(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.Context,java.util.Map)>
<org.apache.hadoop.hive.metastore.cache.SharedCache$TableWrapper: void refreshTableColStats(java.util.List)>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.JobProgressUpdate progressUpdateLog(boolean,org.apache.hive.service.cli.operation.Operation,org.apache.hadoop.hive.conf.HiveConf)>
<org.apache.hadoop.hive.metastore.utils.MetaStoreUtils: java.util.List aggrPartitionStats(java.util.Map,java.util.List,boolean,boolean,double)>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.OperationHandle getFunctions(org.apache.hive.service.cli.SessionHandle,java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator: void process(java.lang.Object,int)>
<org.apache.hadoop.hive.ql.parse.GenTezUtils: org.apache.hadoop.hive.ql.plan.ReduceWork createReduceWork(org.apache.hadoop.hive.ql.parse.GenTezProcContext,org.apache.hadoop.hive.ql.exec.Operator,org.apache.hadoop.hive.ql.plan.TezWork)>
<org.apache.hadoop.hive.ql.parse.repl.dump.io.FileOperations: void copyMmPath()>
<org.apache.hadoop.hive.ql.log.PerfLogger: long PerfLogEnd(java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$DataWrapperForOrc: org.apache.hadoop.hive.common.io.DiskRangeList getFileData(java.lang.Object,org.apache.hadoop.hive.common.io.DiskRangeList,long,org.apache.hadoop.hive.common.io.DataCache$DiskRangeListFactory,org.apache.hadoop.hive.common.io.DataCache$BooleanRef)>
<org.apache.hadoop.hive.ql.io.HiveInputFormat: void processForWriteIds(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.common.ValidWriteIdList,boolean,java.util.List,java.util.List)>
<org.apache.hadoop.hive.ql.io.orc.OrcFileFormatProxy: org.apache.hadoop.hive.metastore.Metastore$SplitInfos applySargToMetadata(org.apache.hadoop.hive.ql.io.sarg.SearchArgument,java.nio.ByteBuffer)>
<org.apache.hadoop.hive.ql.parse.repl.load.message.TruncateTableHandler: java.util.List handle(org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandler$Context)>
<org.apache.hadoop.hive.serde2.lazy.LazyFloat: void init(org.apache.hadoop.hive.serde2.lazy.ByteArrayRef,int,int)>
<org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter$HeartbeatCallable: void maybeLogCounters()>
<org.apache.hadoop.hive.metastore.ObjectStore: java.lang.String createDbGuidAndPersist()>
<org.apache.hadoop.hive.metastore.cache.CachedStore: org.apache.hadoop.hive.metastore.api.AggrStats get_aggr_stats_for(java.lang.String,java.lang.String,java.lang.String,java.util.List,java.util.List)>
<org.apache.hadoop.hive.ql.ppd.OpProcFactory$DefaultPPD: void logExpr(org.apache.hadoop.hive.ql.lib.Node,org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo)>
<org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator$EntityTracker: void registerContainer(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.String,int)>
<org.apache.hive.spark.client.RemoteDriver$DriverProtocol: void jobFinished(java.lang.String,java.io.Serializable,java.lang.Throwable,org.apache.hive.spark.counter.SparkCounters)>
<org.apache.hadoop.hive.ql.io.orc.LocalCache: void getAndValidate(java.util.List,boolean,org.apache.orc.impl.OrcTail[],java.nio.ByteBuffer[])>
<org.apache.hadoop.hive.ql.parse.TezCompiler: long getCombinedKeyDomainCardinality(org.apache.hadoop.hive.ql.plan.ColStatistics,org.apache.hadoop.hive.ql.plan.ColStatistics,org.apache.hadoop.hive.ql.plan.ColStatistics)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listTableAllPartitionColumnGrants(java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger: void <init>(org.apache.hadoop.conf.Configuration,boolean,org.apache.hadoop.hive.ql.io.orc.Reader,boolean,int,org.apache.hadoop.hive.common.ValidWriteIdList,org.apache.orc.Reader$Options,org.apache.hadoop.fs.Path[],org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$Options)>
<org.apache.hadoop.hive.llap.ext.LlapTaskUmbilicalExternalClient$LlapTaskUmbilicalExternalImpl: void nodeHeartbeat(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,int,org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol$TezAttemptArray,org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol$BooleanArray)>
<org.apache.hadoop.hive.ql.parse.ParseDriver: org.apache.hadoop.hive.ql.parse.ASTNode parseSelect(java.lang.String,org.apache.hadoop.hive.ql.Context)>
<org.apache.hive.service.cli.thrift.ThriftHttpServlet: java.lang.String validateCookie(javax.servlet.http.HttpServletRequest)>
<org.apache.hadoop.hive.metastore.ObjectStore: org.apache.hadoop.hive.metastore.api.Catalog getCatalog(java.lang.String)>
<org.apache.hadoop.hive.metastore.MetaStoreDirectSql: org.apache.hadoop.hive.metastore.api.Database getDatabase(java.lang.String,java.lang.String)>
<org.apache.hive.spark.client.rpc.RpcDispatcher: void registerRpc(long,io.netty.util.concurrent.Promise,java.lang.String)>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genFileSinkPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator)>
<org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable: void setupMDCFromNDC(java.util.concurrent.Callable)>
<org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter$HeartbeatCallable: boolean taskSucceeded(org.apache.tez.dag.records.TezTaskAttemptID)>
<org.apache.hadoop.hive.ql.parse.CalcitePlanner: java.lang.String fixCtasColumnName(java.lang.String)>
<org.apache.hadoop.hive.ql.exec.FileSinkOperator: void initializeSpecPath()>
<org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction: org.apache.calcite.rel.RelNode genUDTFPlan(org.apache.hadoop.hive.ql.udf.generic.GenericUDTF,java.lang.String,java.lang.String,java.util.ArrayList,org.apache.hadoop.hive.ql.parse.QB,java.util.List,org.apache.hadoop.hive.ql.parse.RowResolver,org.apache.calcite.rel.RelNode)>
<org.apache.hadoop.hive.metastore.ObjectStore$GetHelper: java.lang.Object commit()>
<org.apache.hadoop.hive.serde2.avro.AvroSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>
<org.apache.hadoop.hive.ql.io.orc.OrcNewInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
<org.apache.hadoop.hive.ql.metadata.Hive: boolean deleteTableColumnStatistics(java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.llap.daemon.impl.QueryFragmentInfo: boolean canFinish()>
<org.apache.hadoop.hive.llap.io.encoded.VectorDeserializeOrcWriter: void <init>(org.apache.hadoop.conf.Configuration,java.util.Properties,org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector,java.util.List,boolean[],int)>
<org.apache.hadoop.hive.ql.exec.persistence.FlatRowContainer: void addRow(java.lang.Object[])>
<org.apache.hive.hcatalog.mapreduce.MultiOutputFormat: void checkOutputSpecs(org.apache.hadoop.mapreduce.JobContext)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator: void allocateOverflowBatchColumnVector(org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch,int,java.lang.String)>
<org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor: void initializeMapRecordSources()>
<org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer$SortedDynamicPartitionProc: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.ql.exec.tez.TezTask: void setAccessControlsForCurrentUser(org.apache.tez.dag.api.DAG,java.lang.String,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.metastore.tools.MetastoreSchemaTool$CommandBuilder: void logScript()>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: boolean heartbeatLockMaterializationRebuild(java.lang.String,java.lang.String,long)>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genSelectPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator,org.apache.hadoop.hive.ql.exec.Operator,boolean)>
<org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler: java.util.List findColumnsWithStats(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>
<org.apache.hadoop.hive.druid.DruidStorageHandler: boolean lambda$checkLoadStatus$12(java.net.URL)>
<org.apache.hadoop.hive.llap.LlapRowRecordReader: boolean next(org.apache.hadoop.io.NullWritable,org.apache.hadoop.hive.llap.Row)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: org.apache.hadoop.hive.metastore.txn.TxnHandler$ConnectionLockIdPair enqueueLockWithRetry(org.apache.hadoop.hive.metastore.api.LockRequest)>
<org.apache.hadoop.hive.metastore.ObjectStore: long cleanupEvents()>
<org.apache.hadoop.hive.ql.parse.repl.load.message.AbortTxnHandler: java.util.List handle(org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandler$Context)>
<org.apache.hadoop.hive.ql.stats.StatsUpdaterThread: boolean runOneWorkerIteration(org.apache.hadoop.hive.ql.session.SessionState,java.lang.String,org.apache.hadoop.hive.conf.HiveConf,boolean)>
<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorInputFormat: void addFileToMap(java.util.regex.Matcher,org.apache.hadoop.fs.Path,boolean,java.util.Map)>
<org.apache.hive.hcatalog.common.HiveClientCache: void <init>(int,int,int,boolean)>
<org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler: java.util.Set findPotentialCompactions(int)>
<org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader: java.lang.Boolean readFileWithCache(long)>
<org.apache.hive.hcatalog.mapreduce.PartInfo: void dedupWithTableInfo()>
<org.apache.hadoop.hive.ql.io.HiveInputFormat: org.apache.hadoop.mapred.InputFormat wrapForLlap(org.apache.hadoop.mapred.InputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.PartitionDesc)>
<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR: void run(org.apache.hadoop.hive.conf.HiveConf,java.lang.String,org.apache.hadoop.hive.metastore.api.Table,org.apache.hadoop.hive.metastore.api.Partition,org.apache.hadoop.hive.metastore.api.StorageDescriptor,org.apache.hadoop.hive.common.ValidWriteIdList,org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.ql.txn.compactor.Worker$StatsUpdater,org.apache.hadoop.hive.metastore.txn.TxnStore)>
<org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor: void setLlapOfFragmentId(org.apache.tez.runtime.api.ProcessorContext)>
<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionManagerImpl: org.apache.hadoop.hive.ql.exec.spark.session.SparkSession getSession(org.apache.hadoop.hive.ql.exec.spark.session.SparkSession,org.apache.hadoop.hive.conf.HiveConf,boolean)>
<org.apache.hadoop.hive.ql.parse.spark.SparkCompiler: void optimizeTaskPlan(java.util.List,org.apache.hadoop.hive.ql.parse.ParseContext,org.apache.hadoop.hive.ql.Context)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listTableGrantsAll(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.ql.ppd.OpProcFactory$ScriptPPD: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead: void logExceptionMessage(byte[],int,int,java.lang.String)>
<org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader: org.apache.hadoop.hive.ql.plan.MapWork findMapWork(org.apache.hadoop.mapred.JobConf)>
<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean closeConnection(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>
<org.apache.hadoop.hive.ql.exec.AppMasterEventOperator: void process(java.lang.Object,int)>
<org.apache.hadoop.hive.metastore.tools.MetastoreSchemaTool: boolean validateSchemaTables(java.sql.Connection)>
<org.apache.hadoop.hive.metastore.cache.SharedCache$TableWrapper: boolean cachePartitions(java.util.List,org.apache.hadoop.hive.metastore.cache.SharedCache)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: org.apache.hadoop.hive.metastore.txn.TxnStore$MutexAPI$LockHandle acquireLock(java.lang.String)>
<org.apache.hadoop.hive.ql.parse.TezCompiler: double computeBloomFilterNetBenefit(org.apache.hadoop.hive.ql.exec.SelectOperator,org.apache.hadoop.hive.ql.plan.ExprNodeDesc,org.apache.hadoop.hive.ql.exec.FilterOperator,org.apache.hadoop.hive.ql.plan.ExprNodeDesc)>
<org.apache.hadoop.hive.ql.Driver: void releaseFetchTask()>
<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>
<org.apache.hadoop.hive.ql.parse.repl.dump.events.AddUniqueConstraintHandler: void handle(org.apache.hadoop.hive.ql.parse.repl.dump.events.EventHandler$Context)>
<org.apache.hadoop.hive.ql.exec.MaterializedViewTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>
<org.apache.hive.service.cli.thrift.ThriftHttpServlet: void doPost(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>
<org.apache.hadoop.hive.ql.exec.tez.WorkloadManager: int transferSessionsToDestroy(java.util.Collection,java.util.List,int)>
<org.apache.hadoop.hive.serde2.avro.AvroLazyObjectInspector: java.lang.Object deserializeStruct(java.lang.Object,java.lang.String)>
<org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.util.PathInfo: org.apache.hadoop.fs.Path computeStagingDir(org.apache.hadoop.fs.Path)>
<org.apache.hadoop.hive.ql.lockmgr.DbTxnManager: void rollbackTxn()>
<org.apache.hadoop.hive.ql.cache.results.QueryResultsCache$CacheEntry: boolean addReader()>
<org.apache.hadoop.hive.ql.stats.BasicStatsTask: int aggregateStats(org.apache.hadoop.hive.ql.metadata.Hive)>
<org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge: org.apache.hadoop.security.UserGroupInformation getCurrentUGIWithConf(java.lang.String)>
<org.apache.hive.hcatalog.data.JsonSerDe: void populateRecord(java.util.List,org.codehaus.jackson.JsonToken,org.codehaus.jackson.JsonParser,org.apache.hive.hcatalog.data.schema.HCatSchema)>
<org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer: org.apache.hadoop.hive.ql.parse.ParseContext transform(org.apache.hadoop.hive.ql.parse.ParseContext)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.Properties getDataSourceProps(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.ql.lockmgr.DbLockManager: void unlock(org.apache.hadoop.hive.ql.lockmgr.HiveLock)>
<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: org.apache.hadoop.hive.ql.io.AcidInputFormat$RowReader getReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.hive.ql.io.AcidInputFormat$Options)>
<org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator$EntityTracker: void registerTaskAttempt(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.tez.dag.records.TezTaskAttemptID,java.lang.String,int)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: void replTableWriteIdState(org.apache.hadoop.hive.metastore.api.ReplTblWriteIdStateRequest)>
<org.apache.hadoop.hive.llap.io.ChunkedInputStream: int read(byte[],int,int)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listPrincipalAllPartitionColumnGrants(java.lang.String,org.apache.hadoop.hive.metastore.api.PrincipalType,org.apache.hadoop.hive.metastore.ObjectStore$QueryWrapper)>
<org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory: org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc typeCast(org.apache.hadoop.hive.ql.plan.ExprNodeDesc,org.apache.hadoop.hive.serde2.typeinfo.TypeInfo,boolean)>
<org.apache.hadoop.hive.ql.optimizer.PointLookupOptimizer$FilterTransformer: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.ql.exec.spark.SmallTableCache: void initialize(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.ql.plan.mapper.MetastoreStatsConnector: void logException(java.lang.String,java.lang.Exception)>
<org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService$NodeInfo: boolean canAcceptTask()>
<org.apache.hive.hcatalog.common.HiveClientCache$4: void run()>
<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController: void applyAuthorizationConfigPolicy(org.apache.hadoop.hive.conf.HiveConf)>
<org.apache.hadoop.hive.ql.exec.repl.ReplDumpWork: void overrideEventTo(org.apache.hadoop.hive.ql.metadata.Hive)>
<org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader: java.lang.Void performDataRead()>
<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: void createDefaultRoles_core()>
<org.apache.hadoop.hive.ql.exec.tez.DagUtils: org.apache.hadoop.fs.Path createTezDir(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.ql.exec.Utilities: boolean createDirsWithPermission(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)>
<org.apache.hive.beeline.HiveSchemaTool: void updateCatalogNameInTable(java.sql.Statement,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)>
<org.apache.hadoop.hive.llap.LlapOutputFormatService$LlapOutputFormatServiceHandler: void registerReader(io.netty.channel.ChannelHandlerContext,java.lang.String,byte[])>
<org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction: org.apache.calcite.rel.RelNode genLogicalPlan(org.apache.hadoop.hive.ql.parse.QB,boolean,com.google.common.collect.ImmutableMap,org.apache.hadoop.hive.ql.parse.RowResolver)>
<org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader: void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.conf.Configuration,org.apache.hive.druid.com.fasterxml.jackson.databind.ObjectMapper,org.apache.hive.druid.com.fasterxml.jackson.databind.ObjectMapper,org.apache.hive.druid.com.metamx.http.client.HttpClient)>
<org.apache.hadoop.hive.upgrade.acid.UpgradeTool: void makeConvertTableScript(java.util.List,java.util.List,java.lang.String)>
<org.apache.hadoop.hive.ql.metadata.JarUtils: java.lang.String getJar(java.lang.Class)>
<org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable: org.apache.tez.runtime.task.TaskRunner2Result callInternal()>
<org.apache.hadoop.hive.ql.lockmgr.DbTxnManager: void commitTxn()>
<org.apache.hadoop.hive.ql.exec.tez.TezProcessor$ReflectiveProgressHelper: void <init>(org.apache.hadoop.conf.Configuration,java.util.Map,org.apache.tez.runtime.api.ProcessorContext,java.lang.String)>
<org.apache.hadoop.hive.metastore.ReplChangeManager: java.lang.String getReplPolicyIdString(org.apache.hadoop.hive.metastore.api.Database)>
<org.apache.hadoop.hive.llap.shufflehandler.IndexCache: org.apache.hadoop.hive.llap.shufflehandler.IndexCache$IndexInformation readIndexFileToCache(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.ql.metadata.Hive: java.util.List getTableColumnStatistics(java.lang.String,java.lang.String,java.util.List)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerMultiKeyOperator: void process(java.lang.Object,int)>
<org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner: org.apache.hadoop.hive.ql.parse.PrunedPartitionList prune(org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.ql.plan.ExprNodeDesc,org.apache.hadoop.hive.conf.HiveConf,java.lang.String,java.util.Map)>
<org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl: void mergeStripeInfos(org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl$StripeData,org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl$StripeData)>
<org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider$LlapDecisionDispatcher: boolean checkExpression(org.apache.hadoop.hive.ql.plan.ExprNodeDesc)>
<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void checkTargetLocationEmpty(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.slf4j.Logger)>
<org.apache.hadoop.hive.ql.ppd.OpProcFactory: org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc pushFilterToStorageHandler(org.apache.hadoop.hive.ql.exec.TableScanOperator,org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc,org.apache.hadoop.hive.ql.ppd.OpWalkerInfo,org.apache.hadoop.hive.conf.HiveConf)>
<org.apache.hadoop.hive.ql.optimizer.physical.AnnotateRunTimeStatsOptimizer: void setRuntimeStatsDir(org.apache.hadoop.hive.ql.exec.Operator,org.apache.hadoop.hive.ql.parse.ParseContext)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listMSecurityPrincipalMembershipRole(java.lang.String,org.apache.hadoop.hive.metastore.api.PrincipalType,org.apache.hadoop.hive.metastore.ObjectStore$QueryWrapper)>
<org.apache.hadoop.hive.metastore.tools.MetastoreSchemaTool: void runSqlLine(java.lang.String)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: org.apache.hadoop.hive.metastore.api.AllocateTableWriteIdsResponse allocateTableWriteIds(org.apache.hadoop.hive.metastore.api.AllocateTableWriteIdsRequest)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listPartitionsPsWithAuth(java.lang.String,java.lang.String,java.lang.String,java.util.List,short,java.lang.String,java.util.List)>
<org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory$ColumnPrunerReduceSinkProc: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.metastore.ObjectStore: java.lang.String makeQueryFilterString(java.lang.String,java.lang.String,org.apache.hadoop.hive.metastore.api.Table,org.apache.hadoop.hive.metastore.parser.ExpressionTree,java.util.Map,boolean)>
<org.apache.hadoop.hive.ql.util.HiveStrictManagedMigration: void checkAndSetFileOwnerPermissions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission,boolean,boolean)>
<org.apache.hadoop.hive.llap.cache.LlapAllocatorBuffer: int invalidate()>
<org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask: void prepareReturnValues(java.util.List,java.lang.String)>
<org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient: java.util.Map getTempTableColumnStatsForTable(java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator: void initializeOp(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.accumulo.HiveAccumuloHelper: void setOutputFormatConnectorInfo(org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.accumulo.core.client.security.tokens.AuthenticationToken)>
<org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils: org.apache.hadoop.hive.ql.exec.Task findMoveTaskForFsopOutput(java.util.List,org.apache.hadoop.fs.Path,boolean)>
<org.apache.hadoop.hive.llap.cache.SimpleBufferManager: void unlockBuffer(org.apache.hadoop.hive.llap.cache.LlapAllocatorBuffer)>
<org.apache.hadoop.hive.ql.metadata.Hive: com.google.common.collect.ImmutableMap dumpAndClearMetaCallTiming(java.lang.String)>
<org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService: boolean updateFragment(java.lang.String,boolean)>
<org.apache.hadoop.hive.metastore.ObjectStore: void dropCatalog(java.lang.String)>
<org.apache.hadoop.hive.common.JavaUtils: void closeClassLoader(java.lang.ClassLoader)>
<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: boolean isEventNotReplayed(java.util.Map,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.hive.ql.parse.repl.DumpType)>
<org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.JDBCAggregationPushDownRule: void onMatch(org.apache.calcite.plan.RelOptRuleCall)>
<org.apache.hadoop.hive.metastore.security.MemoryTokenStore: int addMasterKey(java.lang.String)>
<org.apache.hadoop.hive.ql.exec.DDLTask: int showTablesOrViews(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.ShowTablesDesc)>
<org.apache.hadoop.hive.serde2.lazy.LazyBinary: byte[] decodeIfNeeded(byte[])>
<org.apache.hadoop.hive.ql.exec.vector.VectorizationContext: org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression getVectorExpression(org.apache.hadoop.hive.ql.plan.ExprNodeDesc,org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor$Mode)>
<org.apache.hadoop.hive.ql.util.IncrementalObjectSizeEstimator: void addCollectionEstimator(java.util.HashMap,java.util.Deque,java.lang.reflect.Field,java.lang.Class,java.lang.Object)>
<org.apache.hadoop.hive.ql.exec.tez.TezProcessor$ReflectiveProgressHelper: void scheduleProgressTaskService(long,long)>
<org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler: void setHadoopJobId(java.lang.String,long)>
<org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl: org.apache.orc.impl.BufferChunk readLengthBytesFromSmallBuffers(org.apache.orc.impl.BufferChunk,long,int[],java.util.List,boolean,org.apache.hadoop.hive.ql.io.orc.encoded.IoTrace)>
<org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler: void revokeFromLocalWorkers(java.lang.String)>
<org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask: int executeInChildVM(org.apache.hadoop.hive.ql.DriverContext)>
<org.apache.hive.beeline.hs2connection.BeelineSiteParser: java.util.Properties getConnectionProperties(java.lang.String)>
<org.apache.hadoop.hive.llap.cache.LlapAllocatorBuffer: java.lang.Boolean cancelDiscard()>
<org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl: org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl$FileData getFileData(java.lang.Object,long,long,boolean[],org.apache.hadoop.hive.common.io.DataCache$DiskRangeListFactory,org.apache.hadoop.hive.llap.cache.LowLevelCacheCounters,org.apache.hadoop.hive.common.io.DataCache$BooleanRef)>
<org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader: void stop()>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listAllTableGrants(java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: void heartbeatTxn(java.sql.Connection,long)>
<org.apache.hadoop.hive.metastore.utils.MetaStoreUtils: java.util.List aggrPartitionStats(java.util.List,java.lang.String,java.lang.String,java.lang.String,java.util.List,java.util.List,boolean,boolean,double)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List getPartitionNamesByFilter(java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean,long)>
<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.fs.Path extractNonDpMmDir(java.lang.Long,int,org.apache.hadoop.fs.FileStatus[],boolean)>
<org.apache.hive.jdbc.Utils: org.apache.hive.jdbc.Utils$JdbcConnectionParams extractURLComponents(java.lang.String,java.util.Properties)>
<org.apache.hadoop.hive.hbase.HiveHBaseInputFormatUtil: void setupKeyRange(org.apache.hadoop.hbase.client.Scan,java.util.List,boolean)>
<org.apache.hive.spark.client.SparkClientImpl: void stop()>
<org.apache.hadoop.hive.druid.security.DruidKerberosUtil: java.lang.String kerberosChallenge(java.lang.String)>
<org.apache.hadoop.hive.ql.stats.BasicStatsNoJobTask: int updatePartitions(org.apache.hadoop.hive.ql.metadata.Hive,java.util.List,org.apache.hadoop.hive.ql.metadata.Table)>
<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils: java.util.Set getPrivilegesFromFS(java.lang.String,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,boolean)>
<org.apache.hadoop.hive.ql.lockmgr.DbTxnManager: org.apache.hadoop.hive.metastore.api.LockResponse acquireMaterializationRebuildLock(java.lang.String,java.lang.String,long)>
<org.apache.hadoop.hive.serde2.lazy.LazyPrimitive: void logExceptionMessage(org.apache.hadoop.hive.serde2.lazy.ByteArrayRef,int,int,java.lang.String)>
<org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils: org.apache.hadoop.hive.ql.plan.ReduceWork createReduceWork(org.apache.hadoop.hive.ql.parse.spark.GenSparkProcContext,org.apache.hadoop.hive.ql.exec.Operator,org.apache.hadoop.hive.ql.plan.SparkWork)>
<org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy: org.apache.hadoop.hive.llap.cache.LlapCacheableBuffer evictHeapElementUnderLock(long,int)>
<org.apache.hadoop.hive.ql.Context: void removeScratchDir()>
<org.apache.hadoop.hive.schshim.FairSchedulerShim: void refreshDefaultQueue(org.apache.hadoop.conf.Configuration,java.lang.String)>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.TableSchema getResultSetMetadata(org.apache.hive.service.cli.OperationHandle)>
<org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.JDBCAbstractSplitFilterRule: boolean matches(org.apache.calcite.plan.RelOptRuleCall,org.apache.calcite.sql.SqlDialect)>
<org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$DefaultStatsRule: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter: org.apache.hadoop.hive.ql.exec.ReduceSinkOperator genReduceSink(org.apache.hadoop.hive.ql.exec.Operator,java.lang.String,org.apache.hadoop.hive.ql.plan.ExprNodeDesc[],int,java.util.ArrayList,java.lang.String,java.lang.String,int,org.apache.hadoop.hive.ql.io.AcidUtils$Operation,org.apache.hadoop.hive.conf.HiveConf)>
<org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader: org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl$StripeData createSliceToCache(org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader$CacheWriter$CacheStripeData,org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl$StripeData)>
<org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils: org.apache.hadoop.hive.ql.plan.MapWork createMergeTask(org.apache.hadoop.hive.ql.plan.FileSinkDesc,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.hive.ql.CompilationOpContext)>
<org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable: void updateFileSystemCounters(java.util.List,java.util.concurrent.Callable)>
<org.apache.hadoop.hive.ql.optimizer.spark.SparkReduceSinkMapJoinProc: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionManagerImpl: void closeSession(org.apache.hadoop.hive.ql.exec.spark.session.SparkSession)>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genLimitPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator,int,int)>
<org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl: org.apache.hadoop.hive.common.io.DiskRangeList readEncodedStream(long,org.apache.hadoop.hive.common.io.DiskRangeList,long,long,org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch$ColumnStreamData,long,long,java.util.IdentityHashMap)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listPartitionAllColumnGrants(java.lang.String,java.lang.String,java.lang.String,java.util.List)>
<org.apache.hadoop.hive.ql.exec.Utilities: java.util.HashMap removeTempOrDuplicateFilesNonMm(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.FileSystem)>
<org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>
<org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory: void propagate(org.apache.hadoop.hive.ql.udf.generic.GenericUDF,java.util.List,org.apache.hadoop.hive.ql.exec.RowSchema,java.util.Map)>
<org.apache.hadoop.hive.ql.io.HiveInputFormat: boolean checkInputFormatForLlapEncode(org.apache.hadoop.conf.Configuration,java.lang.String)>
<org.apache.hadoop.hive.ql.txn.compactor.Cleaner: void removeFiles(java.lang.String,org.apache.hadoop.hive.common.ValidWriteIdList,org.apache.hadoop.hive.metastore.txn.CompactionInfo)>
<org.apache.hive.service.cli.thrift.ThriftHttpServlet: java.lang.String getDoAsQueryParam(java.lang.String)>
<org.apache.hadoop.hive.ql.optimizer.StatsOptimizer$MetaDataProcessor: java.lang.Long getRowCnt(org.apache.hadoop.hive.ql.parse.ParseContext,org.apache.hadoop.hive.ql.exec.TableScanOperator,org.apache.hadoop.hive.ql.metadata.Table)>
<org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer: org.apache.hadoop.hive.ql.exec.Operator validateAndVectorizeOperator(org.apache.hadoop.hive.ql.exec.Operator,org.apache.hadoop.hive.ql.exec.vector.VectorizationContext,boolean,boolean,org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorTaskColumnInfo)>
<org.apache.hadoop.hive.metastore.HiveClientCache$4: void run()>
<org.apache.hadoop.hive.metastore.MetaStoreDirectSql: java.util.Map getColAndPartNamesWithStats(java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hive.service.cli.CLIService: void closeOperation(org.apache.hive.service.cli.OperationHandle)>
<org.apache.hadoop.hive.llap.daemon.impl.AMReporter: org.apache.hadoop.hive.llap.daemon.impl.AMReporter$AMNodeInfo registerTask(java.lang.String,int,java.lang.String,org.apache.hadoop.security.token.Token,org.apache.hadoop.hive.llap.daemon.impl.QueryIdentifier,org.apache.tez.dag.records.TezTaskAttemptID,boolean)>
<org.apache.hadoop.hive.druid.security.RetryIfUnauthorizedResponseHandler: org.apache.hive.druid.com.metamx.http.client.response.ClientResponse handleResponse(org.apache.hive.druid.org.jboss.netty.handler.codec.http.HttpResponse)>
<org.apache.hadoop.hive.ql.parse.repl.load.message.CommitTxnHandler: java.util.List handle(org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandler$Context)>
<org.apache.hive.service.cli.thrift.ThriftHttpServlet: java.lang.String getClientNameFromCookie(javax.servlet.http.Cookie[])>
<org.apache.hadoop.hive.accumulo.serde.AccumuloRowSerializer: void writeWithLevel(org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector,java.lang.Object,org.apache.hadoop.hive.serde2.ByteStream$Output,org.apache.hadoop.hive.accumulo.columns.ColumnMapping,int)>
<org.apache.hadoop.hive.metastore.security.DBTokenStore: boolean removeToken(org.apache.hadoop.hive.metastore.security.DelegationTokenIdentifier)>
<org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider$LlapDecisionDispatcher: void handleWork(org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.hive.ql.plan.BaseWork)>
<org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory$ConstantPropagateStopProc: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.ql.exec.tez.ColumnarSplitSizeEstimator: long getEstimatedSize(org.apache.hadoop.mapred.InputSplit)>
<org.apache.hadoop.hive.ql.security.authorization.plugin.fallback.FallbackHiveAuthorizer: void applyAuthorizationConfigPolicy(org.apache.hadoop.hive.conf.HiveConf)>
<org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer: void addTablePartsOutputs(org.apache.hadoop.hive.ql.metadata.Table,java.util.List,boolean,boolean,org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.hooks.WriteEntity$WriteType)>
<org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator: void closeOp(boolean)>
<org.apache.hadoop.hive.llap.daemon.impl.QueryTracker: org.apache.hadoop.hive.llap.daemon.impl.QueryInfo queryComplete(org.apache.hadoop.hive.llap.daemon.impl.QueryIdentifier,long,boolean)>
<org.apache.hadoop.hive.llap.log.LlapWrappedAppender: void stop()>
<org.apache.hive.service.cli.session.SessionManager: void setProxyUserName(java.lang.String)>
<org.apache.hadoop.hive.metastore.utils.HdfsUtils: void checkFileAccess(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.security.UserGroupInformation)>
<org.apache.hadoop.hive.ql.exec.vector.VectorizationContext: org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression getVectorExpressionForUdf(org.apache.hadoop.hive.ql.udf.generic.GenericUDF,java.lang.Class,java.util.List,org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor$Mode,org.apache.hadoop.hive.serde2.typeinfo.TypeInfo)>
<org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager: void acquireLocks(org.apache.hadoop.hive.ql.QueryPlan,org.apache.hadoop.hive.ql.Context,java.lang.String,org.apache.hadoop.hive.ql.Driver$LockedDriverState)>
<org.apache.hadoop.hive.ql.metadata.Hive: void alterPartitionSpecInMemory(org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,org.apache.hadoop.hive.metastore.api.Partition,boolean,java.lang.String)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: long cleanupMaterializationRebuildLocks(org.apache.hadoop.hive.common.ValidTxnList,long)>
<org.apache.hadoop.hive.llap.daemon.impl.LlapTokenChecker: java.util.List getLlapTokens(org.apache.hadoop.security.UserGroupInformation,java.lang.String)>
<org.apache.hadoop.hive.ql.io.RCFile$Reader: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int,org.apache.hadoop.conf.Configuration,long,long)>
<org.apache.hadoop.hive.metastore.cache.SharedCache$TableWrapper: void refreshPartitions(java.util.List,org.apache.hadoop.hive.metastore.cache.SharedCache)>
<org.apache.hadoop.hive.ql.exec.DDLTask: boolean addIfAbsentByName(org.apache.hadoop.hive.ql.hooks.WriteEntity,java.util.Set)>
<org.apache.hadoop.hive.metastore.utils.MetaStoreUtils: java.lang.String getDDLFromFieldSchema(java.lang.String,java.util.List)>
<org.apache.hadoop.hive.ql.optimizer.PartitionColumnsSeparator$StructInTransformer: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hive.storage.jdbc.conf.JdbcStorageConfigManager: void resolveMetadata(java.util.Properties)>
<org.apache.hadoop.hive.llap.cache.LlapAllocatorBuffer: boolean startMoveOrDiscard(int,int,boolean)>
<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void createRegularImportTasks(org.apache.hadoop.hive.ql.plan.ImportTableDesc,java.util.List,boolean,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.metadata.Table,java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext,java.lang.Long,int)>
<org.apache.hive.service.cli.thrift.ThriftHttpCLIService: void initServer()>
<org.apache.hive.service.cli.thrift.ThriftCLIService: java.lang.String getProxyUser(java.lang.String,java.util.Map,java.lang.String)>
<org.apache.hadoop.hive.druid.io.DruidOutputFormat: org.apache.hadoop.hive.ql.exec.FileSinkOperator$RecordWriter getHiveRecordWriter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,boolean,java.util.Properties,org.apache.hadoop.util.Progressable)>
<org.apache.hadoop.hive.metastore.ObjectStore: void clearOutPmfClassLoaderCache(javax.jdo.PersistenceManagerFactory)>
<org.apache.hadoop.hive.ql.parse.repl.load.message.AddForeignKeyHandler: java.util.List handle(org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandler$Context)>
<org.apache.hadoop.hive.ql.parse.repl.dump.io.FileOperations: java.io.BufferedWriter writer()>
<org.apache.hadoop.hive.llap.ext.LlapTaskUmbilicalExternalClient$LlapTaskUmbilicalExternalImpl: org.apache.tez.runtime.api.impl.TezHeartbeatResponse heartbeat(org.apache.tez.runtime.api.impl.TezHeartbeatRequest)>
<org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable: void killTask()>
<org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader: boolean processOneFileSplit(org.apache.hadoop.mapred.FileSplit,long,org.apache.hive.common.util.Ref,org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl$StripeData)>
<org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdRowCount: java.lang.Double getRowCount(org.apache.calcite.rel.core.Join,org.apache.calcite.rel.metadata.RelMetadataQuery)>
<org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin: boolean checkConvertJoinSMBJoin(org.apache.hadoop.hive.ql.exec.JoinOperator,org.apache.hadoop.hive.ql.parse.OptimizeTezProcContext,int,org.apache.hadoop.hive.ql.optimizer.TezBucketJoinProcCtx)>
<org.apache.hadoop.hive.ql.udf.generic.BaseMaskUDF: org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector initialize(org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector[])>
<org.apache.hadoop.hive.common.StatsSetupConst: void clearColumnStatsState(java.util.Map)>
<org.apache.hive.hcatalog.data.HCatRecordObjectInspectorFactory: org.apache.hive.hcatalog.data.HCatRecordObjectInspector getHCatRecordObjectInspector(org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo)>
<org.apache.hadoop.hive.ql.parse.TezCompiler: void markSemiJoinForDPP(org.apache.hadoop.hive.ql.parse.OptimizeTezProcContext)>
<org.apache.hadoop.hive.cli.CliDriver: void <init>()>
<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void analyzeReplDump(org.apache.hadoop.hive.ql.parse.ASTNode)>
<org.apache.hadoop.hive.llap.cache.LlapAllocatorBuffer: int invalidateAndRelease()>
<org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader$IncludesImpl: void <init>(java.util.List,boolean,org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx,org.apache.orc.TypeDescription,org.apache.hadoop.mapred.JobConf)>
<org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.ql.parse.ParseDriver: org.apache.hadoop.hive.ql.parse.ASTNode parse(java.lang.String,org.apache.hadoop.hive.ql.Context,java.lang.String)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listRoleNames()>
<org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSemiJoinRule: void perform(org.apache.calcite.plan.RelOptRuleCall,org.apache.calcite.util.ImmutableBitSet,org.apache.calcite.rel.RelNode,org.apache.calcite.rel.core.Join,org.apache.calcite.rel.RelNode,org.apache.calcite.rel.core.Aggregate)>
<org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction: org.apache.calcite.rel.RelNode apply(org.apache.calcite.plan.RelOptCluster,org.apache.calcite.plan.RelOptSchema,org.apache.calcite.schema.SchemaPlus)>
<org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader: void processColumnCacheData(org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl$LlapSerDeDataBuffer[][][],org.apache.hadoop.hive.ql.io.orc.encoded.Reader$OrcEncodedColumnBatch,int)>
<org.apache.hadoop.hive.llap.io.ChunkedOutputStream: void close()>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: int numLocksInLockTable()>
<org.apache.hadoop.hive.metastore.MetaStoreDirectSql: java.util.List getCheckConstraints(java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listPrincipalAllDBGrant(java.lang.String,org.apache.hadoop.hive.metastore.api.PrincipalType,org.apache.hadoop.hive.metastore.ObjectStore$QueryWrapper)>
<org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$InternalCompletionListener: void onFailure(java.lang.Throwable)>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.OperationHandle executeStatement(org.apache.hive.service.cli.SessionHandle,java.lang.String,java.util.Map,long)>
<org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader$CacheOutputReceiver: void output(java.nio.ByteBuffer)>
<org.apache.hadoop.hive.ql.parse.repl.dump.events.AddPartitionHandler: void handle(org.apache.hadoop.hive.ql.parse.repl.dump.events.EventHandler$Context)>
<org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter: org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter$OpAttr visit(org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveSortLimit)>
<org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader: void returnData(org.apache.hadoop.hive.ql.io.orc.encoded.Reader$OrcEncodedColumnBatch)>
<org.apache.hadoop.hive.ql.parse.repl.load.message.AlterDatabaseHandler: java.util.List handle(org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandler$Context)>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.OperationHandle getCatalogs(org.apache.hive.service.cli.SessionHandle)>
<org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinResolver$SkewJoinTaskDispatcher: java.lang.Object dispatch(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,java.lang.Object[])>
<org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat: org.apache.hadoop.mapred.RecordReader getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: void addDynamicPartitions(org.apache.hadoop.hive.metastore.api.AddDynamicPartitions)>
<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorInputSplit: void readFields(java.io.DataInput)>
<org.apache.hadoop.hive.metastore.ObjectStore: org.apache.hadoop.hive.metastore.utils.ObjectPair getPartQueryWithParams(java.lang.String,java.lang.String,java.lang.String,java.util.List)>
<org.apache.hadoop.hive.ql.stats.StatsUtils: java.util.List getTableColumnStats(org.apache.hadoop.hive.ql.metadata.Table,java.util.List,java.util.List,org.apache.hadoop.hive.ql.parse.ColumnStatsList)>
<org.apache.hadoop.hive.ql.optimizer.TablePropertyEnrichmentOptimizer$Processor: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: org.apache.hadoop.hive.metastore.api.ShowLocksResponse showLocks(org.apache.hadoop.hive.metastore.api.ShowLocksRequest)>
<org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils: org.apache.avro.Schema getSchemaFromFS(java.lang.String,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.ql.cache.results.QueryResultsCache: boolean entryMatches(org.apache.hadoop.hive.ql.cache.results.QueryResultsCache$LookupInfo,org.apache.hadoop.hive.ql.cache.results.QueryResultsCache$CacheEntry,java.util.Set)>
<org.apache.hadoop.hive.ql.io.parquet.vector.BaseVectorizedColumnReader: void readPageV1(org.apache.parquet.column.page.DataPageV1)>
<org.apache.hadoop.hive.llap.LlapBaseInputFormat: void close(java.lang.String)>
<org.apache.hadoop.hive.ql.exec.tez.TezProcessor$ReflectiveProgressHelper: void shutDownProgressTaskService()>
<org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader: void <init>(org.apache.hadoop.hive.llap.cache.LowLevelCache,org.apache.hadoop.hive.llap.cache.BufferUsageManager,org.apache.hadoop.hive.llap.io.metadata.MetadataCache,org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit,org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer$Includes,org.apache.hadoop.hive.ql.io.sarg.SearchArgument,org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer,org.apache.hadoop.hive.llap.counters.QueryFragmentCounters,org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer$SchemaEvolutionFactory,org.apache.hadoop.hive.common.Pool)>
<org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader$CacheWriter: void finalizeStripe(org.apache.orc.OrcProto$StripeFooter$Builder,org.apache.orc.OrcProto$StripeInformation$Builder)>
<org.apache.hive.jdbc.logs.InPlaceUpdateStream$EventNotifier: void operationLogShowedToUser()>
<org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask: void dumpTable(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.hive.metastore.PartFilterExprUtil: org.apache.hadoop.hive.metastore.parser.ExpressionTree makeExpressionTree(java.lang.String)>
<org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>
<org.apache.hadoop.hive.ql.exec.Operator: void setStatistics(org.apache.hadoop.hive.ql.plan.Statistics)>
<org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl: void unlockBuffer(org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl$LlapSerDeDataBuffer,boolean)>
<org.apache.hadoop.hive.ql.exec.Utilities: void createEmptyBuckets(org.apache.hadoop.conf.Configuration,java.util.List,boolean,org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.Reporter)>
<org.apache.hadoop.hive.ql.exec.tez.HostAffinitySplitLocationProvider: java.lang.String[] getLocations(org.apache.hadoop.mapred.InputSplit)>
<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Task tableUpdateReplStateTask(java.lang.String,java.lang.String,java.util.Map,java.lang.String,org.apache.hadoop.hive.ql.exec.Task)>
<org.apache.hive.storage.jdbc.dao.GenericJdbcDatabaseAccessor: java.util.List getColumnNames(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$ColumnExprProcessor: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.metastore.ObjectStore: boolean removeMasterKey(java.lang.Integer)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listPrincipalTableGrantsAll(java.lang.String,org.apache.hadoop.hive.metastore.api.PrincipalType)>
<org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator: void fixTmpPath(org.apache.hadoop.fs.Path,int)>
<org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory: org.apache.orc.impl.TreeReaderFactory$TreeReader createEncodedTreeReader(org.apache.orc.TypeDescription,java.util.List,org.apache.hadoop.hive.ql.io.orc.encoded.Reader$OrcEncodedColumnBatch,org.apache.orc.CompressionCodec,org.apache.orc.impl.TreeReaderFactory$Context,boolean)>
<org.apache.hadoop.hive.metastore.columnstats.aggr.DoubleColumnStatsAggregator: org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj aggregate(java.util.List,java.util.List,boolean)>
<org.apache.hive.beeline.HiveSchemaTool: void createCatalog(java.lang.String,java.lang.String,java.lang.String,boolean)>
<org.apache.hadoop.hive.llap.cache.LlapAllocatorBuffer: int decRef()>
<org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelFieldTrimmer: void fetchColStats(org.apache.calcite.rel.RelNode,org.apache.calcite.rel.core.TableScan,org.apache.calcite.util.ImmutableBitSet,java.util.Set)>
<org.apache.hadoop.hive.metastore.HiveAlterHandler: void alterTable(org.apache.hadoop.hive.metastore.RawStore,org.apache.hadoop.hive.metastore.Warehouse,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.hive.metastore.api.Table,org.apache.hadoop.hive.metastore.api.EnvironmentContext,org.apache.hadoop.hive.metastore.IHMSHandler)>
<org.apache.hadoop.hive.llap.tezplugins.LlapContainerLauncher: void stopContainer(org.apache.tez.serviceplugins.api.ContainerStopRequest)>
<org.apache.hadoop.hive.ql.exec.tez.TezSessionPool: void replaceSession(org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolSession)>
<org.apache.hadoop.hive.ql.stats.StatsUtils$1: java.lang.Long call()>
<org.apache.hadoop.hive.ql.exec.DDLTask: void dropTable(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.ql.plan.DropTableDesc)>
<org.apache.hadoop.hive.ql.parse.repl.CopyUtils: boolean isSourceFileMismatch(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.metastore.ReplChangeManager$FileInfo)>
<org.apache.hadoop.hive.ql.exec.ReplTxnTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>
<org.apache.hadoop.hive.metastore.utils.MetastoreVersionInfo: void main(java.lang.String[])>
<org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkUniformHashOperator: void process(java.lang.Object,int)>
<org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator: void updatePaths(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator: boolean closeConnection(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>
<org.apache.hadoop.hive.llap.daemon.impl.AMReporter: void queryComplete(org.apache.hadoop.hive.llap.daemon.impl.QueryIdentifier)>
<org.apache.hadoop.hive.ql.stats.StatsUpdaterThread: java.util.List findPartitionsToAnalyze(org.apache.hadoop.hive.metastore.utils.MetaStoreUtils$FullTableName,java.lang.String,java.lang.String,java.lang.String,java.util.List,java.util.Map)>
<org.apache.hadoop.hive.ql.exec.tez.WorkloadManager: void processCurrentEvents(org.apache.hadoop.hive.ql.exec.tez.WorkloadManager$EventState,org.apache.hadoop.hive.ql.exec.tez.WorkloadManager$WmThreadSyncWork)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler$LockHandleImpl: void releaseLocks()>
<org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager: void releaseLocks(java.util.List)>
<org.apache.hadoop.hive.ql.exec.tez.RecordProcessor: void init(org.apache.tez.mapreduce.processor.MRTaskReporter,java.util.Map,java.util.Map)>
<org.apache.hadoop.hive.ql.metadata.Hive: boolean setPartitionColumnStatistics(org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest)>
<org.apache.hadoop.hive.llap.tezplugins.endpoint.LlapPluginSecurityInfo: org.apache.hadoop.security.KerberosInfo getKerberosInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction: org.apache.calcite.util.Pair genSelectLogicalPlan(org.apache.hadoop.hive.ql.parse.QB,org.apache.calcite.rel.RelNode,org.apache.calcite.rel.RelNode,com.google.common.collect.ImmutableMap,org.apache.hadoop.hive.ql.parse.RowResolver,boolean)>
<org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl: void readEncodedColumns(int,org.apache.orc.StripeInformation,org.apache.orc.OrcProto$RowIndex[],java.util.List,java.util.List,boolean[],boolean[],org.apache.hadoop.hive.ql.io.orc.encoded.Consumer)>
<org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader: void setError(java.lang.Throwable)>
<org.apache.hadoop.hive.metastore.conf.MetastoreConf: org.apache.hadoop.conf.Configuration newMetastoreConf()>
<org.apache.hadoop.hive.metastore.ObjectStore: boolean isPartitionMarkedForEvent(java.lang.String,java.lang.String,java.lang.String,java.util.Map,org.apache.hadoop.hive.metastore.api.PartitionEventType)>
<org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils: void setupReduceSink(org.apache.hadoop.hive.ql.parse.spark.GenSparkProcContext,org.apache.hadoop.hive.ql.plan.ReduceWork,org.apache.hadoop.hive.ql.exec.ReduceSinkOperator)>
<org.apache.hadoop.hive.ql.metadata.Hive: void walkDirTree(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.FileSystem,java.util.Map,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.metastore.api.SkewedInfo)>
<org.apache.hadoop.hive.metastore.cache.CachedStore$CacheUpdateMasterWork: void update()>
<org.apache.hadoop.hive.ql.stats.StatsUtils: void setUnknownRcDsToAverage(java.util.List,java.util.List,int)>
<org.apache.hadoop.hive.metastore.MetaStoreSchemaInfoFactory: org.apache.hadoop.hive.metastore.IMetaStoreSchemaInfo get(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.ql.optimizer.QueryPlanPostProcessor: void <init>(java.util.List,java.util.Set,java.lang.String)>
<org.apache.hadoop.hive.ql.parse.GenTezUtils: org.apache.hadoop.hive.ql.plan.MapWork createMapWork(org.apache.hadoop.hive.ql.parse.GenTezProcContext,org.apache.hadoop.hive.ql.exec.Operator,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.hive.ql.parse.PrunedPartitionList)>
<org.apache.hadoop.hive.ql.io.parquet.vector.BaseVectorizedColumnReader: void readPageV2(org.apache.parquet.column.page.DataPageV2)>
<org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader: void processAsyncCacheData(org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader$CacheWriter$CacheStripeData,boolean[])>
<org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin: void removeCycleCreatingSemiJoinOps(org.apache.hadoop.hive.ql.exec.MapJoinOperator,org.apache.hadoop.hive.ql.exec.Operator,org.apache.hadoop.hive.ql.parse.ParseContext)>
<org.apache.hadoop.hive.ql.optimizer.spark.SparkMapJoinOptimizer: long[] getMapJoinConversionInfo(org.apache.hadoop.hive.ql.exec.JoinOperator,org.apache.hadoop.hive.ql.parse.spark.OptimizeSparkProcContext)>
<org.apache.hadoop.hive.ql.exec.Utilities: boolean isEmptyPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.Context)>
<org.apache.hadoop.hive.common.log.LogRedirector: void run()>
<org.apache.hadoop.hive.metastore.RetryingMetaStoreClient: boolean hasConnectionLifeTimeReached(java.lang.reflect.Method)>
<org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction: org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction$AggInfo getHiveAggInfo(org.apache.hadoop.hive.ql.parse.ASTNode,int,org.apache.hadoop.hive.ql.parse.RowResolver)>
<org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService: org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService$SelectHostResult selectHost(org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService$TaskInfo)>
<org.apache.hadoop.hive.ql.metadata.Hive: void copyFiles(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,java.util.List,boolean,boolean)>
<org.apache.hadoop.hive.registry.impl.ZkRegistryBase: void addToCache(java.lang.String,java.lang.String,org.apache.hadoop.hive.registry.ServiceInstance)>
<org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader: boolean processOneSlice(org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader$Vectors,boolean[],int,org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl$StripeData,long)>
<org.apache.hadoop.hive.druid.security.KerberosHttpClient: void inner_go(org.apache.hive.druid.com.metamx.http.client.Request,org.apache.hive.druid.com.metamx.http.client.response.HttpResponseHandler,org.joda.time.Duration,org.apache.hive.druid.com.google.common.util.concurrent.SettableFuture)>
<org.apache.hadoop.hive.accumulo.mr.AccumuloIndexedOutputFormat$AccumuloRecordWriter: void close(org.apache.hadoop.mapred.Reporter)>
<org.apache.hadoop.hive.llap.cli.LlapStatusServiceDriver: org.apache.hadoop.hive.llap.cli.LlapStatusServiceDriver$ExitCode populateAppStatusFromLlapRegistry(org.apache.hadoop.hive.llap.cli.status.LlapStatusHelpers$AppStatusBuilder,long)>
<org.apache.hive.storage.jdbc.JdbcSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>
<org.apache.hadoop.hive.metastore.ReplChangeManager: java.lang.String[] decodeFileUri(java.lang.String)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator: void commonSetup(org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch)>
<org.apache.hadoop.hive.common.StatsSetupConst: void removeColumnStatsState(java.util.Map,java.util.List)>
<org.apache.hive.http.Log4j2ConfiguratorServlet: void configureLogger(org.apache.hive.http.Log4j2ConfiguratorServlet$ConfLoggers)>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.SessionHandle openSessionWithImpersonation(org.apache.hive.service.rpc.thrift.TProtocolVersion,java.lang.String,java.lang.String,java.util.Map,java.lang.String)>
<org.apache.hadoop.hive.common.CompressionUtils: java.util.List unTar(java.lang.String,java.lang.String,boolean)>
<org.apache.hadoop.hive.ql.exec.Utilities: java.util.List removeTempOrDuplicateFiles(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus[],java.lang.String,int,int,org.apache.hadoop.conf.Configuration,java.lang.Long,int,boolean,java.util.Set,boolean)>
<org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$FilterStatsRule: long evaluateExpression(org.apache.hadoop.hive.ql.plan.Statistics,org.apache.hadoop.hive.ql.plan.ExprNodeDesc,org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateStatsProcCtx,java.util.List,org.apache.hadoop.hive.ql.exec.Operator,long)>
<org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl: org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SubmitWorkResponseProto submitWork(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SubmitWorkRequestProto)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: org.apache.hadoop.hive.metastore.api.LockResponse checkLockWithRetry(java.sql.Connection,long,long)>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.SessionHandle openSession(org.apache.hive.service.rpc.thrift.TProtocolVersion,java.lang.String,java.lang.String,java.util.Map)>
<org.apache.hadoop.hive.metastore.conf.MetastoreConf: void lambda$newMetastoreConf$1(org.apache.hadoop.conf.Configuration,java.lang.String)>
<org.apache.hadoop.hive.llap.daemon.impl.QueryTracker: void handleFragmentCompleteExternalQuery(org.apache.hadoop.hive.llap.daemon.impl.QueryInfo)>
<org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl: void readIndexStreams(org.apache.orc.impl.OrcIndex,org.apache.orc.StripeInformation,java.util.List,boolean[],boolean[])>
<org.apache.hadoop.hive.contrib.serde2.s3.S3LogDeserializer: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>
<org.apache.hadoop.hive.metastore.ObjectStore: boolean removeToken(java.lang.String)>
<org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.TaskTracker: void debugLog(java.lang.String)>
<org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils: org.apache.hadoop.hive.ql.exec.ConditionalTask createCondTask(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.exec.Task,org.apache.hadoop.hive.ql.plan.MoveWork,java.io.Serializable,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.exec.Task,org.apache.hadoop.hive.ql.exec.DependencyCollectionTask,org.apache.hadoop.hive.ql.session.LineageState)>
<org.apache.hadoop.hive.ql.metadata.Hive: boolean deletePartitionColumnStatistics(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils: org.apache.hadoop.hive.ql.plan.MapWork createMapWork(org.apache.hadoop.hive.ql.parse.spark.GenSparkProcContext,org.apache.hadoop.hive.ql.exec.Operator,org.apache.hadoop.hive.ql.plan.SparkWork,org.apache.hadoop.hive.ql.parse.PrunedPartitionList,boolean)>
<org.apache.hadoop.hive.metastore.security.MemoryTokenStore: boolean addToken(org.apache.hadoop.hive.metastore.security.DelegationTokenIdentifier,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation)>
<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: void addAdminUsers_core()>
<org.apache.hive.service.auth.ldap.GroupFilterFactory$GroupMembershipKeyFilter: void apply(org.apache.hive.service.auth.ldap.DirSearch,java.lang.String)>
<org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkJoinDeDuplication$ReducerProc: void propagateMaxNumReducers(org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkJoinDeDuplication$ReduceSinkJoinDeDuplicateProcCtx,org.apache.hadoop.hive.ql.exec.ReduceSinkOperator,int)>
<org.apache.hadoop.hive.ql.parse.TezCompiler: double getBloomFilterBenefit(org.apache.hadoop.hive.ql.exec.SelectOperator,org.apache.hadoop.hive.ql.plan.ExprNodeDesc,org.apache.hadoop.hive.ql.exec.FilterOperator,org.apache.hadoop.hive.ql.plan.ExprNodeDesc)>
<org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveExceptRewriteRule: void onMatch(org.apache.calcite.plan.RelOptRuleCall)>
<org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$JoinStatsRule: long inferPKFKRelationship(int,java.util.List,org.apache.hadoop.hive.ql.exec.CommonJoinOperator)>
<org.apache.hadoop.hive.metastore.HiveAlterHandler: org.apache.hadoop.hive.metastore.api.Partition alterPartition(org.apache.hadoop.hive.metastore.RawStore,org.apache.hadoop.hive.metastore.Warehouse,java.lang.String,java.lang.String,java.lang.String,java.util.List,org.apache.hadoop.hive.metastore.api.Partition,org.apache.hadoop.hive.metastore.api.EnvironmentContext,org.apache.hadoop.hive.metastore.IHMSHandler)>
<org.apache.hadoop.hive.shims.Utils: javax.servlet.Filter getXSRFFilter()>
<org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer$FetchData$1: java.lang.Long call()>
<org.apache.hadoop.hive.ql.exec.spark.SparkTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>
<org.apache.hadoop.hive.ql.exec.tez.TezSessionPool: boolean returnSessionInternal(org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolSession,boolean)>
<org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService: void schedulePendingTasks()>
<org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin: boolean convertJoinDynamicPartitionedHashJoin(org.apache.hadoop.hive.ql.exec.JoinOperator,org.apache.hadoop.hive.ql.parse.OptimizeTezProcContext,long)>
<org.apache.hadoop.hive.ql.exec.mr.ExecReducer: void reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)>
<org.apache.hadoop.hive.ql.exec.MoveTask: void logMessage(org.apache.hadoop.hive.ql.plan.LoadTableDesc)>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.OperationHandle getTables(org.apache.hive.service.cli.SessionHandle,java.lang.String,java.lang.String,java.lang.String,java.util.List)>
<org.apache.hadoop.hive.ql.io.HiveInputFormat: org.apache.hadoop.mapred.RecordReader getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter)>
<org.apache.hadoop.hive.llap.io.ChunkedOutputStream: void writeChunk()>
<org.apache.hadoop.hive.ql.metadata.Hive: void cleanUpOneDirectoryForReplace(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,org.apache.hadoop.hive.conf.HiveConf,boolean,boolean)>
<org.apache.hadoop.hive.ql.cache.results.QueryResultsCache: void notifyTableChanged(java.lang.String,java.lang.String,long)>
<org.apache.hive.service.cli.operation.SQLOperation: org.apache.hadoop.hive.serde2.AbstractSerDe getSerDe()>
<org.apache.hadoop.hive.accumulo.HiveAccumuloHelper: void setInputFormatZooKeeperInstance(org.apache.hadoop.mapred.JobConf,java.lang.String,java.lang.String,boolean)>
<org.apache.hadoop.hive.llap.cli.LlapServiceDriver: void main(java.lang.String[])>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: void performTimeOuts()>
<org.apache.hadoop.hive.serde2.lazy.LazyHiveDecimal: void init(org.apache.hadoop.hive.serde2.lazy.ByteArrayRef,int,int)>
<org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction: org.apache.calcite.rel.RelNode genJoinLogicalPlan(org.apache.hadoop.hive.ql.parse.ASTNode,java.util.Map)>
<org.apache.hadoop.hive.ql.parse.GenTezUtils: void removeSemiJoinOperator(org.apache.hadoop.hive.ql.parse.ParseContext,org.apache.hadoop.hive.ql.exec.AppMasterEventOperator,org.apache.hadoop.hive.ql.exec.TableScanOperator)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listPrincipalAllPartitionGrants(java.lang.String,org.apache.hadoop.hive.metastore.api.PrincipalType,org.apache.hadoop.hive.metastore.ObjectStore$QueryWrapper)>
<org.apache.hadoop.hive.metastore.utils.FileUtils: boolean moveToTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>
<org.apache.hadoop.hive.ql.optimizer.calcite.rules.PartitionPrune$ExtractPartPruningPredicate: org.apache.calcite.rex.RexNode visitCall(org.apache.calcite.rex.RexCall)>
<org.apache.hadoop.hive.ql.exec.mr.ExecMapper: void close()>
<org.apache.hadoop.hive.metastore.columnstats.aggr.StringColumnStatsAggregator: org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj aggregate(java.util.List,java.util.List,boolean)>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.ASTNode analyzeCreateTable(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$PlannerContext)>
<org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService: boolean handleScheduleAttemptedRejection(org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper)>
<org.apache.hive.beeline.HiveSchemaTool: void updateDbNameForTable(java.sql.Statement,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator: void fixTmpPath(org.apache.hadoop.fs.Path)>
<org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin: boolean checkConvertJoinBucketMapJoin(org.apache.hadoop.hive.ql.exec.JoinOperator,int,org.apache.hadoop.hive.ql.optimizer.TezBucketJoinProcCtx)>
<org.apache.hive.beeline.hs2connection.BeelineSiteParser: java.util.Properties getConnectionProperties()>
<org.apache.hadoop.hive.ql.exec.Operator: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector[])>
<org.apache.hadoop.hive.ql.io.BatchToRowReader: void <init>(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx,java.util.List)>
<org.apache.hadoop.hive.ql.stats.StatsUpdaterThread: java.util.List processOneTable(org.apache.hadoop.hive.metastore.utils.MetaStoreUtils$FullTableName)>
<org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader: org.apache.orc.OrcProto$StripeFooter getStripeFooterFromCacheOrDisk(org.apache.orc.StripeInformation,org.apache.hadoop.hive.ql.io.orc.encoded.OrcBatchKey)>
<org.apache.hadoop.hive.ql.exec.FileSinkOperator: void createBucketForFileIdx(org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths,int)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List getPartitionsViaOrmFilter(org.apache.hadoop.hive.metastore.api.Table,org.apache.hadoop.hive.metastore.parser.ExpressionTree,short,boolean)>
<org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner: boolean prunePartitionNames(java.util.List,java.util.List,org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc,java.lang.String,java.util.List)>
<org.apache.hadoop.hive.llap.cache.SimpleBufferManager: long[] putFileData(java.lang.Object,org.apache.hadoop.hive.common.io.DiskRange[],org.apache.hadoop.hive.common.io.encoded.MemoryBuffer[],long,org.apache.hadoop.hive.llap.cache.LowLevelCache$Priority,org.apache.hadoop.hive.llap.cache.LowLevelCacheCounters,java.lang.String)>
<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: void create_database_core(org.apache.hadoop.hive.metastore.RawStore,org.apache.hadoop.hive.metastore.api.Database)>
<org.apache.hadoop.hive.metastore.HiveClientCache: void <init>(int,int,int,boolean)>
<org.apache.hadoop.hive.metastore.ObjectStore: void shutdown()>
<org.apache.hadoop.hive.metastore.utils.LogUtils: void logConfigLocation()>
<org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$InternalCompletionListener: void onSuccess(org.apache.tez.runtime.task.TaskRunner2Result)>
<org.apache.hadoop.hive.ql.exec.tez.WorkloadManagerFederation: org.apache.hadoop.hive.ql.exec.tez.TezSessionState getSession(org.apache.hadoop.hive.ql.exec.tez.TezSessionState,org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.exec.tez.UserPoolMapping$MappingInput,boolean,org.apache.hadoop.hive.ql.wm.WmContext)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listPartitionNames(java.lang.String,java.lang.String,java.lang.String,short)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listTableAllPartitionGrants(java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler: void purgeCompactionHistory()>
<org.apache.hadoop.hive.ql.exec.tez.LlapObjectCache: java.lang.Object retrieve(java.lang.String)>
<org.apache.hadoop.hive.ql.util.IncrementalObjectSizeEstimator: void addArrayEstimator(java.util.HashMap,java.util.Deque,java.lang.reflect.Field,java.lang.Object)>
<org.apache.hadoop.hive.metastore.tools.HiveMetaTool: void printTblURIUpdateSummary(org.apache.hadoop.hive.metastore.ObjectStore$UpdateMStorageDescriptorTblURIRetVal,boolean)>
<org.apache.hadoop.hive.llap.tezplugins.endpoint.LlapPluginSecurityInfo: org.apache.hadoop.security.token.TokenInfo getTokenInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler: void markFailed(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>
<org.apache.hadoop.hive.ql.optimizer.ppr.PartitionExpressionForMetastore: boolean filterPartitionsByExpr(java.util.List,byte[],java.lang.String,java.util.List)>
<org.apache.hadoop.hive.ql.exec.tez.DynamicPartitionPruner: void prunePartitionSingleSource(java.lang.String,org.apache.hadoop.hive.ql.exec.tez.DynamicPartitionPruner$SourceInfo)>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.SessionHandle openSession(org.apache.hive.service.rpc.thrift.TProtocolVersion,java.lang.String,java.lang.String,java.lang.String,java.util.Map)>
<org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler: org.apache.hadoop.hive.metastore.txn.CompactionInfo findNextToCompact(java.lang.String)>
<org.apache.hadoop.hive.ql.exec.FunctionRegistry: java.lang.reflect.Method getMethodInternal(java.lang.Class,java.util.List,boolean,java.util.List)>
<org.apache.hadoop.hive.metastore.AggregateStatsCache: org.apache.hadoop.hive.metastore.AggregateStatsCache$AggrColStats get(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.List)>
<org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService: org.apache.hadoop.hive.llap.daemon.impl.Scheduler$SubmissionState schedule(org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable)>
<org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl: void processCacheCollisions(long[],java.util.List,org.apache.hadoop.hive.common.io.encoded.MemoryBuffer[],java.util.List)>
<org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$SortMergedDeleteEventRegistry: void <init>(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.io.orc.OrcSplit,org.apache.orc.Reader$Options)>
<org.apache.hadoop.hive.ql.util.IncrementalObjectSizeEstimator: java.lang.Class getCollectionArg(java.lang.reflect.Field)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: boolean isTxnsInOpenState(java.util.List,java.sql.Statement)>
<org.apache.hadoop.hive.metastore.cache.SharedCache$TableWrapper: boolean updateTableColStats(java.util.List)>
<org.apache.hadoop.hive.ql.io.orc.ExternalCache: void translateSargToTableColIndexes(org.apache.hadoop.hive.ql.io.sarg.SearchArgument,org.apache.hadoop.conf.Configuration,int)>
<org.apache.hadoop.hive.ql.parse.repl.load.message.CreateFunctionHandler$PrimaryToReplicaResourceFunction: org.apache.hadoop.hive.metastore.api.ResourceUri destinationResourceUri(org.apache.hadoop.hive.metastore.api.ResourceUri)>
<org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter: org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter$OpAttr translateJoin(org.apache.calcite.rel.RelNode)>
<org.apache.hadoop.hive.ql.exec.ReplCopyTask: java.util.List filesInFileListing(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager: java.util.List lock(java.util.List,int,long)>
<org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveExpandDistinctAggregatesRule: void onMatch(org.apache.calcite.plan.RelOptRuleCall)>
<org.apache.hadoop.hive.metastore.MetaStoreDirectSql: void timingTrace(boolean,java.lang.String,long,long)>
<org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager: void printConflictingLocks(org.apache.hadoop.hive.ql.lockmgr.HiveLockObject,org.apache.hadoop.hive.ql.lockmgr.HiveLockMode,java.util.Set)>
<org.apache.hadoop.hive.ql.exec.tez.TriggerValidatorRunnable: void run()>
<org.apache.hadoop.hive.ql.util.IncrementalObjectSizeEstimator: java.lang.Class[] getMapArgs(java.lang.reflect.Field)>
<org.apache.hadoop.hive.ql.exec.Utilities: java.util.List getFullDPSpecs(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx)>
<org.apache.hadoop.hive.ql.io.AcidUtils$MetaDataFile: boolean isRawFormatFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)>
<org.apache.hadoop.hive.metastore.datasource.HikariCPDataSourceProvider: boolean supports(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.ql.io.HiveInputFormat: void pushFilters(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.exec.TableScanOperator,org.apache.hadoop.hive.ql.plan.MapWork)>
<org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer$FetchData: long getPathLength(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,long)>
<org.apache.hadoop.hive.ql.exec.tez.DynamicPartitionPruner$SourceInfo: void <init>(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.hive.ql.plan.ExprNodeDesc,java.lang.String,java.lang.String,org.apache.hadoop.mapred.JobConf)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: void countOpenTxns()>
<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void abortJob(org.apache.hadoop.mapred.JobContext,int)>
<org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager: void unlockPrimitive(org.apache.hadoop.hive.ql.lockmgr.HiveLock,java.lang.String,org.apache.curator.framework.CuratorFramework)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterGenerateResultOperator: void finishOuter(org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch,int,int,boolean,boolean,int,int,int)>
<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: org.apache.hadoop.hive.metastore.api.Partition append_partition_with_environment_context(java.lang.String,java.lang.String,java.util.List,org.apache.hadoop.hive.metastore.api.EnvironmentContext)>
<org.apache.hadoop.hive.ql.metadata.Hive: java.util.Map loadDynamicPartitions(org.apache.hadoop.fs.Path,java.lang.String,java.util.Map,org.apache.hadoop.hive.ql.plan.LoadTableDesc$LoadFileType,int,int,boolean,long,int,boolean,org.apache.hadoop.hive.ql.io.AcidUtils$Operation,boolean)>
<org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache: void setupOutput()>
<org.apache.hadoop.hive.ql.stats.StatsUpdaterThread: boolean runOneIteration()>
<org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient$JobStatusJob: void logConfigurations(org.apache.hadoop.mapred.JobConf)>
<org.apache.hadoop.hive.ql.exec.mr.ObjectCache: java.lang.Object retrieve(java.lang.String,java.util.concurrent.Callable)>
<org.apache.hadoop.hive.llap.LlapBaseInputFormat$LlapRecordReaderTaskUmbilicalExternalResponder: void sendOrQueueEvent(org.apache.hadoop.hive.llap.LlapBaseRecordReader$ReaderEvent)>
<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: boolean canCreateSargFromConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.metastore.cache.CachedStore: boolean isInWhitelist(java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.metastore.utils.MetaStoreUtils$3: org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj call()>
<org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler: void registerDag(java.lang.String,int,org.apache.hadoop.security.token.Token,java.lang.String,java.lang.String[])>
<org.apache.hadoop.hive.llap.daemon.impl.AMReporter$AMHeartbeatCallable: java.lang.Void callInternal()>
<org.apache.hadoop.hive.llap.security.LlapTokenSelector: org.apache.hadoop.security.token.Token selectToken(org.apache.hadoop.io.Text,java.util.Collection)>
<org.apache.hadoop.hive.ql.parse.GenTezUtils: void processDynamicSemiJoinPushDownOperator(org.apache.hadoop.hive.ql.parse.GenTezProcContext,org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo,org.apache.hadoop.hive.ql.exec.ReduceSinkOperator)>
<org.apache.hadoop.hive.metastore.columnstats.aggr.DateColumnStatsAggregator: org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj aggregate(java.util.List,java.util.List,boolean)>
<org.apache.hadoop.hive.ql.exec.MapOperator: void cleanUpInputFileChangedOp()>
<org.apache.hadoop.hive.metastore.security.MemoryTokenStore: boolean removeToken(org.apache.hadoop.hive.metastore.security.DelegationTokenIdentifier)>
<org.apache.hadoop.hive.common.auth.HiveAuthUtils: org.apache.thrift.transport.TServerSocket getServerSSLSocket(java.lang.String,int,java.lang.String,java.lang.String,java.util.List)>
<org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler: void markCompacted(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>
<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.ql.metadata.Partition getPartition(org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,boolean,java.lang.String,boolean)>
<org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat: org.apache.hadoop.hbase.client.Scan createFilterScan(org.apache.hadoop.mapred.JobConf,int,int,boolean)>
<org.apache.hadoop.hive.metastore.Warehouse: boolean isWritable(org.apache.hadoop.fs.Path)>
<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void findUnknownPartitions(org.apache.hadoop.hive.ql.metadata.Table,java.util.Set,org.apache.hadoop.hive.ql.metadata.CheckResult)>
<org.apache.hadoop.hive.ql.lockmgr.DbTxnManager: void heartbeat()>
<org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory$GenericFuncExprProcessor: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.ClassLoader getSessionSpecifiedClassLoader()>
<org.apache.hadoop.hive.ql.exec.tez.TezTask: void logResources(java.util.List)>
<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: java.lang.String getPartitionName(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Set)>
<org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles: org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles$AverageSize getAverageSize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)>
<org.apache.hive.spark.client.rpc.RpcDispatcher: void discardRpc(long)>
<org.apache.hadoop.hive.serde2.JsonSerDe: void populateRecord(java.util.List,org.codehaus.jackson.JsonToken,org.codehaus.jackson.JsonParser,org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo)>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genTablePlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB)>
<org.apache.hadoop.hive.ql.optimizer.IdentityProjectRemover$ProjectRemover: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.ql.parse.repl.load.message.RenamePartitionHandler: java.util.List handle(org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandler$Context)>
<org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler: java.util.List findReadyToClean()>
<org.apache.hadoop.hive.ql.exec.PartitionKeySampler: byte[][] toPartitionKeys(byte[][],int)>
<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean init(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listPrincipalMDBGrants(java.lang.String,org.apache.hadoop.hive.metastore.api.PrincipalType,java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.ql.DriverContext: void shutdown()>
<org.apache.hive.service.cli.CLIService: java.lang.String getQueryId(org.apache.hive.service.rpc.thrift.TOperationHandle)>
<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$Context: void <init>(org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.hive.ql.io.orc.ExternalCache$ExternalFooterCachesByConf)>
<org.apache.hive.storage.jdbc.JdbcSerDe: java.lang.Object deserialize(org.apache.hadoop.io.Writable)>
<org.apache.hadoop.hive.serde2.JsonSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>
<org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter: org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter$OpAttr visit(org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveProject)>
<org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner: void applyFilterToPartitions(org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters$Converter,org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator,java.lang.String,java.util.Set)>
<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: org.apache.orc.TypeDescription getDesiredRowTypeDescr(org.apache.hadoop.conf.Configuration,boolean,int)>
<org.apache.hadoop.hive.ql.parse.repl.dump.TableExport: void writeMetaData(org.apache.hadoop.hive.ql.metadata.PartitionIterable)>
<org.apache.hadoop.hive.llap.log.LlapRoutingAppenderPurgePolicy: void <init>(java.lang.String)>
<org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask: java.lang.Long bootStrapDump(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.parse.repl.load.DumpMetaData,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.plan.FileSinkDesc createFileSinkDesc(java.lang.String,org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.hive.ql.metadata.Partition,org.apache.hadoop.fs.Path,int,boolean,boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$SortBucketRSCtx,org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx,org.apache.hadoop.hive.ql.plan.ListBucketingCtx,org.apache.hadoop.hive.ql.exec.RowSchema,boolean,org.apache.hadoop.hive.ql.metadata.Table,java.lang.Long,boolean,java.lang.Integer,org.apache.hadoop.hive.ql.parse.QB)>
<org.apache.hadoop.hive.ql.io.HiveInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.OperationHandle getTableTypes(org.apache.hive.service.cli.SessionHandle)>
<org.apache.hive.service.cli.thrift.ThriftHttpServlet: void <init>(org.apache.thrift.TProcessor,org.apache.thrift.protocol.TProtocolFactory,java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.security.UserGroupInformation,org.apache.hive.service.auth.HiveAuthFactory)>
<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.net.URI initializeFromURI(java.lang.String,boolean)>
<org.apache.hive.storage.jdbc.JdbcStorageHandler: void configureInputJobProperties(org.apache.hadoop.hive.ql.plan.TableDesc,java.util.Map)>
<org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl: org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch$ColumnStreamData createRgColumnStreamData(int,boolean,int,org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl$StreamContext,long,long,boolean,long)>
<org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy: void notifyUnlock(org.apache.hadoop.hive.llap.cache.LlapCacheableBuffer)>
<org.apache.hadoop.hive.metastore.security.DBTokenStore: int addMasterKey(java.lang.String)>
<org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl: org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl$ProcCacheChunk addOneCompressionBuffer(org.apache.orc.impl.BufferChunk,java.util.List,java.util.List,java.util.IdentityHashMap,java.util.List,java.util.List)>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: void analyzeInternal(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$PlannerContextFactory)>
<org.apache.hadoop.hive.serde2.lazy.LazyHiveChar: void init(org.apache.hadoop.hive.serde2.lazy.ByteArrayRef,int,int)>
<org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.DatabaseEventsIterator: org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.BootstrapEvent postProcessing(org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.BootstrapEvent)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: org.apache.hadoop.hive.metastore.api.LockResponse checkLock(org.apache.hadoop.hive.metastore.api.CheckLockRequest)>
<org.apache.hadoop.hive.ql.Driver: void releaseResStream()>
<org.apache.hadoop.hive.ql.exec.tez.monitoring.TezJobMonitor: int monitorExecution()>
<org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$InternalCompletionListener: void updatePreemptionListAndNotify(org.apache.tez.runtime.task.EndReason)>
<org.apache.hive.service.auth.ldap.LdapSearchFactory: javax.naming.directory.DirContext createDirContext(org.apache.hadoop.hive.conf.HiveConf,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType determineCompactionType(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidWriteIdList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map)>
<org.apache.hadoop.hive.metastore.security.MemoryTokenStore: org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation getToken(org.apache.hadoop.hive.metastore.security.DelegationTokenIdentifier)>
<org.apache.hadoop.hive.ql.Driver: java.util.concurrent.locks.ReentrantLock tryAcquireCompileLock(boolean,java.lang.String)>
<org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService: java.lang.Object deallocateContainer(org.apache.hadoop.yarn.api.records.ContainerId)>
<org.apache.hadoop.hive.ql.exec.MoveTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.SessionHandle openSessionWithImpersonation(java.lang.String,java.lang.String,java.util.Map,java.lang.String)>
<org.apache.hive.hcatalog.data.HCatRecordSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>
<org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService: void finishableStateUpdated(org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper,boolean)>
<org.apache.hadoop.hive.ql.parse.repl.dump.events.AddPrimaryKeyHandler: void handle(org.apache.hadoop.hive.ql.parse.repl.dump.events.EventHandler$Context)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator: void reloadHashTable(byte,int)>
<org.apache.hadoop.hive.llap.io.metadata.MetadataCache: boolean lockOldVal(java.lang.Object,org.apache.hadoop.hive.llap.io.metadata.MetadataCache$LlapBufferOrBuffers,org.apache.hadoop.hive.llap.io.metadata.MetadataCache$LlapBufferOrBuffers)>
<org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin: int getMapJoinConversionPos(org.apache.hadoop.hive.ql.exec.JoinOperator,org.apache.hadoop.hive.ql.parse.OptimizeTezProcContext,int,boolean,long,boolean)>
<org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher: boolean connect(org.apache.hadoop.hive.ql.stats.StatsCollectionContext)>
<org.apache.hadoop.hive.llap.daemon.impl.FunctionLocalizer: void runWorkThread()>
<org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.JDBCAbstractSplitFilterRule: void onMatch(org.apache.calcite.plan.RelOptRuleCall,org.apache.calcite.sql.SqlDialect)>
<org.apache.hive.service.server.ThreadWithGarbageCleanup: void cleanRawStore()>
<org.apache.hadoop.hive.ql.parse.spark.GenSparkWork: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.ql.exec.Utilities: java.util.List getInputPaths(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.Context,boolean)>
<org.apache.hadoop.hive.ql.metadata.Hive: void deleteOldPathForReplace(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,boolean,org.apache.hadoop.fs.PathFilter,boolean)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: void wait(java.sql.Connection,java.sql.Savepoint)>
<org.apache.hadoop.hive.llap.shufflehandler.FadvisedFileRegion: void transferSuccessful()>
<org.apache.hadoop.hive.ql.io.CombineHiveInputFormat: org.apache.hadoop.mapred.InputSplit[] getCombineSplits(org.apache.hadoop.mapred.JobConf,int,java.util.Map)>
<org.apache.hadoop.hive.ql.io.CodecPool: org.apache.hadoop.io.compress.Compressor getCompressor(org.apache.hadoop.io.compress.CompressionCodec)>
<org.apache.hadoop.hive.ql.lockmgr.DbTxnManager: long openTxn(org.apache.hadoop.hive.ql.Context,java.lang.String,long)>
<org.apache.hadoop.hive.ql.exec.Operator: void setOpTraits(org.apache.hadoop.hive.ql.plan.OpTraits)>
<org.apache.hadoop.hive.ql.exec.FetchOperator: org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit[] getNextSplits()>
<org.apache.hadoop.hive.ql.optimizer.StatsOptimizer$MetaDataProcessor: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.accumulo.AccumuloStorageHandler: void configureInputJobProperties(org.apache.hadoop.hive.ql.plan.TableDesc,java.util.Map)>
<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Task addSinglePartition(java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.ql.plan.ImportTableDesc,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.plan.AddPartitionDesc,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext,java.lang.Long,int)>
<org.apache.hadoop.hive.ql.parse.TezCompiler: void connect(org.apache.hadoop.hive.ql.exec.Operator,java.util.concurrent.atomic.AtomicInteger,java.util.Stack,java.util.Map,java.util.Map,java.util.Set,org.apache.hadoop.hive.ql.parse.ParseContext)>
<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.hive.llap.Schema,org.apache.hadoop.yarn.api.records.ApplicationId,boolean)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable: void add(long,org.apache.hadoop.io.BytesWritable)>
<org.apache.hadoop.hive.ql.exec.MoveTask: org.apache.hadoop.hive.ql.hooks.LineageInfo$DataContainer handleStaticParts(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.ql.plan.LoadTableDesc,org.apache.hadoop.hive.ql.exec.MoveTask$TaskInformation)>
<org.apache.hadoop.hive.ql.exec.DDLTask: int describeTable(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.DescTableDesc)>
<org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$ReaderPairAcid: void next(org.apache.hadoop.hive.ql.io.orc.OrcStruct)>
<org.apache.hadoop.hive.metastore.utils.HdfsUtils: void run(org.apache.hadoop.fs.FsShell,java.lang.String[])>
<org.apache.hive.service.cli.operation.OperationManager: void cancelOperation(org.apache.hive.service.cli.OperationHandle,java.lang.String)>
<org.apache.hadoop.hive.ql.exec.FetchOperator: void setValidWriteIdList(java.lang.String)>
<org.apache.hadoop.hive.ql.parse.MacroSemanticAnalyzer: void analyzeInternal(org.apache.hadoop.hive.ql.parse.ASTNode)>
<org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeHashAggregate: void flush(boolean)>
<org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization: boolean generateSemiJoinOperatorPlan(org.apache.hadoop.hive.ql.parse.GenTezUtils$DynamicListContext,org.apache.hadoop.hive.ql.parse.ParseContext,org.apache.hadoop.hive.ql.exec.TableScanOperator,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.hive.ql.parse.SemiJoinHint)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: void abortTxn(org.apache.hadoop.hive.metastore.api.AbortTxnRequest)>
<org.apache.hadoop.hive.ql.plan.LoadTableDesc: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.plan.TableDesc,java.util.Map,org.apache.hadoop.hive.ql.plan.LoadTableDesc$LoadFileType,org.apache.hadoop.hive.ql.io.AcidUtils$Operation,java.lang.Long)>
<org.apache.hadoop.hive.ql.exec.Operator: void defaultEndGroup()>
<org.apache.hadoop.hive.ql.parse.repl.dump.PartitionExport: void lambda$write$1(org.apache.hadoop.hive.ql.metadata.Partition,org.apache.hadoop.hive.ql.parse.ReplicationSpec)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiLongOperator: void process(java.lang.Object,int)>
<org.apache.hadoop.hive.ql.exec.MapOperator: void initOperatorContext(java.util.List)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List getCatalogs()>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable: void expandAndRehash()>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.OperationHandle executeStatementAsync(org.apache.hive.service.cli.SessionHandle,java.lang.String,java.util.Map)>
<org.apache.hadoop.hive.llap.cache.LlapAllocatorBuffer: int incRefInternal(boolean)>
<org.apache.hadoop.hive.llap.tezplugins.LlapContainerLauncher: void launchContainer(org.apache.tez.serviceplugins.api.ContainerLaunchRequest)>
<org.apache.hadoop.hive.ql.exec.tez.SessionExpirationTracker: void <init>(long,long,org.apache.hadoop.hive.ql.exec.tez.SessionExpirationTracker$RestartImpl)>
<org.apache.hadoop.hive.accumulo.HiveAccumuloHelper: void setInputFormatConnectorInfo(org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.accumulo.core.client.security.tokens.AuthenticationToken)>
<org.apache.hadoop.hive.upgrade.acid.UpgradeTool: void makeRenameFileScript(java.lang.String)>
<org.apache.hadoop.hive.ql.stats.BasicStatsTask: java.util.concurrent.ExecutorService buildBasicStatsExecutor()>
<org.apache.hadoop.hive.ql.reexec.ReExecDriver: boolean shouldReExecute()>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: boolean doPhase1(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$Phase1Ctx,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$PlannerContext)>
<org.apache.hadoop.hive.hbase.HBaseSerDeHelper: void generateColumnTypes(java.util.Properties,java.util.List,java.lang.StringBuilder,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler$Shuffle: void exceptionCaught(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ExceptionEvent)>
<org.apache.hadoop.hive.ql.exec.Utilities: java.lang.String getIdFromFilename(java.lang.String,java.util.regex.Pattern)>
<org.apache.hadoop.hive.ql.Driver: void releaseContext()>
<org.apache.hadoop.hive.metastore.ReplChangeManager$CMClearer: void run()>
<org.apache.hive.spark.client.rpc.KryoMessageCodec: void encode(io.netty.channel.ChannelHandlerContext,java.lang.Object,io.netty.buffer.ByteBuf)>
<org.apache.hadoop.hive.ql.txn.compactor.Worker: void run()>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: void replaceDefaultKeyword(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.metadata.Table,java.util.List)>
<org.apache.hadoop.hive.metastore.security.MemoryTokenStore: boolean removeMasterKey(int)>
<org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$JoinStatsRule: void updateColStats(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.plan.Statistics,long,long,org.apache.hadoop.hive.ql.exec.CommonJoinOperator,java.util.Map)>
<org.apache.hadoop.hive.ql.parse.TezCompiler: void removeSemijoinOptimizationByBenefit(org.apache.hadoop.hive.ql.parse.OptimizeTezProcContext)>
<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void prepareReturnValues(java.util.List,java.lang.String)>
<org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeHashAggregate: void checkHashModeEfficiency()>
<org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl: org.apache.hadoop.hive.ql.io.orc.encoded.CacheChunk prepareRangesForCompressedRead(long,long,long,long,org.apache.hadoop.hive.common.io.DiskRangeList,org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch$ColumnStreamData,java.util.IdentityHashMap,java.util.List,java.util.List,java.util.List)>
<org.apache.hadoop.hive.common.io.DiskRangeList$CreateHelper: void addOrMerge(long,long,boolean,boolean)>
<org.apache.hive.storage.jdbc.JdbcInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>
<org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.JDBCAbstractSplitFilterRule$JDBCSplitFilterAboveJoinRule: boolean matches(org.apache.calcite.plan.RelOptRuleCall)>
<org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader: void stop()>
<org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor: void init(org.apache.tez.mapreduce.processor.MRTaskReporter,java.util.Map,java.util.Map)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listPartitionGrants(java.lang.String,java.lang.String,java.lang.String,java.util.List)>
<org.apache.hadoop.hive.ql.parse.TezCompiler: void removeCycleOperator(java.util.Set,org.apache.hadoop.hive.ql.parse.OptimizeTezProcContext)>
<org.apache.hadoop.hive.ql.io.orc.OrcSplit: void write(java.io.DataOutput)>
<org.apache.hadoop.hive.accumulo.mr.AccumuloIndexedOutputFormat$AccumuloRecordWriter: void addTable(org.apache.hadoop.io.Text)>
<org.apache.hive.spark.client.RemoteDriver$DriverProtocol: void sendMetrics(java.lang.String,int,int,long,org.apache.hive.spark.client.metrics.Metrics)>
<org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer$FetchData: boolean isDataLengthWithInThreshold(org.apache.hadoop.hive.ql.parse.ParseContext,long)>
<org.apache.hadoop.hive.ql.exec.Utilities: java.util.List getValidMmDirectoriesFromTableOrPart(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.common.ValidWriteIdList)>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genSelectPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator,org.apache.hadoop.hive.ql.exec.Operator)>
<org.apache.hadoop.hive.ql.exec.tez.HostAffinitySplitLocationProvider: int determineLocation(java.util.List,java.lang.String,long,java.lang.String)>
<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator: java.util.List filterListCmdObjects(java.util.List,org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzContext)>
<org.apache.hadoop.hive.ql.exec.MoveTask: void releaseLocks(org.apache.hadoop.hive.ql.plan.LoadTableDesc)>
<org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider$LlapDecisionDispatcher$1: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: void moveTaskOutputs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)>
<org.apache.hadoop.hive.serde2.avro.AvroSerDe: org.apache.avro.Schema getSchemaFromCols(java.util.Properties,java.util.List,java.util.List,java.lang.String)>
<org.apache.hadoop.hive.llap.io.ChunkedInputStream: void close()>
<org.apache.hive.spark.client.RemoteDriver$DriverProtocol: void sendErrorMessage(java.lang.String)>
<org.apache.hadoop.hive.llap.log.LlapRoutingAppenderPurgePolicy: void keyComplete(java.lang.String)>
<org.apache.hadoop.hive.ql.exec.tez.DynamicValueRegistryTez: void init(org.apache.hadoop.hive.ql.exec.DynamicValueRegistry$RegistryConf)>
<org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl: void releaseEcbRefCountsOnError(org.apache.hadoop.hive.ql.io.orc.encoded.Reader$OrcEncodedColumnBatch)>
<org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer: void repositionInStreams(org.apache.orc.impl.TreeReaderFactory$TreeReader[],org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch,boolean,org.apache.hadoop.hive.llap.io.metadata.ConsumerStripeMetadata)>
<org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader: org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader$ReaderWithOffsets createOffsetReader(org.apache.hadoop.mapred.RecordReader)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: void ensureValidTxn(java.sql.Connection,long,java.sql.Statement)>
<org.apache.hadoop.hive.ql.hooks.ATSHook: org.apache.hadoop.yarn.api.records.timeline.TimelineEntity createPreHookEvent(java.lang.String,java.lang.String,org.json.JSONObject,long,java.lang.String,java.lang.String,int,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.List,java.util.List,org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)>
<org.apache.hadoop.hive.metastore.columnstats.merge.StringColumnStatsMerger: void merge(org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj,org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj)>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genBucketingSortingDest(java.lang.String,org.apache.hadoop.hive.ql.exec.Operator,org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$SortBucketRSCtx)>
<org.apache.hadoop.hive.ql.exec.tez.DynamicPartitionPruner: java.lang.String processPayload(java.nio.ByteBuffer,java.lang.String)>
<org.apache.hadoop.hive.ql.parse.TezCompiler: void removeSemijoinsParallelToMapJoin(org.apache.hadoop.hive.ql.parse.OptimizeTezProcContext)>
<org.apache.hadoop.hive.ql.lockmgr.DbTxnManager$MaterializationRebuildLockHeartbeater: void run()>
<org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateTableHandler: void handle(org.apache.hadoop.hive.ql.parse.repl.dump.events.EventHandler$Context)>
<org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper: void initializeSerProperties(org.apache.hadoop.mapreduce.JobContext,java.util.Properties)>
<org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction: org.apache.calcite.rel.RelNode genTableLogicalPlan(java.lang.String,org.apache.hadoop.hive.ql.parse.QB)>
<org.apache.hive.service.cli.CLIService: void cancelOperation(org.apache.hive.service.cli.OperationHandle)>
<org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler: void markCleaned(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>
<org.apache.hadoop.hive.ql.metadata.Hive: void close()>
<org.apache.hive.hcatalog.data.HCatRecordSerDe: void initialize(org.apache.hive.hcatalog.data.schema.HCatSchema)>
<org.apache.hadoop.hive.metastore.tools.MetastoreSchemaTool: java.util.List findCreateTable(java.lang.String,java.util.List)>
<org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils: void setMapWork(org.apache.hadoop.hive.ql.plan.MapWork,org.apache.hadoop.hive.ql.parse.ParseContext,java.util.Set,org.apache.hadoop.hive.ql.parse.PrunedPartitionList,org.apache.hadoop.hive.ql.exec.TableScanOperator,java.lang.String,org.apache.hadoop.hive.conf.HiveConf,boolean)>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.OperationHandle getColumns(org.apache.hive.service.cli.SessionHandle,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.ql.exec.ExportTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.lang.String getGuidFromDB()>
<org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler: void close()>
<org.apache.hadoop.hive.metastore.MetaStoreDirectSql: byte[] extractSqlBlob(java.lang.Object)>
<org.apache.hadoop.hive.ql.optimizer.QueryPlanPostProcessor: void collectFileSinkDescs(org.apache.hadoop.hive.ql.exec.Operator,java.util.Set)>
<org.apache.hadoop.hive.metastore.cache.SharedCache: void refreshDatabasesInCache(java.util.List)>
<org.apache.hadoop.hive.ql.metadata.Hive: java.util.Set getValidPartitionsInPath(int,int,org.apache.hadoop.fs.Path,java.lang.Long,int,boolean,boolean)>
<org.apache.hadoop.hive.ql.cache.results.QueryResultsCache$CacheEntry: void releaseReader()>
<org.apache.hadoop.hive.llap.cache.LlapAllocatorBuffer: int releaseInvalidated()>
<org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable: void updateColStats(java.util.Set,boolean)>
<org.apache.hadoop.hive.ql.io.parquet.LeafFilterFactory: org.apache.hadoop.hive.ql.io.parquet.FilterPredicateLeafBuilder getLeafFilterBuilderByType(org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf$Type,org.apache.parquet.schema.Type)>
<org.apache.hadoop.hive.metastore.AggregateStatsCache$1: void run()>
<org.apache.hadoop.hive.ql.optimizer.SetReducerParallelism: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.metastore.ObjectStore: boolean dropType(java.lang.String)>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genJoinOperatorChildren(org.apache.hadoop.hive.ql.parse.QBJoinTree,org.apache.hadoop.hive.ql.exec.Operator,org.apache.hadoop.hive.ql.exec.Operator[],java.util.HashSet,org.apache.hadoop.hive.ql.plan.ExprNodeDesc[][])>
<org.apache.hadoop.hive.ql.ppd.SyntheticJoinPredicate$JoinSynthetic: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl: org.apache.hadoop.hive.ql.io.orc.encoded.IncompleteCb addIncompleteCompressionBuffer(long,org.apache.hadoop.hive.common.io.DiskRangeList,int,boolean,org.apache.hadoop.hive.ql.io.orc.encoded.IoTrace)>
<org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl: void traceLogBuffersUsedToParse(org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch$ColumnStreamData)>
<org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Server$SaslDigestCallbackHandler: void handle(javax.security.auth.callback.Callback[])>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.OperationStatus getOperationStatus(org.apache.hive.service.cli.OperationHandle,boolean)>
<org.apache.hadoop.hive.ql.util.HiveStrictManagedMigration: org.apache.hadoop.hive.ql.util.HiveStrictManagedMigration$TableMigrationOption determineMigrationTypeAutomatically(org.apache.hadoop.hive.metastore.api.Table,org.apache.hadoop.hive.metastore.TableType)>
<org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.JDBCJoinPushDownRule: void onMatch(org.apache.calcite.plan.RelOptRuleCall)>
<org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider$LlapDecisionDispatcher: boolean checkAggregator(org.apache.hadoop.hive.ql.plan.AggregationDesc)>
<org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveOnTezCostModel$TezCommonJoinAlgorithm: org.apache.calcite.plan.RelOptCost getCost(org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin)>
<org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService: void killFragment(java.lang.String)>
<org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer: boolean validPreConditions(org.apache.hadoop.hive.ql.parse.ParseContext,org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer$SharedWorkOptimizerCache,org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer$SharedResult)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: void heartbeat(org.apache.hadoop.hive.metastore.api.HeartbeatRequest)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: org.apache.hadoop.hive.metastore.api.ShowCompactResponse showCompact(org.apache.hadoop.hive.metastore.api.ShowCompactRequest)>
<org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkJoinDeDuplication$ReducerProc: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.ql.exec.Utilities$1: void interrupt()>
<org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.JDBCSortPushDownRule: void onMatch(org.apache.calcite.plan.RelOptRuleCall)>
<org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat: org.apache.hadoop.hive.ql.io.orc.OrcFile$WriterOptions getOptions(org.apache.hadoop.mapred.JobConf,java.util.Properties)>
<org.apache.hadoop.hive.druid.DruidStorageHandler: void configureJobConf(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: long getDbTime(java.sql.Connection)>
<org.apache.hadoop.hive.ql.hooks.HiveProtoLoggingHook$EventLogger: void generateEvent(org.apache.hadoop.hive.ql.hooks.HookContext)>
<org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge: org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Client createClientWithConf(java.lang.String)>
<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Task dbUpdateReplStateTask(java.lang.String,java.lang.String,org.apache.hadoop.hive.ql.exec.Task)>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: void validate()>
<org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory$ConstantPropagateReduceSinkProc: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.ql.exec.Utilities: boolean isCopyFile(java.lang.String)>
<org.apache.hive.storage.jdbc.JdbcStorageHandler: void configureTableJobProperties(org.apache.hadoop.hive.ql.plan.TableDesc,java.util.Map)>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: void replaceDefaultKeywordForUpdate(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.metadata.Table)>
<org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory: org.apache.hadoop.hive.ql.plan.ExprNodeDesc foldExprFull(org.apache.hadoop.hive.ql.plan.ExprNodeDesc,java.util.Map,org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcCtx,org.apache.hadoop.hive.ql.exec.Operator,int,boolean)>
<org.apache.hadoop.hive.ql.optimizer.physical.SerializeFilter$Serializer: void evaluateOperators(org.apache.hadoop.hive.ql.plan.BaseWork,org.apache.hadoop.hive.ql.optimizer.physical.PhysicalContext)>
<org.apache.hive.service.auth.ldap.LdapSearch: java.lang.String findUserDn(java.lang.String)>
<org.apache.hadoop.hive.metastore.cache.CachedStore: boolean shouldCacheTable(java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hive.streaming.AbstractRecordWriter$OrcMemoryPressureMonitor: void memoryUsageAboveThreshold(long,long)>
<org.apache.hadoop.hive.ql.exec.MapOperator: void setChildren(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.ql.cache.results.QueryResultsCache$InvalidationEventConsumer: void accept(org.apache.hadoop.hive.metastore.api.NotificationEvent)>
<org.apache.hadoop.hive.metastore.ObjectStore: void checkSchema()>
<org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService: org.apache.hadoop.yarn.api.records.Resource getTotalResources()>
<org.apache.hadoop.hive.ql.metadata.Hive: void fireInsertEvent(org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,boolean,java.util.List)>
<org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$ReduceSinkStatsRule: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader: void determineStripesToRead()>
<org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$JoinStatsRule: long computeFinalRowCount(java.util.List,long,org.apache.hadoop.hive.ql.exec.CommonJoinOperator)>
<org.apache.hadoop.hive.ql.io.CombineHiveInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>
<org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Task analyzeExport(org.apache.hadoop.hive.ql.parse.ASTNode,java.lang.String,org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.conf.HiveConf,java.util.Set,java.util.Set)>
<org.apache.hive.streaming.HiveStreamingConnection: void setHiveConf(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.conf.HiveConf$ConfVars,boolean)>
<org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher: boolean logExplainVectorization(org.apache.hadoop.hive.ql.plan.BaseWork,java.lang.String)>
<org.apache.hadoop.hive.ql.io.AcidUtils: java.lang.Long extractWriteId(org.apache.hadoop.fs.Path)>
<org.apache.hadoop.hive.metastore.security.TokenStoreDelegationTokenSecretManager: void stopThreads()>
<org.apache.hadoop.hive.llap.log.LlapRoutingAppenderPurgePolicy: void update(java.lang.String,org.apache.logging.log4j.core.LogEvent)>
<org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.JDBCUnionPushDownRule: void onMatch(org.apache.calcite.plan.RelOptRuleCall)>
<org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths: void <init>(org.apache.hadoop.hive.ql.exec.FileSinkOperator,org.apache.hadoop.fs.Path,boolean)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listPartitionColumnGrantsAll(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiStringOperator: void process(java.lang.Object,int)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: java.util.List getTargetTxnIdList(java.lang.String,java.util.List,java.sql.Statement)>
<org.apache.hadoop.hive.metastore.cache.CachedStore: org.apache.hadoop.hive.metastore.cache.CachedStore$MergedColumnStatsForPartitions mergeColStatsForPartitions(java.lang.String,java.lang.String,java.lang.String,java.util.List,java.util.List,org.apache.hadoop.hive.metastore.cache.SharedCache)>
<org.apache.hadoop.hive.metastore.tools.MetastoreSchemaTool: int run(java.lang.String[])>
<org.apache.hadoop.hive.ql.optimizer.StatsOptimizer$MetaDataProcessor: java.util.Collection verifyAndGetPartColumnStats(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.metadata.Table,java.lang.String,java.util.Set)>
<org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats$GenericUDAFBooleanStatsEvaluator: void printDebugOutput(java.lang.String,org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator$AggregationBuffer)>
<org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable: void newTableTasks(org.apache.hadoop.hive.ql.plan.ImportTableDesc,org.apache.hadoop.hive.ql.exec.Task)>
<org.apache.hadoop.hive.ql.io.parquet.ParquetRecordReaderBase: org.apache.parquet.hadoop.ParquetInputSplit getSplit(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf)>
<org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcCtx: java.util.Map getPropagatedConstants(org.apache.hadoop.hive.ql.exec.Operator)>
<org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeHashAggregate: void initialize(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory: java.util.Map initiateSparkConf(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>
<org.apache.hadoop.hive.llap.shufflehandler.IndexCache: org.apache.tez.runtime.library.common.sort.impl.TezIndexRecord getIndexInformation(java.lang.String,int,org.apache.hadoop.fs.Path,java.lang.String)>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: java.util.Map parseSemiJoinHint(java.util.List)>
<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void analyzeReplStatus(org.apache.hadoop.hive.ql.parse.ASTNode)>
<org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap: int findKeySlotToWrite(long,int,int)>
<org.apache.hadoop.hive.ql.parse.repl.load.message.AddPrimaryKeyHandler: java.util.List handle(org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandler$Context)>
<org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader$CacheWriter: org.apache.orc.PhysicalWriter$OutputReceiver createDataStream(org.apache.orc.impl.StreamName)>
<org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader: void consumeData(org.apache.hadoop.hive.llap.io.api.impl.ColumnVectorBatch)>
<org.apache.hadoop.hive.ql.parse.TaskCompiler: void setLoadFileLocation(org.apache.hadoop.hive.ql.parse.ParseContext,org.apache.hadoop.hive.ql.plan.LoadFileDesc)>
<org.apache.hadoop.hive.llap.cli.LlapServiceDriver: int run(java.lang.String[])>
<org.apache.hadoop.hive.metastore.ObjectStore: void createCatalog(org.apache.hadoop.hive.metastore.api.Catalog)>
<org.apache.hadoop.hive.llap.io.ChunkedOutputStream: void <init>(java.io.OutputStream,int,java.lang.String)>
<org.apache.hive.storage.jdbc.JdbcRecordReader: boolean next(org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.MapWritable)>
<org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler$Shuffle: void setResponseHeaders(org.jboss.netty.handler.codec.http.HttpResponse,boolean,long)>
<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void analyzeInternal(org.apache.hadoop.hive.ql.parse.ASTNode)>
<org.apache.hadoop.hive.ql.exec.ReplCopyTask: org.apache.hadoop.hive.ql.exec.Task getLoadCopyTask(org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf)>
<org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter: org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter$OpAttr visit(org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveUnion)>
<org.apache.hadoop.hive.common.type.TimestampTZUtil: org.apache.hadoop.hive.common.type.TimestampTZ parseOrNull(java.lang.String,java.time.ZoneId)>
<org.apache.hadoop.hive.common.LogUtils: void logConfigLocation(org.apache.hadoop.hive.conf.HiveConf)>
<org.apache.hive.hcatalog.listener.DbNotificationListener: void process(org.apache.hadoop.hive.metastore.api.NotificationEvent,org.apache.hadoop.hive.metastore.events.ListenerEvent)>
<org.apache.hadoop.hive.llap.LlapBaseInputFormat$LlapRecordReaderTaskUmbilicalExternalResponder: void setRecordReader(org.apache.hadoop.hive.llap.LlapBaseRecordReader)>
<org.apache.hadoop.hive.metastore.ObjectStore: void initialize(java.util.Properties)>
<org.apache.hadoop.hive.ql.exec.spark.SmallTableCache: void cache(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer)>
<org.apache.hadoop.hive.ql.optimizer.SamplePruner: org.apache.hadoop.fs.Path[] prune(org.apache.hadoop.hive.ql.metadata.Partition,org.apache.hadoop.hive.ql.plan.FilterDesc$SampleDesc)>
<org.apache.hadoop.hive.ql.exec.vector.rowbytescontainer.VectorRowBytesContainer: void setupOutputFileStreams()>
<org.apache.hadoop.hive.llap.log.LlapWrappedAppender: void setupAppenderIfRequired(org.apache.logging.log4j.core.LogEvent)>
<org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge: boolean loginUserHasCurrentAuthMethod(org.apache.hadoop.security.UserGroupInformation,java.lang.String)>
<org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer$SkewJoinProc: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.ql.log.PerfLogger: void PerfLogBegin(java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.conf.HiveConf: void initialize(java.lang.Class)>
<org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler: void cleanTxnToWriteIdTable()>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterStringOperator: void process(java.lang.Object,int)>
<org.apache.hive.streaming.HiveStreamingConnection: void setHiveConf(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>
<org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.JDBCFilterPushDownRule: void onMatch(org.apache.calcite.plan.RelOptRuleCall)>
<org.apache.hadoop.hive.ql.parse.GenTezUtils: void removeUnionOperators(org.apache.hadoop.hive.ql.parse.GenTezProcContext,org.apache.hadoop.hive.ql.plan.BaseWork,int)>
<org.apache.hive.spark.client.rpc.KryoMessageCodec: void decode(io.netty.channel.ChannelHandlerContext,io.netty.buffer.ByteBuf,java.util.List)>
<org.apache.hadoop.hive.ql.exec.spark.SparkTask: void killJob()>
<org.apache.hadoop.hive.ql.exec.Utilities: void ponderRemovingTempOrDuplicateFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.util.HashMap)>
<org.apache.hadoop.hive.ql.parse.ParseUtils: boolean createChildColumnRef(org.antlr.runtime.tree.Tree,java.lang.String,java.util.List,java.util.HashSet)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listPartitionGrantsAll(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.registry.impl.ZkRegistryBase: void removeFromCache(java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.conf.HiveConfUtil: void updateJobCredentialProviders(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils: void addStatsTask(org.apache.hadoop.hive.ql.exec.FileSinkOperator,org.apache.hadoop.hive.ql.exec.MoveTask,org.apache.hadoop.hive.ql.exec.Task,org.apache.hadoop.hive.conf.HiveConf)>
<org.apache.hadoop.hive.metastore.columnstats.merge.LongColumnStatsMerger: void merge(org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj,org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listPrincipalMPartitionColumnGrants(java.lang.String,org.apache.hadoop.hive.metastore.api.PrincipalType,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hive.beeline.HiveSchemaTool: java.util.List findCreateTable(java.lang.String,java.util.List)>
<org.apache.hadoop.hive.llap.security.LlapServerSecurityInfo: org.apache.hadoop.security.token.TokenInfo getTokenInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.ql.io.CodecPool: org.apache.hadoop.io.compress.Decompressor getDecompressor(org.apache.hadoop.io.compress.CompressionCodec)>
<org.apache.hive.service.server.ThreadWithGarbageCleanup: void cacheThreadLocalRawStore()>
<org.apache.hadoop.hive.ql.plan.LoadTableDesc: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx,org.apache.hadoop.hive.ql.io.AcidUtils$Operation,boolean,java.lang.Long)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyMultiKeyOperator: void process(java.lang.Object,int)>
<org.apache.hadoop.hive.ql.session.SessionState: void setupAuth()>
<org.apache.hadoop.hive.registry.impl.ZkRegistryBase: java.util.Set getByHostInternal(java.lang.String)>
<org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader: void close()>
<org.apache.hadoop.hive.ql.metadata.Hive: java.util.Map getPartitionColumnStatistics(java.lang.String,java.lang.String,java.util.List,java.util.List)>
<org.apache.hive.common.util.HiveVersionInfo: void main(java.lang.String[])>
<org.apache.hadoop.hive.metastore.cache.CachedStore$CacheUpdateMasterWork: void updateTables(org.apache.hadoop.hive.metastore.RawStore,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.hbase.HBaseTableSnapshotInputFormatUtil: org.apache.hadoop.mapred.InputSplit createTableSnapshotRegionSplit()>
<org.apache.hadoop.hive.ql.parse.TezCompiler: void generateTaskTree(java.util.List,org.apache.hadoop.hive.ql.parse.ParseContext,java.util.List,java.util.Set,java.util.Set)>
<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: void setSearchArgument(org.apache.orc.Reader$Options,java.util.List,org.apache.hadoop.conf.Configuration,boolean)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse heartbeatTxnRange(org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeRequest)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listMPartitions(java.lang.String,java.lang.String,java.lang.String,int,org.apache.hadoop.hive.metastore.ObjectStore$QueryWrapper)>
<org.apache.hadoop.hive.ql.parse.CalcitePlanner: org.apache.hadoop.hive.ql.exec.Operator getOptimizedHiveOPDag()>
<org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler: void processRow(java.lang.Object,java.util.Iterator)>
<org.apache.hadoop.hive.upgrade.acid.UpgradeTool: void makeCompactionScript(java.util.List,java.lang.String,org.apache.hadoop.hive.upgrade.acid.UpgradeTool$CompactionMetaInfo)>
<org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils: org.apache.hadoop.fs.Path createMoveTask(org.apache.hadoop.hive.ql.exec.Task,boolean,org.apache.hadoop.hive.ql.exec.FileSinkOperator,org.apache.hadoop.hive.ql.parse.ParseContext,java.util.List,org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.exec.DependencyCollectionTask)>
<org.apache.hadoop.hive.ql.exec.vector.VectorizationContext: org.apache.hadoop.hive.common.type.HiveDecimal castConstantToDecimal(java.lang.Object,org.apache.hadoop.hive.serde2.typeinfo.TypeInfo)>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.OperationHandle executeStatement(org.apache.hive.service.cli.SessionHandle,java.lang.String,java.util.Map)>
<org.apache.hadoop.hive.metastore.txn.AcidWriteSetService: void run()>
<org.apache.hive.storage.jdbc.JdbcRecordReader: void <init>(org.apache.hadoop.mapred.JobConf,org.apache.hive.storage.jdbc.JdbcInputSplit)>
<org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator$LlapTaskUmbilicalProtocolImpl: void nodeHeartbeat(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,int,org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol$TezAttemptArray,org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol$BooleanArray)>
<org.apache.hadoop.hive.accumulo.HiveAccumuloHelper: org.apache.hadoop.security.token.Token setConnectorInfoForInputAndOutput(org.apache.hadoop.hive.accumulo.AccumuloConnectionParameters,org.apache.accumulo.core.client.Connector,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl: void getCacheDataForOneSlice(int,org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl$FileData,org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl$FileData,org.apache.hadoop.hive.common.io.DataCache$BooleanRef,boolean[],org.apache.hadoop.hive.llap.cache.LowLevelCacheCounters)>
<org.apache.hadoop.hive.metastore.repl.DumpDirCleanerTask: void run()>
<org.apache.hadoop.hive.metastore.HiveMetaStoreClient: org.apache.hadoop.hive.metastore.api.NotificationEventResponse getNextNotification(long,int,org.apache.hadoop.hive.metastore.IMetaStoreClient$NotificationFilter)>
<org.apache.hadoop.hive.ql.exec.tez.TezProcessor: void run(java.util.Map,java.util.Map)>
<org.apache.hadoop.hive.accumulo.AccumuloStorageHandler: void configureJobConf(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.mapred.JobConf)>
<org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>
<org.apache.hadoop.hive.ql.metadata.events.NotificationEventPoll: void <init>(org.apache.hadoop.conf.Configuration)>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.OperationHandle getTypeInfo(org.apache.hive.service.cli.SessionHandle)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterLongOperator: void process(java.lang.Object,int)>
<org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager: org.apache.hadoop.hive.ql.lockmgr.HiveLock lock(org.apache.hadoop.hive.ql.lockmgr.HiveLockObject,org.apache.hadoop.hive.ql.lockmgr.HiveLockMode,boolean)>
<org.apache.hadoop.hive.metastore.ObjectStore: void updateMasterKey(java.lang.Integer,java.lang.String)>
<org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction: org.apache.calcite.rel.RelNode genOBLogicalPlan(org.apache.hadoop.hive.ql.parse.QB,org.apache.calcite.util.Pair,boolean)>
<org.apache.hadoop.hive.druid.DruidStorageHandler: void preCreateTable(org.apache.hadoop.hive.metastore.api.Table)>
<org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer: org.apache.hadoop.fs.Path getFinalPath(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.OperationHandle getPrimaryKeys(org.apache.hive.service.cli.SessionHandle,java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.io.HdfsUtils: void setFullFileStatus(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.io.HdfsUtils$HadoopFileStatus,java.lang.String,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.fs.FsShell)>
<org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$SelectStatsRule: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: void seedWriteIdOnAcidConversion(org.apache.hadoop.hive.metastore.api.InitializeTableWriteIdsRequest)>
<org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl: long[] putFileData(java.lang.Object,org.apache.hadoop.hive.common.io.DiskRange[],org.apache.hadoop.hive.common.io.encoded.MemoryBuffer[],long,org.apache.hadoop.hive.llap.cache.LowLevelCache$Priority,org.apache.hadoop.hive.llap.cache.LowLevelCacheCounters,java.lang.String)>
<org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler$Shuffle: void registerAttemptDirs(org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler$AttemptPathIdentifier,org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler$AttemptPathInfo)>
<org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader: boolean determineRgsToRead(int,java.util.ArrayList)>
<org.apache.hadoop.hive.metastore.security.DBTokenStore: org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation getToken(org.apache.hadoop.hive.metastore.security.DelegationTokenIdentifier)>
<org.apache.hadoop.hive.conf.VariableSubstitution: java.lang.String substitute(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>
<org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter: org.apache.hadoop.hive.ql.exec.SelectOperator genReduceSinkAndBacktrackSelect(org.apache.hadoop.hive.ql.exec.Operator,org.apache.hadoop.hive.ql.plan.ExprNodeDesc[],int,java.util.ArrayList,java.lang.String,java.lang.String,int,org.apache.hadoop.hive.ql.io.AcidUtils$Operation,org.apache.hadoop.hive.conf.HiveConf,java.util.List)>
<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter: void commitJob(org.apache.hadoop.mapred.JobContext)>
<org.apache.hadoop.hive.ql.exec.DDLTask: int alterDatabase(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterDatabaseDesc)>
<org.apache.hadoop.hive.ql.exec.Utilities: org.apache.hadoop.hive.ql.plan.BaseWork getBaseWork(org.apache.hadoop.conf.Configuration,java.lang.String)>
<org.apache.hadoop.hive.llap.daemon.impl.AMReporter$QueueLookupCallable: java.lang.Void callInternal()>
<org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl: void putFileData(org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl$FileData,org.apache.hadoop.hive.llap.cache.LowLevelCache$Priority,org.apache.hadoop.hive.llap.cache.LowLevelCacheCounters,java.lang.String)>
<org.apache.hadoop.hive.ql.metadata.Hive: void moveAcidFiles(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.Path,java.util.List)>
<org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc: boolean checkConvertBucketMapJoin(org.apache.hadoop.hive.ql.optimizer.BucketJoinProcCtx,java.util.Map,java.util.Map,java.lang.String,java.util.List)>
<org.apache.hadoop.hive.ql.parse.TezCompiler$SemiJoinRemovalIfNoStatsProc: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List getAllTokenIdentifiers()>
<org.apache.hadoop.hive.ql.exec.Utilities: void addBucketFileIfMissing(java.util.List,java.util.HashMap,java.lang.String,org.apache.hadoop.fs.Path,int)>
<org.apache.hadoop.hive.ql.optimizer.physical.MemoryDecider$MemoryCalculator: void evaluateOperators(org.apache.hadoop.hive.ql.plan.BaseWork,org.apache.hadoop.hive.ql.optimizer.physical.PhysicalContext)>
<org.apache.hadoop.hive.metastore.HiveClientCache$2: void onRemoval(com.google.common.cache.RemovalNotification)>
<org.apache.hadoop.hive.ql.metadata.Partition: java.util.List getSkewedColNames()>
<org.apache.hadoop.hive.ql.exec.MapJoinOperator: void completeInitializationOp(java.lang.Object[])>
<org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat: java.util.List fetchLocatedSegmentDescriptors(java.lang.String,org.apache.hive.druid.io.druid.query.BaseQuery)>
<org.apache.hive.spark.client.RemoteDriver$DriverProtocol: void jobSubmitted(java.lang.String,int)>
<org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher: void pushFilters(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.exec.RowSchema,org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc)>
<org.apache.hadoop.hive.ql.parse.GenTezWork: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.ql.reexec.ReExecDriver: boolean shouldReExecuteAfterCompile(org.apache.hadoop.hive.ql.plan.mapper.PlanMapper,org.apache.hadoop.hive.ql.plan.mapper.PlanMapper)>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory: org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer get(org.apache.hadoop.hive.ql.QueryState,org.apache.hadoop.hive.ql.parse.ASTNode)>
<org.apache.hadoop.hive.ql.exec.tez.LlapObjectCache: java.lang.Object retrieve(java.lang.String,java.util.concurrent.Callable)>
<org.apache.hadoop.hive.ql.optimizer.TablePropertyEnrichmentOptimizer$WalkerCtx: void <init>(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.registry.impl.ServiceInstanceBase: void <init>(org.apache.hadoop.registry.client.types.ServiceRecord,java.lang.String)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: org.apache.hadoop.hive.metastore.txn.TxnHandler$LockInfo getTxnIdFromLockId(java.sql.Connection,long)>
<org.apache.hadoop.hive.metastore.metrics.PerfLogger: void PerfLogBegin(java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.metastore.ObjectStore: void addRuntimeStat(org.apache.hadoop.hive.metastore.api.RuntimeStat)>
<org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.JDBCRexCallValidator$JdbcRexCallValidatorVisitor: boolean validRexCall(org.apache.calcite.rex.RexCall)>
<org.apache.hadoop.hive.ql.cache.results.QueryResultsCache: org.apache.hadoop.hive.ql.cache.results.QueryResultsCache$CacheEntry lookup(org.apache.hadoop.hive.ql.cache.results.QueryResultsCache$LookupInfo)>
<org.apache.hive.beeline.HiveSchemaTool: void alterCatalog(java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.metastore.ObjectStore: void clearClr(org.datanucleus.ClassLoaderResolver)>
<org.apache.hadoop.hive.ql.exec.Operator: void defaultStartGroup()>
<org.apache.hadoop.hive.ql.exec.spark.SmallTableCache: org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer get(org.apache.hadoop.fs.Path)>
<org.apache.hive.jdbc.Utils: void configureConnParamsFromZooKeeper(org.apache.hive.jdbc.Utils$JdbcConnectionParams)>
<org.apache.hadoop.hive.ql.parse.repl.load.message.RenameTableHandler: java.util.List handle(org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandler$Context)>
<org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl: void decompressChunk(java.nio.ByteBuffer,org.apache.orc.CompressionCodec,java.nio.ByteBuffer)>
<org.apache.hadoop.hive.metastore.txn.AcidHouseKeeperService: void run()>
<org.apache.hadoop.hive.ql.exec.CopyTask: int copyOnePath(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
<org.apache.hive.service.cli.thrift.ThriftHttpServlet: javax.servlet.http.Cookie createCookie(java.lang.String)>
<org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization: void generateEventOperatorPlan(org.apache.hadoop.hive.ql.parse.GenTezUtils$DynamicListContext,org.apache.hadoop.hive.ql.parse.ParseContext,org.apache.hadoop.hive.ql.exec.TableScanOperator,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader: org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl$LlapSerDeDataBuffer[][] createArrayToCache(org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl$StripeData,int,java.util.List)>
<org.apache.hadoop.hive.llap.log.LlapWrappedAppender: void <init>(java.lang.String,org.apache.logging.log4j.core.config.Node,org.apache.logging.log4j.core.config.Configuration,boolean,java.lang.String)>
<org.apache.hadoop.hive.ql.exec.DDLTask: java.util.List generateAddMmTasks(org.apache.hadoop.hive.ql.metadata.Table,java.lang.Long)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listPrincipalPartitionGrantsAll(java.lang.String,org.apache.hadoop.hive.metastore.api.PrincipalType)>
<org.apache.hadoop.hive.ql.parse.repl.dump.events.AddNotNullConstraintHandler: void handle(org.apache.hadoop.hive.ql.parse.repl.dump.events.EventHandler$Context)>
<org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory$UnionNoProcessFile: void pushOperatorsAboveUnion(org.apache.hadoop.hive.ql.exec.UnionOperator,java.util.Stack,int)>
<org.apache.hadoop.hive.ql.util.ResourceDownloader: java.lang.String downloadResource(java.net.URI,java.lang.String,boolean)>
<org.apache.hadoop.hive.ql.exec.Utilities: void handleMmTableFinalPath(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration,boolean,int,int,org.apache.hadoop.hive.ql.exec.Utilities$MissingBucketsContext,long,int,org.apache.hadoop.mapred.Reporter,boolean,boolean,boolean)>
<org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$WaitQueueWorker: void run()>
<org.apache.hadoop.hive.metastore.MetaStoreDirectSql: void getStatsTableListResult(java.lang.String,java.util.List)>
<org.apache.hadoop.hive.common.FileUtils: boolean moveToTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,boolean)>
<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Task loadTable(java.net.URI,org.apache.hadoop.hive.ql.metadata.Table,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.parse.ReplicationSpec,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext,java.lang.Long,int)>
<org.apache.hadoop.hive.upgrade.acid.UpgradeTool: void prepareAcidUpgradeInternal(java.lang.String,boolean,boolean,boolean)>
<org.apache.hadoop.hive.ql.lockmgr.DbTxnManager: org.apache.hadoop.hive.ql.lockmgr.DbTxnManager$Heartbeater startHeartbeat(long)>
<org.apache.hadoop.hive.metastore.cache.SharedCache$TableWrapper: void refreshAggrPartitionColStats(org.apache.hadoop.hive.metastore.api.AggrStats,org.apache.hadoop.hive.metastore.api.AggrStats)>
<org.apache.hadoop.hive.ql.exec.DDLTask: int addConstraints(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.AlterTableDesc)>
<org.apache.hadoop.hive.ql.parse.TableMask: java.lang.String create(org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject,org.apache.hadoop.hive.ql.parse.MaskAndFilterInfo)>
<org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler: void setRunAs(long,java.lang.String)>
<org.apache.hadoop.hive.ql.parse.repl.load.message.DropFunctionHandler: java.util.List handle(org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandler$Context)>
<org.apache.hadoop.hive.ql.exec.FileSinkOperator: void jobCloseOp(org.apache.hadoop.conf.Configuration,boolean)>
<org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead: boolean doReadField(org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead$Field)>
<org.apache.hadoop.hive.metastore.ReplChangeManager: int recycle(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.metastore.ReplChangeManager$RecycleType,boolean)>
<org.apache.hadoop.hive.ql.exec.MapredContext: org.apache.hadoop.hive.ql.exec.MapredContext init(boolean,org.apache.hadoop.mapred.JobConf)>
<org.apache.hadoop.hive.ql.metadata.Hive: void constructOneLBLocationMap(org.apache.hadoop.fs.FileStatus,java.util.Map,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.metastore.api.SkewedInfo)>
<org.apache.hive.service.servlet.QueryProfileServlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>
<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR: void runMmCompaction(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.metastore.api.Table,org.apache.hadoop.hive.metastore.api.Partition,org.apache.hadoop.hive.metastore.api.StorageDescriptor,org.apache.hadoop.hive.common.ValidWriteIdList,org.apache.hadoop.hive.metastore.txn.CompactionInfo)>
<org.apache.hadoop.hive.ql.metadata.events.NotificationEventPoll$Poller: void run()>
<org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin: boolean checkShuffleSizeForLargeTable(org.apache.hadoop.hive.ql.exec.JoinOperator,int,org.apache.hadoop.hive.ql.parse.OptimizeTezProcContext)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerStringOperator: void process(java.lang.Object,int)>
<org.apache.hadoop.hive.ql.exec.FetchOperator: org.apache.hadoop.hive.common.ValidWriteIdList extractValidWriteIdList()>
<org.apache.hadoop.hive.ql.exec.DDLTask: int renamePartition(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.RenamePartitionDesc)>
<org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer: void gatherDPPTableScanOps(org.apache.hadoop.hive.ql.parse.ParseContext,org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer$SharedWorkOptimizerCache)>
<org.apache.hive.service.auth.ldap.LdapSearchFactory: org.apache.hive.service.auth.ldap.DirSearch getInstance(org.apache.hadoop.hive.conf.HiveConf,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.ql.exec.tez.SessionExpirationTracker: void addToExpirationQueue(org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolSession)>
<org.apache.hadoop.hive.ql.exec.tez.WorkloadManager: void lambda$scheduleWork$3(org.apache.hadoop.hive.ql.exec.tez.WmTezSession,java.lang.String,org.apache.hadoop.hive.ql.exec.tez.WorkloadManager$KillQueryContext)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: long generateCompactionQueueId(java.sql.Statement)>
<org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter$HeartbeatCallable: org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter$ResponseWrapper heartbeat(java.util.Collection)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listMRoleMembers(java.lang.String)>
<org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService: java.util.List preemptTasksFromMap(java.util.TreeMap,int,int,int,java.lang.String[],java.util.Set,java.util.List)>
<org.apache.hadoop.hive.ql.exec.MapJoinOperator: void initializeOp(org.apache.hadoop.conf.Configuration)>
<org.apache.hive.spark.client.rpc.RpcDispatcher: void channelRead0(io.netty.channel.ChannelHandlerContext,java.lang.Object)>
<org.apache.hive.service.cli.thrift.ThriftCLIService: java.lang.String getUserName(org.apache.hive.service.rpc.thrift.TOpenSessionReq)>
<org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer: void decodeBatch(org.apache.hadoop.hive.ql.io.orc.encoded.Reader$OrcEncodedColumnBatch,org.apache.hadoop.hive.ql.io.orc.encoded.Consumer)>
<org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask: void dumpConstraintMetadata(java.lang.String,java.lang.String,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl: org.apache.hadoop.hive.ql.io.orc.encoded.CacheChunk prepareRangesForUncompressedRead(long,long,long,long,org.apache.hadoop.hive.common.io.DiskRangeList,org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch$ColumnStreamData)>
<org.apache.hadoop.hive.ql.parse.repl.load.message.DropTableHandler: java.util.List handle(org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandler$Context)>
<org.apache.hadoop.hive.metastore.utils.MetaStoreUtils: void mergeColStats(org.apache.hadoop.hive.metastore.api.ColumnStatistics,org.apache.hadoop.hive.metastore.api.ColumnStatistics)>
<org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>
<org.apache.hive.beeline.HiveSchemaTool: void moveTable(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.ql.exec.mr.ObjectCache: void release(java.lang.String)>
<org.apache.hive.spark.client.RemoteDriver$DriverProtocol: void handle(io.netty.channel.ChannelHandlerContext,org.apache.hive.spark.client.BaseProtocol$EndSession)>
<org.apache.hadoop.hive.ql.metadata.Hive: boolean isSubDir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileSystem,boolean)>
<org.apache.hadoop.hive.ql.parse.repl.load.message.TruncatePartitionHandler: java.util.List handle(org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandler$Context)>
<org.apache.hadoop.hive.llap.LlapBaseInputFormat: void closeAll()>
<org.apache.hadoop.hive.ql.exec.tez.LlapObjectCache: void remove(java.lang.String)>
<org.apache.hadoop.hive.ql.exec.DDLTask: int createView(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.CreateViewDesc)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: org.apache.hadoop.hive.metastore.api.TableValidWriteIds getValidWriteIdsForTable(java.sql.Statement,java.lang.String,org.apache.hadoop.hive.common.ValidTxnList)>
<org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService: void tryScheduleUnderLock(org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper)>
<org.apache.hadoop.hive.llap.security.LlapServerSecurityInfo: org.apache.hadoop.security.KerberosInfo getKerberosInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.ql.exec.DDLTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>
<org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler: boolean set_aggr_stats_for(org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest)>
<org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory: void foldOperator(org.apache.hadoop.hive.ql.exec.Operator,org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcCtx)>
<org.apache.hadoop.hive.ql.exec.mr.MapRedTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: java.lang.Integer genColListRegex(java.lang.String,java.lang.String,org.apache.hadoop.hive.ql.parse.ASTNode,java.util.ArrayList,java.util.HashSet,org.apache.hadoop.hive.ql.parse.RowResolver,org.apache.hadoop.hive.ql.parse.RowResolver,java.lang.Integer,org.apache.hadoop.hive.ql.parse.RowResolver,java.util.List,boolean)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listPrincipalTableColumnGrantsAll(java.lang.String,org.apache.hadoop.hive.metastore.api.PrincipalType)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: void commitTxn(org.apache.hadoop.hive.metastore.api.CommitTxnRequest)>
<org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkObjectHashOperator: void process(java.lang.Object,int)>
<org.apache.hive.spark.client.RemoteDriver$DriverProtocol: void sendError(java.lang.Throwable)>
<org.apache.hadoop.hive.metastore.cache.CachedStore: void prewarm(org.apache.hadoop.hive.metastore.RawStore)>
<org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer: void createReplImportTasks(org.apache.hadoop.hive.ql.plan.ImportTableDesc,java.util.List,org.apache.hadoop.hive.ql.parse.ReplicationSpec,boolean,org.apache.hadoop.hive.ql.metadata.Table,java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.hive.metastore.Warehouse,org.apache.hadoop.hive.ql.parse.EximUtil$SemanticAnalyzerWrapperContext,java.lang.Long,int,org.apache.hadoop.hive.ql.parse.repl.load.UpdatedMetaDataTracker)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: java.util.List getLockInfoFromLockId(java.sql.Connection,long)>
<org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher: boolean getOnlyStructObjectInspectors(org.apache.hadoop.hive.ql.plan.ReduceWork,org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorTaskColumnInfo)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List getTableMeta(java.lang.String,java.lang.String,java.lang.String,java.util.List)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator: void determineCommonInfo(boolean)>
<org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader: void <init>(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.io.orc.OrcSplit,org.apache.hadoop.mapred.Reporter,org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx,boolean)>
<org.apache.hadoop.hive.accumulo.HiveAccumuloHelper: void setInputFormatMockInstance(org.apache.hadoop.mapred.JobConf,java.lang.String)>
<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: java.util.List analyzeEventLoad(org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandler$Context)>
<org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$JoinStatsRule: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.ql.txn.compactor.CompactorThread: java.lang.String findUserToRunAs(java.lang.String,org.apache.hadoop.hive.metastore.api.Table)>
<org.apache.hadoop.hive.ql.util.IncrementalObjectSizeEstimator: void addMapEstimator(java.util.HashMap,java.util.Deque,java.lang.reflect.Field,java.lang.Class,java.lang.Object)>
<org.apache.hadoop.hive.metastore.security.MemoryTokenStore: void updateMasterKey(int,java.lang.String)>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genBodyPlan(org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator,java.util.Map)>
<org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer: java.util.List applyConstraintsAndGetFiles(java.net.URI,org.apache.hadoop.hive.ql.metadata.Table)>
<org.apache.hadoop.hive.common.ndv.fm.FMSketch: void printNumDistinctValueEstimator()>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listTableColumnGrantsAll(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.ql.exec.tez.Utils: org.apache.hadoop.mapred.split.SplitLocationProvider getSplitLocationProvider(org.apache.hadoop.conf.Configuration,boolean,org.slf4j.Logger)>
<org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager: org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLock lock(org.apache.hadoop.hive.ql.lockmgr.HiveLockObject,org.apache.hadoop.hive.ql.lockmgr.HiveLockMode,boolean,boolean)>
<org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry: org.apache.calcite.plan.RelOptMaterialization addMaterializedView(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry$OpType)>
<org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters: void <init>(org.apache.hadoop.conf.Configuration,java.util.Properties,java.lang.String)>
<org.apache.hadoop.hive.serde2.avro.AvroDeserializer: java.lang.Object deserialize(java.util.List,java.util.List,org.apache.hadoop.io.Writable,org.apache.avro.Schema)>
<org.apache.hadoop.hive.ql.parse.spark.SparkCompiler: void runCycleAnalysisForPartitionPruning(org.apache.hadoop.hive.ql.parse.spark.OptimizeSparkProcContext)>
<org.apache.hadoop.hive.ql.io.AcidUtils: org.apache.hadoop.hive.ql.io.AcidUtils$Directory getAcidState(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.common.ValidWriteIdList,org.apache.hive.common.util.Ref,boolean,java.util.Map)>
<org.apache.hadoop.hive.llap.ext.LlapTaskUmbilicalExternalClient$1: void setResponse(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$TerminateFragmentResponseProto)>
<org.apache.hadoop.hive.ql.plan.JoinCondDesc: java.lang.String getUserLevelJoinCondString()>
<org.apache.hive.service.CookieSigner: java.lang.String signCookie(java.lang.String)>
<org.apache.hadoop.hive.ql.optimizer.PartitionColumnsSeparator$StructInExprProcessor: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer: boolean compareAndGatherOps(org.apache.hadoop.hive.ql.parse.ParseContext,org.apache.hadoop.hive.ql.exec.Operator,org.apache.hadoop.hive.ql.exec.Operator,java.util.List,boolean)>
<org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator: java.lang.String aggregateStats(java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.ql.parse.MaterializedViewRebuildSemanticAnalyzer: void analyzeInternal(org.apache.hadoop.hive.ql.parse.ASTNode)>
<org.apache.hadoop.hive.llap.cli.LlapServiceDriver$1: java.lang.Void call()>
<org.apache.hadoop.hive.ql.Driver: void releasePlan()>
<org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService: org.apache.hadoop.yarn.api.records.Resource getAvailableResources()>
<org.apache.hadoop.hive.ql.io.parquet.ParquetRecordReaderBase: org.apache.parquet.filter2.compat.FilterCompat$Filter setFilter(org.apache.hadoop.mapred.JobConf,org.apache.parquet.schema.MessageType)>
<org.apache.hadoop.hive.ql.exec.Operator: void close(boolean)>
<org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor: org.apache.hadoop.hive.ql.plan.ExprNodeDesc getXpathOrFuncExprNodeDesc(org.apache.hadoop.hive.ql.parse.ASTNode,boolean,java.util.ArrayList,org.apache.hadoop.hive.ql.parse.TypeCheckCtx)>
<org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl: void releaseInitialRefcount(org.apache.hadoop.hive.ql.io.orc.encoded.CacheChunk,boolean)>
<org.apache.hadoop.hive.metastore.ThreadPool: org.apache.hadoop.hive.metastore.ThreadPool initialize(org.apache.hadoop.conf.Configuration)>
<org.apache.hive.spark.client.RemoteDriver: void <init>(java.lang.String[])>
<org.apache.hadoop.hive.ql.stats.StatsUtils: org.apache.hadoop.hive.ql.plan.Statistics collectStatistics(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.parse.PrunedPartitionList,org.apache.hadoop.hive.ql.metadata.Table,java.util.List,java.util.List,org.apache.hadoop.hive.ql.parse.ColumnStatsList,java.util.List,boolean,boolean)>
<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: boolean isStripeSatisfyPredicate(org.apache.orc.StripeStatistics,org.apache.hadoop.hive.ql.io.sarg.SearchArgument,int[],org.apache.orc.impl.SchemaEvolution)>
<org.apache.hadoop.hive.accumulo.AccumuloDefaultIndexScanner: java.util.List getIndexRowRanges(java.lang.String,org.apache.accumulo.core.data.Range)>
<org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin: boolean convertJoinBucketMapJoin(org.apache.hadoop.hive.ql.exec.JoinOperator,org.apache.hadoop.hive.ql.parse.OptimizeTezProcContext,int,org.apache.hadoop.hive.ql.optimizer.TezBucketJoinProcCtx)>
<org.apache.hadoop.hive.metastore.datasource.BoneCPDataSourceProvider: javax.sql.DataSource create(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.ql.stats.OperatorStatsReaderHook: void run(org.apache.hadoop.hive.ql.hooks.HookContext)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: void cleanupRecords(org.apache.hadoop.hive.metastore.api.HiveObjectType,org.apache.hadoop.hive.metastore.api.Database,org.apache.hadoop.hive.metastore.api.Table,java.util.Iterator)>
<org.apache.hadoop.hive.ql.exec.ReplCopyTask: int execute(org.apache.hadoop.hive.ql.DriverContext)>
<org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService: void shutDown(boolean)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listPartitionNamesPs(java.lang.String,java.lang.String,java.lang.String,java.util.List,short)>
<org.apache.hive.service.CookieSigner: java.lang.String verifyAndExtract(java.lang.String)>
<org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter$HeartbeatCallable: boolean taskTerminated(org.apache.tez.dag.records.TezTaskAttemptID,boolean,org.apache.tez.runtime.api.TaskFailureType,java.lang.Throwable,java.lang.String,org.apache.tez.runtime.api.impl.EventMetaData)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiMultiKeyOperator: void process(java.lang.Object,int)>
<org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter: org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter$OpAttr visit(org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveSortExchange)>
<org.apache.hadoop.hive.metastore.MaterializationsRebuildLockCleanerTask: void run()>
<org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin: boolean checkNumberOfEntriesForHashTable(org.apache.hadoop.hive.ql.exec.JoinOperator,int,org.apache.hadoop.hive.ql.parse.OptimizeTezProcContext)>
<org.apache.hadoop.hive.serde2.lazy.LazyDouble: void init(org.apache.hadoop.hive.serde2.lazy.ByteArrayRef,int,int)>
<org.apache.hive.spark.client.SparkClientImpl$ClientProtocol: org.apache.hive.spark.client.JobHandleImpl submit(org.apache.hive.spark.client.Job,java.util.List)>
<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: org.apache.hadoop.yarn.api.records.LocalResource createJarLocalResource(java.lang.String)>
<org.apache.hadoop.hive.accumulo.HiveAccumuloHelper: void setOutputFormatZooKeeperInstance(org.apache.hadoop.mapred.JobConf,java.lang.String,java.lang.String,boolean)>
<org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl: void releaseBuffer(java.nio.ByteBuffer,boolean)>
<org.apache.hadoop.hive.metastore.utils.HdfsUtils$HadoopFileStatus: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)>
<org.apache.hive.spark.client.rpc.RpcDispatcher: void handleCall(io.netty.channel.ChannelHandlerContext,java.lang.Object)>
<org.apache.hadoop.hive.druid.DruidStorageHandler: org.apache.hive.druid.io.druid.metadata.SQLMetadataConnector buildConnector()>
<org.apache.hadoop.hive.ql.parse.repl.dump.events.AddForeignKeyHandler: void handle(org.apache.hadoop.hive.ql.parse.repl.dump.events.EventHandler$Context)>
<org.apache.hadoop.hive.accumulo.mr.AccumuloIndexedOutputFormat$AccumuloRecordWriter: java.util.List getIndexMutations(org.apache.accumulo.core.data.Mutation)>
<org.apache.hadoop.hive.ql.exec.Operator: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector,int)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: org.apache.hadoop.hive.metastore.api.OpenTxnsResponse openTxns(org.apache.hadoop.hive.metastore.api.OpenTxnRequest)>
<org.apache.hadoop.hive.ql.exec.tez.SplitGrouper: boolean schemaEvolved(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.InputSplit,boolean,org.apache.hadoop.hive.ql.plan.MapWork)>
<org.apache.hadoop.hive.druid.DruidStorageHandler: org.apache.hadoop.hive.druid.json.KafkaSupervisorSpec fetchKafkaIngestionSpec(org.apache.hadoop.hive.metastore.api.Table)>
<org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient: java.util.Map getTempTables(java.lang.String)>
<org.apache.hadoop.hive.llap.daemon.impl.QueryTracker: org.apache.hadoop.hive.llap.daemon.impl.QueryFragmentInfo registerFragment(org.apache.hadoop.hive.llap.daemon.impl.QueryIdentifier,java.lang.String,java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,int,int,java.lang.String,org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SignableVertexSpec,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hive.llap.daemon.impl.LlapTokenChecker$LlapTokenInfo,org.apache.hadoop.hive.llap.LlapNodeId)>
<org.apache.hive.service.cli.CLIService: void closeSession(org.apache.hive.service.cli.SessionHandle)>
<org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager: void unregisterOpenSession(org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolSession)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyLongOperator: void process(java.lang.Object,int)>
<org.apache.hadoop.hive.metastore.columnstats.merge.DecimalColumnStatsMerger: void merge(org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj,org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj)>
<org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService: void disableNode(org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService$NodeInfo,boolean)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listPrincipalPartitionColumnGrantsAll(java.lang.String,org.apache.hadoop.hive.metastore.api.PrincipalType)>
<org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader: boolean processOneSlice(org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader$CacheWriter$CacheStripeData,boolean[],int,org.apache.hadoop.hive.llap.cache.SerDeLowLevelCacheImpl$StripeData,long)>
<org.apache.hadoop.hive.metastore.datasource.HikariCPDataSourceProvider: javax.sql.DataSource create(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.metastore.HiveAlterHandler: void alterTableUpdateTableColumnStats(org.apache.hadoop.hive.metastore.RawStore,org.apache.hadoop.hive.metastore.api.Table,org.apache.hadoop.hive.metastore.api.Table)>
<org.apache.hadoop.hive.ql.txn.compactor.Worker$StatsUpdater: void gatherStats()>
<org.apache.hadoop.hive.ql.metadata.Hive: void moveAcidFiles(java.lang.String,org.apache.hadoop.fs.PathFilter,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Set,java.util.List)>
<org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask: org.apache.hadoop.hive.metastore.api.Date readDateValue(java.lang.String)>
<org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveCostModel: org.apache.calcite.plan.RelOptCost getJoinCost(org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin)>
<org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.DatabaseEventsIterator: org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.BootstrapEvent next()>
<org.apache.hadoop.hive.ql.parse.repl.load.message.AddNotNullConstraintHandler: java.util.List handle(org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandler$Context)>
<org.apache.hadoop.hive.metastore.txn.TxnDbUtil: boolean dropTable(java.sql.Statement,java.lang.String,int)>
<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: java.util.List addUpdateReplStateTasks(boolean,org.apache.hadoop.hive.ql.parse.repl.load.UpdatedMetaDataTracker,java.util.List)>
<org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater: org.apache.orc.TypeDescription getTypeDescriptionFromTableProperties(java.util.Properties)>
<org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat: org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter: org.apache.hadoop.hive.ql.exec.JoinOperator genJoin(org.apache.calcite.rel.RelNode,org.apache.hadoop.hive.ql.plan.ExprNodeDesc[][],java.util.List,java.util.List,java.lang.String[],java.lang.String)>
<org.apache.hive.http.JMXJsonServlet: void writeAttribute(org.codehaus.jackson.JsonGenerator,javax.management.ObjectName,javax.management.MBeanAttributeInfo)>
<org.apache.hadoop.hive.metastore.ObjectStore: void debugLog(java.lang.String)>
<org.apache.hadoop.hive.ql.parse.TypeCheckCtx: void setError(java.lang.String,org.apache.hadoop.hive.ql.parse.ASTNode)>
<org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkEmptyKeyOperator: void process(java.lang.Object,int)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator: void displayBatchColumns(org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch,java.lang.String)>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genNotNullFilterForJoinSourcePlan(org.apache.hadoop.hive.ql.parse.QB,org.apache.hadoop.hive.ql.exec.Operator,org.apache.hadoop.hive.ql.parse.QBJoinTree,org.apache.hadoop.hive.ql.plan.ExprNodeDesc[])>
<org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization: boolean disableSemiJoinOptDueToExternalTable(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.exec.TableScanOperator,org.apache.hadoop.hive.ql.parse.GenTezUtils$DynamicListContext)>
<org.apache.hadoop.hive.ql.Context: org.apache.hadoop.fs.Path getStagingDir(org.apache.hadoop.fs.Path,boolean)>
<org.apache.hadoop.hive.llap.cli.LlapStatusServiceDriver: int run(org.apache.hadoop.hive.llap.cli.LlapStatusOptionsProcessor$LlapStatusOptions,long)>
<org.apache.hadoop.hive.metastore.ObjectStore: org.apache.hadoop.hive.metastore.ObjectStore$AttachedMTableInfo getMTable(java.lang.String,java.lang.String,java.lang.String,boolean)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listPrincipalAllTableGrants(java.lang.String,org.apache.hadoop.hive.metastore.api.PrincipalType,org.apache.hadoop.hive.metastore.ObjectStore$QueryWrapper)>
<org.apache.hadoop.hive.ql.util.HiveStrictManagedMigration: void processTable(org.apache.hadoop.hive.metastore.api.Database,java.lang.String,boolean)>
<org.apache.hive.service.auth.ldap.GroupFilterFactory$UserMembershipKeyFilter: void apply(org.apache.hive.service.auth.ldap.DirSearch,java.lang.String)>
<org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter: org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter$OpAttr visit(org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveFilter)>
<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils: org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.RequiredPrivileges getPrivilegesFromFS(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>
<org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader$CacheWriter: void discardData()>
<org.apache.hadoop.hive.ql.optimizer.spark.SetSparkReducerParallelism: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader: void returnData(org.apache.hadoop.hive.ql.io.orc.encoded.Reader$OrcEncodedColumnBatch)>
<org.apache.hive.hcatalog.data.HCatRecordObjectInspectorFactory: org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector getStandardObjectInspectorFromTypeInfo(org.apache.hadoop.hive.serde2.typeinfo.TypeInfo)>
<org.apache.hive.hcatalog.common.HCatUtil: void closeHiveClientQuietly(org.apache.hadoop.hive.metastore.IMetaStoreClient)>
<org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$ColumnizedDeleteEventRegistry: void <init>(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.hive.ql.io.orc.OrcSplit,org.apache.orc.Reader$Options)>
<org.apache.hadoop.hive.common.StatsSetupConst: void setColumnStatsState(java.util.Map,java.util.List)>
<org.apache.hadoop.hive.metastore.ObjectStore: boolean addToken(java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.llap.LlapBaseRecordReader: void handleEvent(org.apache.hadoop.hive.llap.LlapBaseRecordReader$ReaderEvent)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse getOpenTxnsInfo()>
<org.apache.hadoop.hive.ql.parse.repl.load.message.OpenTxnHandler: java.util.List handle(org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandler$Context)>
<org.apache.hadoop.hive.io.HdfsUtils: void run(org.apache.hadoop.fs.FsShell,java.lang.String[])>
<org.apache.hadoop.hive.llap.log.LlapRoutingAppenderPurgePolicy: org.apache.logging.log4j.core.appender.routing.PurgePolicy createPurgePolicy(java.lang.String)>
<org.apache.hadoop.hive.llap.AsyncPbRpcProxy: java.lang.Object createProxy(org.apache.hadoop.hive.llap.LlapNodeId,org.apache.hadoop.security.token.Token)>
<org.apache.hadoop.hive.hbase.HBaseSerDeHelper: void generateColumns(java.util.Properties,java.util.List,java.lang.StringBuilder)>
<org.apache.hadoop.hive.ql.parse.TezCompiler: void runCycleAnalysisForPartitionPruning(org.apache.hadoop.hive.ql.parse.OptimizeTezProcContext,java.util.Set,java.util.Set)>
<org.apache.hadoop.hive.metastore.datasource.BoneCPDataSourceProvider: boolean supports(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.ql.metadata.Hive: void setStatsPropAndAlterPartition(boolean,org.apache.hadoop.hive.ql.metadata.Table,org.apache.hadoop.hive.ql.metadata.Partition)>
<org.apache.hadoop.hive.ql.optimizer.physical.AnnotateRunTimeStatsOptimizer: void annotateRuntimeStats(org.apache.hadoop.hive.ql.exec.Operator,org.apache.hadoop.hive.ql.parse.ParseContext)>
<org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin: boolean selectJoinForLlap(org.apache.hadoop.hive.ql.parse.OptimizeTezProcContext,org.apache.hadoop.hive.ql.exec.JoinOperator,org.apache.hadoop.hive.ql.optimizer.TezBucketJoinProcCtx,org.apache.hadoop.hive.ql.optimizer.physical.LlapClusterStateForCompile,int,int)>
<org.apache.hadoop.hive.ql.exec.DDLTask: int truncateTable(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.TruncateTableDesc)>
<org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService: boolean deallocateTask(java.lang.Object,boolean,org.apache.tez.serviceplugins.api.TaskAttemptEndReason,java.lang.String)>
<org.apache.hadoop.hive.metastore.AggregateStatsCache: void add(java.lang.String,java.lang.String,java.lang.String,java.lang.String,long,org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj,org.apache.hive.common.util.BloomFilter)>
<org.apache.hadoop.hive.metastore.ObjectStore$GetHelper: void handleDirectSqlError(java.lang.Exception)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.lang.String[] getMasterKeys()>
<org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler$Shuffle: org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler$Shuffle$MapOutputInfo getMapOutputInfo(java.lang.String,int,java.lang.String,int,java.lang.String)>
<org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator: void closeOp(boolean)>
<org.apache.hive.hcatalog.data.JsonSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>
<org.apache.hadoop.hive.metastore.ObjectStore: void dropPartitionsNoTxn(java.lang.String,java.lang.String,java.lang.String,java.util.List)>
<org.apache.hadoop.hive.metastore.MetaStoreDirectSql: org.apache.hadoop.hive.metastore.api.AggrStats aggrColStatsForPartitions(java.lang.String,java.lang.String,java.lang.String,java.util.List,java.util.List,boolean,double,boolean)>
<org.apache.hadoop.hive.ql.parse.GenTezWork: void connectUnionWorkWithWork(org.apache.hadoop.hive.ql.plan.UnionWork,org.apache.hadoop.hive.ql.plan.BaseWork,org.apache.hadoop.hive.ql.plan.TezWork,org.apache.hadoop.hive.ql.parse.GenTezProcContext)>
<org.apache.hadoop.hive.ql.exec.GroupByOperator: boolean shouldBeFlushed(org.apache.hadoop.hive.ql.exec.KeyWrapper)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listTableAllColumnGrants(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinToMultiJoinRule: org.apache.calcite.rel.RelNode mergeJoin(org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin,org.apache.calcite.rel.RelNode,org.apache.calcite.rel.RelNode)>
<org.apache.hadoop.hive.ql.lockmgr.DbTxnManager: org.apache.hadoop.hive.metastore.api.LockState acquireLocks(org.apache.hadoop.hive.ql.QueryPlan,org.apache.hadoop.hive.ql.Context,java.lang.String,boolean)>
<org.apache.hadoop.hive.hbase.HBaseSerDe: void initialize(org.apache.hadoop.conf.Configuration,java.util.Properties)>
<org.apache.hadoop.hive.ql.parse.repl.load.message.DropConstraintHandler: java.util.List handle(org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandler$Context)>
<org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$DataWrapperForOrc: org.apache.hadoop.hive.common.io.DiskRangeList readFileData(org.apache.hadoop.hive.common.io.DiskRangeList,long,boolean)>
<org.apache.hadoop.hive.ql.parse.repl.dump.PartitionExport: void write(org.apache.hadoop.hive.ql.parse.ReplicationSpec)>
<org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter: org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter$OpAttr genPTF(org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter$OpAttr,org.apache.hadoop.hive.ql.parse.WindowingSpec)>
<org.apache.hadoop.hive.metastore.utils.MetaStoreUtils: void populateQuickStats(java.util.List,java.util.Map)>
<org.apache.hadoop.hive.ql.txn.compactor.Initiator: void run()>
<org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader: boolean sendEcbToConsumer(org.apache.hadoop.hive.ql.io.orc.encoded.Reader$OrcEncodedColumnBatch,boolean,org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader$CacheWriter$CacheStripeData)>
<org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$GroupByStatsRule: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.metastore.tools.MetastoreSchemaTool: void oracleCreateUserHack(java.io.File)>
<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: boolean shouldReplayEvent(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.hive.ql.parse.repl.DumpType)>
<org.apache.hadoop.hive.metastore.cache.SharedCache: boolean populateTableInCache(org.apache.hadoop.hive.metastore.api.Table,org.apache.hadoop.hive.metastore.api.ColumnStatistics,java.util.List,java.util.List,org.apache.hadoop.hive.metastore.api.AggrStats,org.apache.hadoop.hive.metastore.api.AggrStats)>
<org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator: void changeToStreamingMode()>
<org.apache.hadoop.hive.metastore.columnstats.merge.DateColumnStatsMerger: void merge(org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj,org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj)>
<org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForASTConv: org.apache.calcite.rel.RelNode convertOpTree(org.apache.calcite.rel.RelNode,java.util.List,boolean)>
<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR: org.apache.hadoop.mapred.JobConf createBaseJobConf(org.apache.hadoop.hive.conf.HiveConf,java.lang.String,org.apache.hadoop.hive.metastore.api.Table,org.apache.hadoop.hive.metastore.api.StorageDescriptor,org.apache.hadoop.hive.common.ValidWriteIdList,org.apache.hadoop.hive.metastore.txn.CompactionInfo)>
<org.apache.hadoop.hive.llap.cli.LlapServiceDriver: int runPackagePy(java.lang.String[],org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.hbase.HBaseStorageHandler: java.lang.Class getInputFormatClass()>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listDatabaseGrants(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.hive.metastore.ObjectStore$QueryWrapper)>
<org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl: org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl$ProcCacheChunk addOneCompressionBlockByteBuffer(java.nio.ByteBuffer,boolean,long,long,int,org.apache.orc.impl.BufferChunk,java.util.List,java.util.List,boolean)>
<org.apache.hadoop.hive.ql.util.HiveStrictManagedMigration: boolean migrateToManagedTable(org.apache.hadoop.hive.metastore.api.Table,org.apache.hadoop.hive.metastore.TableType)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: void acquire(java.sql.Connection,java.sql.Statement,java.util.List)>
<org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles: java.util.List getTasks(org.apache.hadoop.hive.conf.HiveConf,java.lang.Object)>
<org.apache.hadoop.hive.ql.parse.TezCompiler: void removeSemijoinOptimizationFromSMBJoins(org.apache.hadoop.hive.ql.parse.OptimizeTezProcContext)>
<org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer: void analyzeReplLoad(org.apache.hadoop.hive.ql.parse.ASTNode)>
<org.apache.hadoop.hive.metastore.HiveMetaStoreClient: void close()>
<org.apache.hive.hcatalog.listener.DbNotificationListener$CleanerThread: void run()>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator: void reProcessBigTable(int)>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: void walkASTMarkTABREF(org.apache.hadoop.hive.ql.parse.TableMask,org.apache.hadoop.hive.ql.parse.ASTNode,java.util.Set,org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.metadata.Hive,java.util.Map,java.util.Set)>
<org.apache.hadoop.hive.ql.exec.FunctionRegistry: org.apache.hadoop.hive.ql.udf.generic.GenericUDAFResolver getGenericUDAFResolver(java.lang.String)>
<org.apache.hadoop.hive.ql.exec.FileSinkOperator: void process(java.lang.Object,int)>
<org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon: void initializeLogging(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker: void checkPartitionDirs(org.apache.hadoop.fs.Path,java.util.Set,java.util.List)>
<org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler: boolean checkFailedCompactions(org.apache.hadoop.hive.metastore.txn.CompactionInfo)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: void onRename(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.ql.exec.Operator: boolean allInitializedParentsAreClosed()>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: void unlock(org.apache.hadoop.hive.metastore.api.UnlockRequest)>
<org.apache.hive.beeline.HiveSchemaTool: void runBeeLine(java.lang.String)>
<org.apache.hadoop.hive.ql.exec.mr.Throttle: void checkJobTracker(org.apache.hadoop.mapred.JobConf,org.slf4j.Logger)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: void abortTxns(org.apache.hadoop.hive.metastore.api.AbortTxnsRequest)>
<org.apache.hadoop.hive.ql.exec.persistence.FlatRowContainer: void addRow(java.util.List)>
<org.apache.hive.spark.client.rpc.SaslHandler: void channelRead0(io.netty.channel.ChannelHandlerContext,org.apache.hive.spark.client.rpc.Rpc$SaslMessage)>
<org.apache.hadoop.hive.ql.txn.compactor.Initiator: org.apache.hadoop.hive.metastore.api.CompactionType checkForCompaction(org.apache.hadoop.hive.metastore.txn.CompactionInfo,org.apache.hadoop.hive.common.ValidWriteIdList,org.apache.hadoop.hive.metastore.api.StorageDescriptor,java.util.Map,java.lang.String)>
<org.apache.hadoop.hive.metastore.security.DBTokenStore: void updateMasterKey(int,java.lang.String)>
<org.apache.hadoop.hive.serde2.avro.AvroLazyObjectInspector: java.lang.Object getStructFieldData(java.lang.Object,org.apache.hadoop.hive.serde2.objectinspector.StructField)>
<org.apache.hive.storage.jdbc.JdbcStorageHandler: void configureInputJobCredentials(org.apache.hadoop.hive.ql.plan.TableDesc,java.util.Map)>
<org.apache.hadoop.hive.serde2.avro.InstanceCache: java.lang.Object retrieve(java.lang.Object,java.util.Set)>
<org.apache.hive.jdbc.logs.InPlaceUpdateStream$EventNotifier: void progressBarCompleted()>
<org.apache.hadoop.hive.ql.parse.GenTezUtils: void processFileSink(org.apache.hadoop.hive.ql.parse.GenTezProcContext,org.apache.hadoop.hive.ql.exec.FileSinkOperator)>
<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR: void launchCompactionJob(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,org.apache.hadoop.hive.metastore.api.CompactionType,org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$StringableList,java.util.List,int,int,org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.metastore.txn.TxnStore,long,java.lang.String)>
<org.apache.hadoop.hive.ql.stats.BasicStatsNoJobTask: void shutdownAndAwaitTermination(java.util.concurrent.ExecutorService)>
<org.apache.hadoop.hive.ql.exec.DDLTask: int createTable(org.apache.hadoop.hive.ql.metadata.Hive,org.apache.hadoop.hive.ql.plan.CreateTableDesc)>
<org.apache.hadoop.hive.ql.optimizer.StatsOptimizer$MetaDataProcessor: org.apache.hadoop.hive.metastore.api.ColumnStatisticsData validateSingleColStat(java.util.List)>
<org.apache.hadoop.hive.ql.stats.StatsUpdaterThread: void addPreviousPartitions(org.apache.hadoop.hive.metastore.api.Table,java.util.List,int,java.util.List,int,java.util.List,java.util.Map)>
<org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter: org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter$OpAttr visit(org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveTableScan)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: org.apache.hadoop.hive.metastore.api.Materialization getMaterializationInvalidationInfo(org.apache.hadoop.hive.metastore.api.CreationMetadata,java.lang.String)>
<org.apache.hadoop.hive.ql.stats.StatsUtils: long getNumRows(org.apache.hadoop.hive.conf.HiveConf,java.util.List,org.apache.hadoop.hive.ql.metadata.Table,long)>
<org.apache.hadoop.hive.ql.ppd.PredicatePushDown: org.apache.hadoop.hive.ql.parse.ParseContext transform(org.apache.hadoop.hive.ql.parse.ParseContext)>
<org.apache.hadoop.hive.ql.metadata.Hive: java.util.List getValidMaterializedViews(java.lang.String,java.util.List,java.util.List,boolean)>
<org.apache.hadoop.hive.llap.cli.LlapStatusServiceDriver: void main(java.lang.String[])>
<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: boolean[] pickStripesInternal(org.apache.hadoop.hive.ql.io.sarg.SearchArgument,int[],java.util.List,int,org.apache.hadoop.fs.Path,org.apache.orc.impl.SchemaEvolution)>
<org.apache.hive.service.cli.session.HiveSessionImpl: void processGlobalInitFile()>
<org.apache.hadoop.hive.ql.exec.GroupByOperator: void process(java.lang.Object,int)>
<org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader: java.lang.Void performDataRead()>
<org.apache.hive.streaming.HiveStreamingConnection: void setHiveConf(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.conf.HiveConf$ConfVars,java.lang.String)>
<org.apache.hadoop.hive.ql.exec.mr.ExecReducer: void close()>
<org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler: void close()>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: void heartbeatLock(java.sql.Connection,long)>
<org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner$SourceInfo: void <init>(org.apache.hadoop.hive.ql.plan.TableDesc,org.apache.hadoop.hive.ql.plan.ExprNodeDesc,java.lang.String,java.lang.String,org.apache.hadoop.mapred.JobConf)>
<org.apache.hive.beeline.hs2connection.HiveSiteHS2ConnectionFileParser: void <init>()>
<org.apache.hadoop.hive.ql.plan.LoadFileDesc: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,java.lang.String,java.lang.String,org.apache.hadoop.hive.ql.io.AcidUtils$Operation,boolean)>
<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.ql.metadata.Partition loadPartition(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,org.apache.hadoop.hive.ql.plan.LoadTableDesc$LoadFileType,boolean,boolean,boolean,boolean,boolean,java.lang.Long,int,boolean)>
<org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator: void checkPrivileges(org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType,java.util.List,java.util.List,org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzContext)>
<org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor: java.lang.Class getVectorExpressionClass(java.lang.Class,org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor$Descriptor,boolean)>
<org.apache.hadoop.hive.metastore.columnstats.aggr.LongColumnStatsAggregator: org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj aggregate(java.util.List,java.util.List,boolean)>
<org.apache.hadoop.hive.ql.optimizer.ReduceSinkMapJoinProc: java.lang.Object processReduceSinkToHashJoin(org.apache.hadoop.hive.ql.exec.ReduceSinkOperator,org.apache.hadoop.hive.ql.exec.MapJoinOperator,org.apache.hadoop.hive.ql.parse.GenTezProcContext)>
<org.apache.hadoop.hive.ql.io.NullRowsInputFormat$NullRowsRecordReader: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.InputSplit)>
<org.apache.hadoop.hive.metastore.utils.SecurityUtils: org.apache.thrift.transport.TServerSocket getServerSSLSocket(java.lang.String,int,java.lang.String,java.lang.String,java.util.List)>
<org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable$TaskRunnerCallback: void onSuccess(org.apache.tez.runtime.task.TaskRunner2Result)>
<org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization: void createFinalRsForSemiJoinOp(org.apache.hadoop.hive.ql.parse.ParseContext,org.apache.hadoop.hive.ql.exec.TableScanOperator,org.apache.hadoop.hive.ql.exec.GroupByOperator,org.apache.hadoop.hive.ql.plan.ExprNodeDesc,java.lang.String,org.apache.hadoop.hive.ql.plan.ExprNodeDesc,boolean)>
<org.apache.hive.service.cli.thrift.ThriftCLIService: java.lang.String getIpAddress()>
<org.apache.hadoop.hive.ql.parse.GenTezUtils: void setupReduceSink(org.apache.hadoop.hive.ql.parse.GenTezProcContext,org.apache.hadoop.hive.ql.plan.ReduceWork,org.apache.hadoop.hive.ql.exec.ReduceSinkOperator)>
<org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Client$SaslClientCallbackHandler: void handle(javax.security.auth.callback.Callback[])>
<org.apache.hadoop.hive.metastore.utils.HdfsUtils: void setFullFileStatus(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.metastore.utils.HdfsUtils$HadoopFileStatus,java.lang.String,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.fs.FsShell)>
<org.apache.hadoop.hive.ql.plan.MoveWork: void <init>(java.util.HashSet,java.util.HashSet,org.apache.hadoop.hive.ql.plan.LoadTableDesc,org.apache.hadoop.hive.ql.plan.LoadFileDesc,boolean,boolean)>
<org.apache.hadoop.hive.ql.exec.tez.DynamicPartitionPruner: void applyFilterToPartitions(org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters$Converter,org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator,java.lang.String,java.util.Set)>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.OperationHandle executeStatementAsync(org.apache.hive.service.cli.SessionHandle,java.lang.String,java.util.Map,long)>
<org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveOnTezCostModel$TezSMBJoinAlgorithm: boolean isExecutable(org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse getOpenTxns()>
<org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierUtil: void fixTopOBSchema(org.apache.calcite.rel.RelNode,org.apache.calcite.util.Pair,java.util.List,boolean)>
<org.apache.hadoop.hive.ql.exec.DemuxOperator: void process(java.lang.Object,int)>
<org.apache.hadoop.hive.ql.Context: void clear()>
<org.apache.hadoop.hive.llap.daemon.impl.QueryTracker$FileCleanerCallable: java.lang.Void callInternal()>
<org.apache.hive.service.auth.ldap.LdapSearch: org.apache.hive.service.auth.ldap.SearchResultHandler execute(java.util.Collection,org.apache.hive.service.auth.ldap.Query)>
<org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: void setGenericUDFClassName(java.lang.String)>
<org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory: org.apache.hadoop.hive.ql.plan.ExprNodeDesc evaluateFunction(org.apache.hadoop.hive.ql.udf.generic.GenericUDF,java.util.List,java.util.List)>
<org.apache.hive.hcatalog.common.HiveClientCache$2: void onRemoval(com.google.common.cache.RemovalNotification)>
<org.apache.hadoop.hive.ql.exec.tez.InterruptibleProcessing: void addRowAndMaybeCheckAbort()>
<org.apache.hadoop.hive.ql.parse.TezCompiler: void optimizeTaskPlan(java.util.List,org.apache.hadoop.hive.ql.parse.ParseContext,org.apache.hadoop.hive.ql.Context)>
<org.apache.hadoop.hive.ql.parse.CalcitePlanner: org.apache.hadoop.hive.ql.exec.Operator genOPTree(org.apache.hadoop.hive.ql.parse.ASTNode,org.apache.hadoop.hive.ql.parse.SemanticAnalyzer$PlannerContext)>
<org.apache.hive.jdbc.ZooKeeperHiveClientHelper: void configureConnParamsHA(org.apache.hive.jdbc.Utils$JdbcConnectionParams)>
<org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler$Shuffle: void messageReceived(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.MessageEvent)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.lang.String getToken(java.lang.String)>
<org.apache.hadoop.hive.ql.txn.compactor.CompactorMR: void removeFilesForMmTable(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.hive.ql.io.AcidUtils$Directory)>
<org.apache.hadoop.hive.ql.metadata.Hive: boolean moveFile(org.apache.hadoop.hive.conf.HiveConf,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean)>
<org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader: void ensureOrcReader()>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: void setupStats(org.apache.hadoop.hive.ql.plan.TableScanDesc,org.apache.hadoop.hive.ql.parse.QBParseInfo,org.apache.hadoop.hive.ql.metadata.Table,java.lang.String,org.apache.hadoop.hive.ql.parse.RowResolver)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashTable: void add(byte[],int,int,org.apache.hadoop.io.BytesWritable)>
<org.apache.hadoop.hive.ql.session.SessionState: void close()>
<org.apache.hadoop.hive.ql.io.AcidUtils: void setValidWriteIdList(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.common.ValidWriteIdList)>
<org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader$FileReaderYieldReturn: org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader$Vectors readNextSlice()>
<org.apache.hadoop.hive.ql.stats.BasicStatsNoJobTask: int aggregateStats(java.util.concurrent.ExecutorService,org.apache.hadoop.hive.ql.metadata.Hive)>
<org.apache.hadoop.hive.llap.LlapCacheAwareFs: void unregisterFile(org.apache.hadoop.fs.Path)>
<org.apache.hadoop.hive.metastore.utils.HdfsUtils: boolean isPathEncrypted(org.apache.hadoop.conf.Configuration,java.net.URI,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory$ConstantPropagateJoinProc: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.ql.lockmgr.DbTxnManager$Heartbeater: void run()>
<org.apache.hive.hcatalog.listener.DbNotificationListener: void addNotificationLog(org.apache.hadoop.hive.metastore.api.NotificationEvent,org.apache.hadoop.hive.metastore.events.ListenerEvent,java.sql.Connection,org.apache.hadoop.hive.metastore.tools.SQLGenerator)>
<org.apache.hive.service.cli.operation.OperationManager: void removeSafeQueryInfo(org.apache.hive.service.cli.OperationHandle)>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listPrincipalMTableColumnGrants(java.lang.String,org.apache.hadoop.hive.metastore.api.PrincipalType,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hive.beeline.SQLCompleter: java.util.Set getSQLCompleters(org.apache.hive.beeline.BeeLine,boolean)>
<org.apache.hadoop.hive.metastore.cache.SharedCache$TableWrapper: boolean updatePartitionColStats(java.util.List,java.util.List)>
<org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory: org.apache.hadoop.hive.ql.plan.Statistics applyRuntimeStats(org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.plan.Statistics,org.apache.hadoop.hive.ql.exec.Operator)>
<org.apache.hadoop.hive.ql.exec.tez.SessionExpirationTracker: void runExpirationThread()>
<org.apache.hadoop.hive.ql.cache.results.QueryResultsCache: boolean shouldEntryBeAdded(org.apache.hadoop.hive.ql.cache.results.QueryResultsCache$CacheEntry,long)>
<org.apache.hive.beeline.hs2connection.UserHS2ConnectionFileParser: java.util.Properties getConnectionProperties()>
<org.apache.hadoop.hive.metastore.columnstats.aggr.DecimalColumnStatsAggregator: org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj aggregate(java.util.List,java.util.List,boolean)>
<org.apache.hadoop.hive.ql.metadata.JarUtils: org.apache.hadoop.fs.Path findOrCreateJar(java.lang.Class,org.apache.hadoop.fs.FileSystem,java.util.Map)>
<org.apache.hadoop.hive.llap.daemon.impl.QueryTracker$ExternalQueryCleanerCallable: java.lang.Void callInternal()>
<org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$FilterStatsRule: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.ql.optimizer.lineage.Generator: org.apache.hadoop.hive.ql.parse.ParseContext transform(org.apache.hadoop.hive.ql.parse.ParseContext)>
<org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction: void setQueryHints(org.apache.hadoop.hive.ql.parse.QB)>
<org.apache.hadoop.hive.ql.exec.DDLTask: void validateSerDe(java.lang.String,org.apache.hadoop.hive.conf.HiveConf)>
<org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor: boolean process(org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)>
<org.apache.hadoop.hive.ql.parse.spark.SparkCompiler: void connect(org.apache.hadoop.hive.ql.exec.Operator,java.util.concurrent.atomic.AtomicInteger,java.util.Stack,java.util.Map,java.util.Map,java.util.Set)>
<org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler$Shuffle$1: org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler$AttemptPathInfo load(org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler$AttemptPathIdentifier)>
<org.apache.hadoop.hive.metastore.cache.CachedStore: boolean isNotInBlackList(java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.llap.cache.LlapAllocatorBuffer: boolean waitForState()>
<org.apache.hadoop.hive.ql.stats.ColStatsProcessor: java.util.List constructColumnStatsFromPackedRows(org.apache.hadoop.hive.ql.metadata.Table)>
<org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher: void pushProjectionsAndFilters(org.apache.hadoop.mapred.JobConf,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: int abortTxns(java.sql.Connection,java.util.List,long,boolean)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: void timeOutLocks(java.sql.Connection,long)>
<org.apache.hadoop.hive.ql.exec.HashTableSinkOperator: void closeOp(boolean)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: org.apache.hadoop.hive.metastore.txn.TxnHandler$TxnStatus findTxnState(long,java.sql.Statement)>
<org.apache.hadoop.hive.ql.util.IncrementalObjectSizeEstimator$ObjectEstimator: int estimate(java.lang.Object,java.util.HashMap,java.util.IdentityHashMap)>
<org.apache.hadoop.hive.ql.parse.TaskCompiler: org.apache.hadoop.hive.ql.exec.Task genTableStats(org.apache.hadoop.hive.ql.parse.ParseContext,org.apache.hadoop.hive.ql.exec.TableScanOperator,org.apache.hadoop.hive.ql.exec.Task,java.util.HashSet)>
<org.apache.hadoop.hive.ql.exec.ReduceSinkOperator: void initializeOp(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: org.apache.hadoop.hive.metastore.api.LockResponse lockMaterializationRebuild(java.lang.String,java.lang.String,long)>
<org.apache.hive.service.server.HiveServer2: void main(java.lang.String[])>
<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.ql.metadata.Hive create(org.apache.hadoop.hive.conf.HiveConf,boolean,org.apache.hadoop.hive.ql.metadata.Hive,boolean)>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.OperationHandle getSchemas(org.apache.hive.service.cli.SessionHandle,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.ql.parse.GenTezUtils: void removeSemiJoinOperator(org.apache.hadoop.hive.ql.parse.ParseContext,org.apache.hadoop.hive.ql.exec.ReduceSinkOperator,org.apache.hadoop.hive.ql.exec.TableScanOperator)>
<org.apache.hadoop.hive.llap.cli.LlapSliderUtils: void startCluster(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,java.lang.String)>
<org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.JDBCProjectPushDownRule: void onMatch(org.apache.calcite.plan.RelOptRuleCall)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: org.apache.hadoop.hive.metastore.api.GetValidWriteIdsResponse getValidWriteIds(org.apache.hadoop.hive.metastore.api.GetValidWriteIdsRequest)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: org.apache.hadoop.hive.metastore.api.CompactionResponse compact(org.apache.hadoop.hive.metastore.api.CompactionRequest)>
<org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory$ConstantPropagateFilterProc: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.metastore.ObjectStore: java.util.List listPrincipalAllTableColumnGrants(java.lang.String,org.apache.hadoop.hive.metastore.api.PrincipalType,org.apache.hadoop.hive.metastore.ObjectStore$QueryWrapper)>
<org.apache.hadoop.hive.ql.Driver: void recordValidWriteIds(org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager)>
<org.apache.hadoop.hive.metastore.metrics.PerfLogger: long PerfLogEnd(java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.ql.parse.TezCompiler: void removeSemiJoinCyclesDueToMapsideJoins(org.apache.hadoop.hive.ql.parse.OptimizeTezProcContext)>
<org.apache.hadoop.hive.metastore.txn.TxnHandler: java.util.List openTxns(java.sql.Connection,java.sql.Statement,org.apache.hadoop.hive.metastore.api.OpenTxnRequest)>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.parse.RowResolver getColForInsertStmtSpec(java.util.Map,org.apache.hadoop.hive.ql.metadata.Table,java.util.Map,int,java.util.List,java.util.ArrayList,java.util.List)>
<org.apache.hadoop.hive.llap.cache.LlapAllocatorBuffer: java.lang.Boolean endDiscard()>
<org.apache.hadoop.hive.ql.optimizer.spark.CombineEquivalentWorkResolver$EquivalentWorkMatcher: void removeDynamicPartitionPruningSink(java.util.List,org.apache.hadoop.hive.ql.plan.SparkWork)>
<org.apache.hive.beeline.HiveSchemaTool: boolean validateSchemaTables(java.sql.Connection)>
<org.apache.hadoop.hive.ql.exec.FetchOperator: org.apache.hadoop.mapred.RecordReader getRecordReader()>
<org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeHashAggregate: void computeMemoryLimits()>
<org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits: org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector initialize(org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector[])>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.RowSet fetchResults(org.apache.hive.service.cli.OperationHandle,org.apache.hive.service.cli.FetchOrientation,long,org.apache.hive.service.cli.FetchType)>
<org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex: void onRootVertexInitialized(java.lang.String,org.apache.tez.dag.api.InputDescriptor,java.util.List)>
<org.apache.hive.http.JMXJsonServlet: void listBeans(org.codehaus.jackson.JsonGenerator,javax.management.ObjectName,java.lang.String,javax.servlet.http.HttpServletResponse)>
<org.apache.hadoop.hive.ql.metadata.Hive: org.apache.hadoop.hive.metastore.api.AggrStats getAggrColStatsFor(java.lang.String,java.lang.String,java.util.List,java.util.List)>
<org.apache.hadoop.hive.ql.exec.FileSinkOperator: void publishStats()>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.OperationHandle getCrossReference(org.apache.hive.service.cli.SessionHandle,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.druid.DruidStorageHandler: int checkLoadStatus(java.util.List)>
<org.apache.hadoop.hive.ql.Driver: void releaseDriverContext()>
<org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl: org.apache.hadoop.hive.common.io.DiskRangeList preReadUncompressedStream(long,org.apache.hadoop.hive.common.io.DiskRangeList,long,long,org.apache.orc.OrcProto$Stream$Kind)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator: void setupVOutContext(java.util.List)>
<org.apache.hadoop.hive.ql.exec.Utilities: void ensurePathIsWritable(org.apache.hadoop.fs.Path,org.apache.hadoop.hive.conf.HiveConf)>
<org.apache.hadoop.hive.ql.ppd.SimplePredicatePushDown: org.apache.hadoop.hive.ql.parse.ParseContext transform(org.apache.hadoop.hive.ql.parse.ParseContext)>
<org.apache.hadoop.hive.metastore.HiveMetaStoreClient: org.apache.hadoop.hive.metastore.api.AggrStats getAggrColStatsFor(java.lang.String,java.lang.String,java.lang.String,java.util.List,java.util.List)>
<org.apache.hadoop.hive.ql.parse.repl.load.message.AddUniqueConstraintHandler: java.util.List handle(org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandler$Context)>
<org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor: void init(org.apache.tez.mapreduce.processor.MRTaskReporter,java.util.Map,java.util.Map)>
<org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths: void commitOneOutPath(int,org.apache.hadoop.fs.FileSystem,java.util.List)>
<org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdRowCount: java.lang.Double getRowCount(org.apache.calcite.rel.core.SemiJoin,org.apache.calcite.rel.metadata.RelMetadataQuery)>
<org.apache.hive.beeline.HiveSchemaTool$CommandBuilder: void logScript()>
<org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>
<org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater: void close(boolean)>
<org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyStringOperator: void process(java.lang.Object,int)>
<org.apache.hadoop.hive.ql.exec.FileSinkOperator: org.apache.hadoop.hive.ql.exec.FileSinkOperator$FSPaths createNewPaths(java.lang.String,java.lang.String)>
<org.apache.hadoop.hive.llap.cli.LlapServiceDriver$2: java.lang.Void call()>
<org.apache.hadoop.hive.ql.stats.BasicStatsNoJobTask$FooterStatCollector: void run()>
<org.apache.hadoop.hive.llap.LlapCacheAwareFs$CacheAwareInputStream: void copyDiskDataToCacheBuffer(byte[],int,int,java.nio.ByteBuffer,org.apache.hadoop.hive.common.io.DiskRange[],int,long)>
<org.apache.hadoop.hive.ql.exec.Utilities: void cleanMmDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,java.lang.String,int,java.util.HashSet)>
<org.apache.hadoop.hive.ql.parse.MapReduceCompiler: void decideExecMode(java.util.List,org.apache.hadoop.hive.ql.Context,org.apache.hadoop.hive.ql.parse.GlobalLimitCtx)>
<org.apache.hadoop.hive.ql.exec.tez.TezSessionState: void setupSessionAcls(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hive.conf.HiveConf)>
<org.apache.hive.service.cli.CLIService: org.apache.hive.service.cli.SessionHandle openSessionWithImpersonation(org.apache.hive.service.rpc.thrift.TProtocolVersion,java.lang.String,java.lang.String,java.lang.String,java.util.Map,java.lang.String)>
<org.apache.hadoop.hive.llap.io.ChunkedInputStream: void <init>(java.io.InputStream,java.lang.String)>
<org.apache.hadoop.hive.ql.parse.repl.load.message.DropPartitionHandler: java.util.List handle(org.apache.hadoop.hive.ql.parse.repl.load.message.MessageHandler$Context)>
<org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer: void createColumnReaders(org.apache.hadoop.hive.ql.io.orc.encoded.Reader$OrcEncodedColumnBatch,org.apache.hadoop.hive.llap.io.metadata.ConsumerStripeMetadata,org.apache.orc.TypeDescription)>
<org.apache.hadoop.hive.ql.parse.SemanticAnalyzer: org.apache.hadoop.hive.ql.exec.Operator genPTFPlan(org.apache.hadoop.hive.ql.parse.PTFInvocationSpec,org.apache.hadoop.hive.ql.exec.Operator)>
<org.apache.hadoop.hive.llap.LlapBaseInputFormat: org.apache.hadoop.mapred.RecordReader getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter)>
<org.apache.hive.service.cli.CLIService: void createSessionWithSessionHandle(org.apache.hive.service.cli.SessionHandle,java.lang.String,java.lang.String,java.util.Map)>
<org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory$ConstantPropagateSelectProc: java.lang.Object process(org.apache.hadoop.hive.ql.lib.Node,java.util.Stack,org.apache.hadoop.hive.ql.lib.NodeProcessorCtx,java.lang.Object[])>
<org.apache.hadoop.hive.serde2.lazy.LazyHiveVarchar: void init(org.apache.hadoop.hive.serde2.lazy.ByteArrayRef,int,int)>
<org.apache.hadoop.hive.ql.session.SessionState: void <init>(org.apache.hadoop.hive.conf.HiveConf,java.lang.String)>
<org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator: java.util.List initialize()>
<org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory: org.apache.hadoop.hive.ql.plan.ExprNodeDesc foldExprShortcut(org.apache.hadoop.hive.ql.plan.ExprNodeDesc,java.util.Map,org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcCtx,org.apache.hadoop.hive.ql.exec.Operator,int,boolean)>
<org.apache.hive.spark.client.RemoteDriver$JobWrapper: java.lang.Void call()>
<org.apache.hadoop.hive.metastore.MetaStoreDirectSql: java.util.List getPartitionsFromPartitionIds(java.lang.String,java.lang.String,java.lang.String,java.lang.Boolean,java.util.List)>
<org.apache.hadoop.hive.metastore.MetaStoreDirectSql: java.util.List getColStatsForAllTablePartitions(java.lang.String,java.lang.String,boolean)>
<org.apache.hadoop.hive.metastore.MetaStoreDirectSql: int loopJoinOrderedResult(java.util.TreeMap,java.lang.String,int,org.apache.hadoop.hive.metastore.MetaStoreDirectSql$ApplyFunc)>
<org.apache.hadoop.hive.metastore.MetaStoreDirectSql: int getNumPartitionsViaSqlFilter(org.apache.hadoop.hive.metastore.MetaStoreDirectSql$SqlFilterForPushdown)>
<org.apache.hadoop.hive.metastore.MetaStoreDirectSql: java.util.List getPartitionsViaSqlFilterInternal(java.lang.String,java.lang.String,java.lang.String,java.lang.Boolean,java.lang.String,java.util.List,java.util.List,java.lang.Integer)>
<org.apache.hadoop.hive.metastore.MetaStoreDirectSql: java.util.List aggrStatsUseDB(java.lang.String,java.lang.String,java.lang.String,java.util.List,java.util.List,boolean,boolean,double)>
<org.apache.hadoop.hive.metastore.MetaStoreDirectSql: void executeNoResult(java.lang.String)>