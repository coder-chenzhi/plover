<org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: org.apache.hadoop.metrics2.util.Metrics2Util$TopN getTopUsersForMetric(long,java.lang.String,org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager$RollingWindowMap)>
<org.apache.hadoop.hdfs.server.datanode.DataNode: void handleVolumeFailures(java.util.Set)>
<org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: void report(long,java.lang.String,java.lang.String)>
<org.apache.hadoop.metrics2.impl.MetricsConfig: java.lang.Object getProperty(java.lang.String)>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: void receivedNewWrite(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.request.WRITE3Request,org.jboss.netty.channel.Channel,int,org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService,org.apache.hadoop.security.IdMappingServiceProvider)>
<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void loadFromZKCache(boolean)>
<org.apache.hadoop.hdfs.server.balancer.KeyManager$BlockKeyUpdater: void run()>
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager: void startDecommission(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry$NewShmInfo createNewMemorySegment(java.lang.String,org.apache.hadoop.net.unix.DomainSocket)>
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void initPerfMonitoring(org.apache.hadoop.hdfs.protocol.DatanodeInfo[])>
<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo: void flush()>
<org.apache.hadoop.security.SaslRpcServer: void <init>(org.apache.hadoop.security.SaslRpcServer$AuthMethod)>
<org.apache.hadoop.hdfs.tools.DFSAdmin: int run(java.lang.String[])>
<org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM: void appFinished(org.apache.hadoop.yarn.api.records.ApplicationId)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: void activateApplications()>
<org.apache.hadoop.hdfs.client.impl.LeaseRenewer: void interruptAndJoin()>
<org.apache.hadoop.mapred.ResourceMgrDelegate: java.lang.String getStagingAreaDir()>
<org.apache.hadoop.mapred.ShuffleHandler$Shuffle: void exceptionCaught(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ExceptionEvent)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: void markContainerForPreemption(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: void printChildQueues()>
<org.apache.hadoop.io.TestSequenceFile: void sortTest(org.apache.hadoop.fs.FileSystem,int,int,int,boolean,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.security.KDiag: void printDefaultRealm()>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx$COMMIT_STATUS checkCommit(org.apache.hadoop.hdfs.DFSClient,long,org.jboss.netty.channel.Channel,int,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes,boolean)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.io.File moveBlockFiles(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi,org.apache.hadoop.hdfs.protocol.Block,java.io.File,java.io.File)>
<org.apache.hadoop.hdfs.client.impl.BlockReaderRemote: int readChunkImpl(long,byte[],int,int,byte[])>
<org.apache.hadoop.registry.client.impl.zk.CuratorService: void zkDelete(java.lang.String,boolean,org.apache.curator.framework.api.BackgroundCallback)>
<org.apache.hadoop.conf.Configuration: void reloadExistingConfigurations()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: void <init>(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerContext,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore: void loadRMDTSecretManagerState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState)>
<org.apache.hadoop.hdfs.server.datanode.VolumeScanner: void expireOldScannedBytesRecords(long)>
<org.apache.hadoop.yarn.util.ProcfsBasedProcessTree: long getSmapBasedRssMemorySize(int)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler: void recover()>
<org.apache.hadoop.fs.contract.AbstractFSContractTestBase: void setup()>
<org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testRBWReplicas()>
<org.apache.hadoop.fs.FileSystem: void loadFileSystems()>
<org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore: org.apache.hadoop.yarn.server.timeline.TimelineStore getCachedStore(org.apache.hadoop.yarn.api.records.timeline.TimelineEntityGroupId,java.util.List)>
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>
<org.apache.hadoop.hdfs.server.namenode.NameNode: void copyEditLogSegmentsToSharedDir(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.util.Collection,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: void removePendingChangeRequests(java.util.List)>
<org.apache.hadoop.util.TestIdentityHashStore: void testAdditionsAndRemovals()>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void storeRMDelegationTokenState(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void logRootNodeAcls(java.lang.String)>
<org.apache.hadoop.yarn.server.resourcemanager.security.AppPriorityACLsManager: void addPrioirityACLs(java.util.List,java.lang.String)>
<org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor: org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest)>
<org.apache.hadoop.mapreduce.JobResourceUploader: void uploadResourcesInternal(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.mapred.lib.InputSampler$RandomSampler: java.lang.Object[] getSample(org.apache.hadoop.mapred.InputFormat,org.apache.hadoop.mapred.JobConf)>
<org.apache.hadoop.security.UserGroupInformation$1: void run()>
<org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: java.util.List loadCompletedResources(org.apache.hadoop.yarn.server.utils.LeveldbIterator,java.lang.String)>
<org.apache.hadoop.examples.terasort.TeraScheduler: org.apache.hadoop.examples.terasort.TeraScheduler$Host pickBestHost()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType getAllowedLocalityLevel(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,int,double,double)>
<org.apache.hadoop.ha.ActiveStandbyElector: void monitorActiveStatus()>
<org.apache.hadoop.metrics.ganglia.GangliaContext31: void init(java.lang.String,org.apache.hadoop.metrics.ContextFactory)>
<org.apache.hadoop.metrics2.lib.MutableRates: void init(java.lang.Class)>
<org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void storeContainerQueued(org.apache.hadoop.yarn.api.records.ContainerId)>
<org.apache.hadoop.yarn.webapp.view.HtmlBlock: void render()>
<org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: void checkPermission(org.apache.hadoop.hdfs.server.namenode.INodesInPath,boolean,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,boolean)>
<org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testFinalizedReplicas()>
<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.BlockStorageLocation[] getBlockStorageLocations(java.util.List)>
<org.apache.hadoop.ipc.Client$Connection$3: void run()>
<org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testRecoveryInProgressException()>
<org.apache.hadoop.hdfs.DFSUtilClient: boolean isLocalAddress(java.net.InetSocketAddress)>
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: java.lang.String[] getGroupsForUser(java.lang.String)>
<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>
<org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp: boolean completeFile(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy: org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi doChooseVolume(java.util.List,long)>
<org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl: void checkMetricsRecords(java.util.List)>
<org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31: void emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: org.apache.hadoop.hdfs.server.datanode.ReplicaInfo selectReplicaToDelete(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,org.apache.hadoop.hdfs.server.datanode.ReplicaInfo)>
<org.apache.hadoop.mapred.TestTextInputFormat: void verifyPartitions(int,int,org.apache.hadoop.fs.Path,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.mapred.JobConf)>
<org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler: org.apache.hadoop.security.authentication.server.AuthenticationToken authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ClientMmap getOrCreateClientMmap(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica,boolean)>
<org.apache.hadoop.registry.server.services.RegistryAdminService: int purge(java.lang.String,org.apache.hadoop.registry.server.services.RegistryAdminService$NodeSelector,org.apache.hadoop.registry.server.services.RegistryAdminService$PurgePolicy,org.apache.curator.framework.api.BackgroundCallback)>
<org.apache.hadoop.ipc.WritableRpcEngine$Server$WritableRpcInvoker: org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)>
<org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskContiguous: void recover()>
<org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore: java.util.List getTimelineStoresForRead(java.lang.String,java.lang.String,java.util.List)>
<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor$ContainerRequest assignWithoutLocality(org.apache.hadoop.yarn.api.records.Container)>
<org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition: org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState transition(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent)>
<org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher: org.apache.hadoop.yarn.api.records.ApplicationReport monitorApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.util.Set)>
<org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol: org.apache.hadoop.hdfs.protocol.DirectoryListing getListing(java.lang.String,byte[],boolean)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl: void save()>
<org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory: void <init>(org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf)>
<org.apache.hadoop.security.UserGroupInformation: java.lang.Object doAs(java.security.PrivilegedExceptionAction)>
<org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: void handleNMContainerStatus(org.apache.hadoop.yarn.server.api.protocolrecords.NMContainerStatus,org.apache.hadoop.yarn.api.records.NodeId)>
<org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: void setAppCollectorsMapToResponse(java.util.List,org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse)>
<org.apache.hadoop.mapreduce.task.reduce.EventFetcher: int getMapCompletionEvents()>
<org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore: java.util.List getTimelineStoresForRead(java.lang.String,org.apache.hadoop.yarn.server.timeline.NameValuePair,java.util.Collection,java.util.List)>
<org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher: void removeCompletedApps(org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher$DecommissioningNodeContext)>
<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: void close()>
<org.apache.hadoop.security.SecurityUtil: java.net.InetAddress getByName(java.lang.String)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: int loadRMApp(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState,org.apache.hadoop.yarn.server.utils.LeveldbIterator,java.lang.String,byte[])>
<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: void handle(org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent)>
<org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: void addResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionTypeRequest,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.client.api.AMRMClient$ContainerRequest,boolean,java.lang.String)>
<org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler: org.apache.hadoop.security.authentication.server.AuthenticationToken alternateAuthenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: boolean checkAndStartWrite(org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService,org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx)>
<org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem: void setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)>
<org.apache.hadoop.yarn.server.uam.UnmanagedApplicationManager$AMRequestHandlerThread: void run()>
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetch(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.util.Waitable)>
<org.apache.hadoop.yarn.security.AMRMTokenSelector: org.apache.hadoop.security.token.Token selectToken(org.apache.hadoop.io.Text,java.util.Collection)>
<org.apache.hadoop.mapred.ShuffleHandler$Shuffle: void populateHeaders(java.util.List,java.lang.String,java.lang.String,int,org.jboss.netty.handler.codec.http.HttpRequest,org.jboss.netty.handler.codec.http.HttpResponse,boolean,java.util.Map)>
<org.apache.hadoop.hdfs.server.blockmanagement.SlowDiskTracker: java.lang.String getSlowDiskReportAsJsonString()>
<org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: void serviceInit(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.http.TestHttpServerWebapps: void testMissingServerResource()>
<org.apache.hadoop.yarn.server.resourcemanager.NodesListManager: void handle(org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEvent)>
<org.apache.hadoop.hdfs.client.impl.BlockReaderFactory: org.apache.hadoop.hdfs.client.impl.BlockReaderFactory$BlockReaderPeer nextTcpPeer()>
<org.apache.hadoop.crypto.key.JavaKeyStoreProvider: org.apache.hadoop.fs.permission.FsPermission loadAndReturnPerm(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerModule: org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsBlkioResourceHandlerImpl getCgroupsBlkioResourceHandler(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: boolean accept(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)>
<org.apache.hadoop.fs.FileSystem: java.lang.Class getFileSystemClass(java.lang.String,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: void free()>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void onCompleteLazyPersist(java.lang.String,long,long,java.io.File[],org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>
<org.apache.hadoop.security.authentication.client.AuthenticatedURL$AuthCookieHandler: void put(java.net.URI,java.util.Map)>
<org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskContiguous: void syncBlock(java.util.List)>
<org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: java.util.List getMapsForHost(org.apache.hadoop.mapreduce.task.reduce.MapHost)>
<org.apache.hadoop.io.file.tfile.Compression$Algorithm: void returnDecompressor(org.apache.hadoop.io.compress.Decompressor)>
<org.apache.hadoop.yarn.server.timeline.LogInfo: long parseForStore(org.apache.hadoop.yarn.server.timeline.TimelineDataManager,org.apache.hadoop.fs.Path,boolean,org.codehaus.jackson.JsonFactory,org.codehaus.jackson.map.ObjectMapper,org.apache.hadoop.fs.FileSystem)>
<org.apache.hadoop.service.AbstractService: void start()>
<org.apache.hadoop.hdfs.client.impl.BlockReaderFactory: org.apache.hadoop.hdfs.client.impl.BlockReaderFactory$BlockReaderPeer nextDomainPeer()>
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void blockChecksum(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: void allocateContainersToNode(org.apache.hadoop.yarn.api.records.NodeId,boolean)>
<org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore: int scanActiveLogs(org.apache.hadoop.fs.Path)>
<org.apache.hadoop.hdfs.client.impl.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>
<org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService: void updateToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier,java.lang.Long)>
<org.apache.hadoop.registry.server.services.DeleteCompletionCallback: void processResult(org.apache.curator.framework.CuratorFramework,org.apache.curator.framework.api.CuratorEvent)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: void removeRMDelegationTokenState(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier)>
<org.apache.hadoop.hdfs.server.federation.store.impl.MembershipStoreImpl: boolean loadCache(boolean)>
<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: void handle(org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask: void run()>
<org.apache.hadoop.mapred.lib.MultithreadedMapRunner: void run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)>
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void requestShortCircuitFds(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,int,boolean)>
<org.apache.hadoop.io.TestWritableUtils: void testValue(int,int)>
<org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher$PollTimerTask: void run()>
<org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer: void handleAppSubmitEvent(org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$AbstractDelegationTokenRenewerAppEvent)>
<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$AttemptFailedTransition: org.apache.hadoop.mapreduce.v2.app.job.TaskStateInternal transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent)>
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: void unregisterSlot(int)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void loadRMDelegationTokenState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime: void addCGroupParentIfRequired(java.lang.String,java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerRunCommand)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl: java.util.List getSubdirEntries()>
<org.apache.hadoop.hdfs.server.namenode.top.TopAuditLogger: void logAuditEvent(boolean,java.lang.String,java.net.InetAddress,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.fs.FileStatus)>
<org.apache.hadoop.ha.ActiveStandbyElector: void processResult(int,java.lang.String,java.lang.Object,java.lang.String)>
<org.apache.hadoop.mapred.ShuffleHandler$Shuffle: void verifyRequest(java.lang.String,org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.handler.codec.http.HttpRequest,org.jboss.netty.handler.codec.http.HttpResponse,java.net.URL)>
<org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void storeContainerKilled(org.apache.hadoop.yarn.api.records.ContainerId)>
<org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: org.apache.hadoop.mapreduce.task.reduce.MapOutput reserve(org.apache.hadoop.mapreduce.TaskAttemptID,long,int)>
<org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextOpImpl(boolean)>
<org.apache.hadoop.metrics2.impl.MBeanInfoBuilder: javax.management.MBeanInfo get()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment allocateContainersOnMultiNodes(org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.PlacementSet)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>
<org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor: void setScriptExecutable(org.apache.hadoop.fs.Path,java.lang.String)>
<org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.READDIR3Response readdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
<org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider: void initFileSystem(java.net.URI)>
<org.apache.hadoop.ha.HAAdmin: int run(java.lang.String[])>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.protocol.LocatedBlocks createLocatedBlocks(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo[],long,boolean,long,long,boolean,boolean,org.apache.hadoop.fs.FileEncryptionInfo)>
<org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void load(java.io.File)>
<org.apache.hadoop.yarn.server.timeline.security.TimelineACLsManager: boolean checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.ApplicationAccessType,org.apache.hadoop.yarn.api.records.timeline.TimelineEntity)>
<org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher: void publishApplicationEvent(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: boolean anyContainerInFinalState(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void adjustCrcChannelPosition(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams,int)>
<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)>
<org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy: void preemptOrkillSelectedContainerAfterWait(java.util.Map,long)>
<org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter: void writeDomain(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.records.timeline.TimelineDomain)>
<org.apache.hadoop.mapreduce.lib.input.TestMRKeyValueTextInputFormat: void testSplitableCodecs()>
<org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor: org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterResponse finishApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterRequest)>
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp readOp(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream)>
<org.apache.hadoop.service.launcher.ServiceLauncher: void launchServiceAndExit(java.util.List)>
<org.apache.hadoop.hdfs.server.datanode.VolumeScanner: long runLoop(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$FlushTimerTask: void run()>
<org.apache.hadoop.hdfs.net.DFSTopologyNodeImpl: void childAddStorage(java.lang.String,org.apache.hadoop.fs.StorageType)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.PlacementSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)>
<org.apache.hadoop.crypto.key.JavaKeyStoreProvider: org.apache.hadoop.fs.permission.FsPermission tryLoadFromPath(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC: org.apache.hadoop.ipc.Server getServer(java.lang.Class,java.lang.Object,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,int,java.lang.String)>
<org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeDiskMetrics$1: void run()>
<org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp: void persistBlocks(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile,boolean)>
<org.apache.hadoop.hdfs.DataStreamer: boolean createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],long,boolean)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.security.LocalizerTokenSelector: org.apache.hadoop.security.token.Token selectToken(org.apache.hadoop.io.Text,java.util.Collection)>
<org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy: void editSchedule()>
<org.apache.hadoop.registry.client.impl.zk.RegistrySecurity: org.apache.zookeeper.data.ACL createACLfromUsername(java.lang.String,int)>
<org.apache.hadoop.hdfs.client.impl.BlockReaderRemote2: void readNextPacket()>
<org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: void addDirectoryToJobListCache(org.apache.hadoop.fs.Path)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: void updateResourceUsagePerUser(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,boolean)>
<org.apache.hadoop.ipc.Client$Connection: void run()>
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void blockReceivedAndDeleted(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks[])>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$RMAppStateFileProcessor: void processChildNode(java.lang.String,java.lang.String,byte[])>
<org.apache.hadoop.mapreduce.lib.map.MultithreadedMapper: void run(org.apache.hadoop.mapreduce.Mapper$Context)>
<org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenCancelThread: void run()>
<org.apache.hadoop.mapreduce.lib.input.TestMRKeyValueTextInputFormat: void testFormat()>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask: boolean shouldDefer()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor: void updateNode(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode)>
<org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$JobListCache: void delete(org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo)>
<org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: java.lang.Object getAttribute(java.lang.String)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: void storeReservationState(org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto,java.lang.String,java.lang.String)>
<org.apache.hadoop.metrics.ganglia.GangliaContext31: void emitMetric(java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer: void removeApplicationFromRenewal(org.apache.hadoop.yarn.api.records.ApplicationId)>
<org.apache.hadoop.security.Groups: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Timer)>
<org.apache.hadoop.hdfs.server.namenode.TestEditLogFileInputStream: void testScanCorruptEditLog()>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void addToInvalidates(org.apache.hadoop.hdfs.protocol.Block)>
<org.apache.hadoop.yarn.util.FSDownload: org.apache.hadoop.fs.Path call()>
<org.apache.hadoop.yarn.client.api.AMRMClient: void waitFor(com.google.common.base.Supplier,int,int)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: void startContainerInternal(org.apache.hadoop.yarn.security.ContainerTokenIdentifier,org.apache.hadoop.yarn.api.protocolrecords.StartContainerRequest)>
<org.apache.hadoop.hdfs.server.datanode.BlockReportTestBase: void blockReport_01()>
<org.apache.hadoop.util.Progress: void set(float)>
<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: java.net.URL getNamenodeURL(java.lang.String,java.lang.String)>
<org.apache.hadoop.util.Shell: void <clinit>()>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaInputStreams: void <init>(java.io.InputStream,java.io.InputStream,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference,org.apache.hadoop.hdfs.server.datanode.FileIoProvider)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: int loadRMDTSecretManagerTokens(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.ActiveUsersManager: void activateApplication(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId)>
<org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.READ3Response read(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
<org.apache.hadoop.hdfs.server.datanode.BlockScanner: void removeVolumeScanner(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)>
<org.apache.hadoop.mapred.BackupStore$MemoryCache: void reinitialize(boolean)>
<org.apache.hadoop.yarn.util.RackResolver: org.apache.hadoop.net.Node coreResolve(java.lang.String)>
<org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC: java.lang.Object getProxy(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void addOrUpdateToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation,boolean)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerAppReport getSchedulerAppInfo(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)>
<org.apache.hadoop.hdfs.server.namenode.ha.RequestHedgingProxyProvider$RequestHedgingInvocationHandler$1: java.lang.Object call()>
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.protocol.LastBlockWithStatus appendFile(java.lang.String,java.lang.String,java.lang.String,java.util.EnumSet,boolean)>
<org.apache.hadoop.metrics2.impl.MetricsConfig: org.apache.hadoop.metrics2.impl.MetricsConfig loadFirst(java.lang.String,java.lang.String[])>
<org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor: void createDir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,java.lang.String)>
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: void close()>
<org.apache.hadoop.fs.contract.AbstractFSContract: void <init>(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.yarn.conf.HAUtil: java.lang.String getConfValueForRMInstance(java.lang.String,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.security.JniBasedUnixGroupsMapping: void <clinit>()>
<org.apache.hadoop.hdfs.DataStreamer: void addDatanode2ExistingPipeline()>
<org.apache.hadoop.net.NetworkTopology: void remove(org.apache.hadoop.net.Node)>
<org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse registerNodeManager(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest)>
<org.apache.hadoop.security.authentication.util.KerberosName: void resetDefaultRealm()>
<org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor: void loadNodeChildrenHelper(org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$Node,java.lang.String,java.lang.String[])>
<org.apache.hadoop.nfs.NfsExports: org.apache.hadoop.nfs.NfsExports$Match getMatch(java.lang.String)>
<org.apache.hadoop.hdfs.server.federation.store.impl.MembershipStoreImpl: org.apache.hadoop.hdfs.server.federation.store.protocol.NamenodeHeartbeatResponse namenodeHeartbeat(org.apache.hadoop.hdfs.server.federation.store.protocol.NamenodeHeartbeatRequest)>
<org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby: int doRun()>
<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager: void close()>
<org.apache.hadoop.hdfs.web.TokenAspect: void initDelegationToken(org.apache.hadoop.security.UserGroupInformation)>
<org.apache.hadoop.hdfs.client.HdfsUtils: boolean isHealthy(java.net.URI)>
<org.apache.hadoop.portmap.RpcProgramPortmap: org.apache.hadoop.oncrpc.XDR set(int,org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.XDR)>
<org.apache.hadoop.hdfs.server.federation.resolver.MountTableResolver: void invalidateLocationCache(java.lang.String)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: void updateNonActiveUsersResourceUsage(java.lang.String)>
<org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor: org.apache.hadoop.fs.Path localizeClasspathJar(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.lang.String)>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: void executeWriteBack()>
<org.apache.hadoop.security.SaslRpcClient: org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth selectSaslClient(java.util.List)>
<org.apache.hadoop.hdfs.client.impl.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getBlockReaderLocal()>
<org.apache.hadoop.hdfs.server.namenode.ha.RequestHedgingProxyProvider$RequestHedgingInvocationHandler: java.lang.Object invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])>
<org.apache.hadoop.mapred.ShuffleHandler$Shuffle: void messageReceived(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.MessageEvent)>
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager$Monitor: void check()>
<org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$NMCallbackHandler: void onContainerStopped(org.apache.hadoop.yarn.api.records.ContainerId)>
<org.apache.hadoop.yarn.server.security.BaseContainerTokenSecretManager: byte[] createPassword(org.apache.hadoop.yarn.security.ContainerTokenIdentifier)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: void handleInitContainerResources(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ContainerLocalizationRequestEvent)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: boolean moveReservedContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int invalidateWorkForOneNode(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>
<org.apache.hadoop.yarn.webapp.WebApps$Builder: org.apache.hadoop.yarn.webapp.WebApp build(org.apache.hadoop.yarn.webapp.WebApp)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerModule: org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficControlBandwidthHandlerImpl getTrafficControlBandwidthHandler(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: void recover(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState)>
<org.apache.hadoop.mapreduce.task.reduce.LocalFetcher: void doCopy(java.util.Set)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int computeReplicationWorkForBlocks(java.util.List)>
<org.apache.hadoop.mapred.SortedRanges: void remove(org.apache.hadoop.mapred.SortedRanges$Range)>
<org.apache.hadoop.hdfs.client.impl.BlockReaderLocal: int read(java.nio.ByteBuffer)>
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager: void stopMaintenance(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
<org.apache.hadoop.mapred.jobcontrol.TestLocalJobControl: void testLocalJobControlDataCopy()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: void setReservable(java.lang.String,boolean)>
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand cacheReport()>
<org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl$TestSink: void putMetrics(org.apache.hadoop.metrics2.MetricsRecord)>
<org.apache.hadoop.ipc.DecayRpcScheduler: void updateAverageResponseTime(boolean)>
<org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void removeContainer(org.apache.hadoop.yarn.api.records.ContainerId)>
<org.apache.hadoop.conf.Configuration: javax.xml.stream.XMLStreamReader parse(java.io.InputStream,java.lang.String,boolean)>
<org.apache.hadoop.ipc.TestRPC: void testCallsInternal(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: java.util.Map parseCredentials(java.util.Map)>
<org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef: void process(org.apache.zookeeper.WatchedEvent)>
<org.apache.hadoop.ipc.Server: void registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,org.apache.hadoop.ipc.RPC$RpcInvoker)>
<org.apache.hadoop.hdfs.server.namenode.LeaseManager: boolean checkLeases()>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: void removeApplicationStateInternal(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData)>
<org.apache.hadoop.yarn.server.timeline.EntityLogInfo: long doParse(org.apache.hadoop.yarn.server.timeline.TimelineDataManager,org.codehaus.jackson.JsonParser,org.codehaus.jackson.map.ObjectMapper,org.apache.hadoop.security.UserGroupInformation,boolean)>
<org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue: void addCall(org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall)>
<org.apache.hadoop.yarn.server.security.ApplicationACLsManager: boolean checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.ApplicationAccessType,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId)>
<org.apache.hadoop.security.SaslRpcClient: java.lang.String getServerPrincipal(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)>
<org.apache.hadoop.hdfs.nfs.nfs3.Nfs3Utils: void writeChannelCommit(org.jboss.netty.channel.Channel,org.apache.hadoop.oncrpc.XDR,int)>
<org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor: void kill(org.apache.hadoop.util.Daemon)>
<org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration: void setOverCommitTimeoutPerNode(java.lang.String,int)>
<org.apache.hadoop.ha.ActiveStandbyElector: void processResult(int,java.lang.String,java.lang.Object,org.apache.zookeeper.data.Stat)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl: boolean checkClosed()>
<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair checkTrustAndSend(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataEncryptionKeyFactory,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID)>
<org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan: void archiveCompletedReservations(long)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor: void updateNodeResource(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode,org.apache.hadoop.yarn.api.records.ResourceOption)>
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender: void sendLifeline()>
<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: void freeSlot(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: org.apache.hadoop.yarn.api.records.Resource getHeadroom()>
<org.apache.hadoop.security.authentication.server.MultiSchemeAuthenticationHandler: org.apache.hadoop.security.authentication.server.AuthenticationToken authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>
<org.apache.hadoop.yarn.server.nodemanager.util.ProcessIdFileReader: java.lang.String getProcessId(org.apache.hadoop.fs.Path)>
<org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData getProxy(java.lang.String,org.apache.hadoop.yarn.api.records.ContainerId)>
<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void stopSinks()>
<org.apache.hadoop.mapreduce.v2.util.MRApps: java.lang.ClassLoader createJobClassLoader(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd: org.apache.hadoop.oncrpc.XDR mnt(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress)>
<org.apache.hadoop.mapred.LocalJobRunner$Job: java.util.concurrent.ExecutorService createMapExecutor()>
<org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer: org.apache.hadoop.hdfs.protocol.DatanodeInfo[] getDatanodeReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType,boolean,long)>
<org.apache.hadoop.registry.client.impl.zk.RegistrySecurity: void bindJVMtoJAASFile(java.io.File)>
<org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping: java.util.List getUsersForNetgroup(java.lang.String)>
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager: void stopDecommission(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
<org.apache.hadoop.hdfs.DataStreamer: void run()>
<org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler: boolean validateSignature(com.nimbusds.jwt.SignedJWT)>
<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: org.apache.hadoop.metrics2.MetricsSink register(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink)>
<org.apache.hadoop.yarn.webapp.Router: java.lang.Class load(java.lang.Class,java.lang.String)>
<org.apache.hadoop.hdfs.server.common.JspHelper: org.apache.hadoop.security.UserGroupInformation getUGI(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,boolean)>
<org.apache.hadoop.io.retry.RetryInvocationHandler: void log(java.lang.reflect.Method,boolean,int,long,java.lang.Exception)>
<org.apache.hadoop.fs.RawLocalFileSystem: boolean handleEmptyDstDirectoryOnWindows(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.Path,java.io.File)>
<org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: void stopThreads()>
<org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$AttemptDirCache: org.apache.hadoop.fs.Path createAttemptDir(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)>
<org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: void checkNotSymlink(org.apache.hadoop.hdfs.server.namenode.INode,byte[][],int)>
<org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$NMCallbackHandler: void onContainerStatusReceived(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.ContainerStatus)>
<org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$EntityLogCleaner: void run()>
<org.apache.hadoop.mapred.YARNRunner: java.util.List generateResourceRequests()>
<org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore: boolean shouldCleanAppLogDir(org.apache.hadoop.fs.Path,long,org.apache.hadoop.fs.FileSystem,long)>
<org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp: org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp$RenameResult renameTo(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,boolean,org.apache.hadoop.fs.Options$Rename[])>
<org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: void handle(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent)>
<org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.RMDIR3Response rmdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: void evictBlocks(long)>
<org.apache.hadoop.hdfs.server.datanode.VolumeScanner: long findNextUsableBlockIter()>
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void accept(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheVisitor)>
<org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx: long dumpData(java.io.FileOutputStream,java.io.RandomAccessFile)>
<org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs: void moveToDone()>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void loadDelegationTokenFromNode(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState,java.lang.String)>
<org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter: java.util.Set getProxyAddresses()>
<org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider: void pushToZK(byte[],byte[],byte[])>
<org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks$PendingReplicationMonitor: void run()>
<org.apache.hadoop.fs.DelegationTokenRenewer: void removeRenewAction(org.apache.hadoop.fs.FileSystem)>
<org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testZeroLenReplicas()>
<org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM: void setNodeId(org.apache.hadoop.yarn.api.records.NodeId)>
<org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.client.api.NMTokenCache)>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx offerNextToWrite()>
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.protocol.HdfsFileStatus create(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,org.apache.hadoop.io.EnumSetWritable,boolean,short,long,org.apache.hadoop.crypto.CryptoProtocolVersion[])>
<org.apache.hadoop.hdfs.nfs.nfs3.DFSClientCache: org.apache.hadoop.security.UserGroupInformation getUserGroupInformation(java.lang.String,org.apache.hadoop.security.UserGroupInformation)>
<org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$ScheduleTransition: org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState transition(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: org.apache.hadoop.yarn.api.records.Resource getComputedResourceLimitForAllUsers(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)>
<org.apache.hadoop.util.concurrent.ExecutorHelper: void logThrowableFromAfterExecute(java.lang.Runnable,java.lang.Throwable)>
<org.apache.hadoop.mapreduce.security.TokenCache: org.apache.hadoop.security.Credentials loadTokens(java.lang.String,org.apache.hadoop.mapred.JobConf)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler: void assignContainers(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode)>
<org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService: void removeTokenMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)>
<org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp: org.apache.hadoop.hdfs.server.namenode.INodesInPath createSingleDirectory(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,byte[],org.apache.hadoop.fs.permission.PermissionStatus)>
<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair createStreamPair(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherOption,java.io.OutputStream,java.io.InputStream,boolean)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor: void removeNode(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode)>
<org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher: void publishContainerEvent(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent)>
<org.apache.hadoop.util.KMSUtil: org.apache.hadoop.crypto.key.KeyProvider createKeyProvider(org.apache.hadoop.conf.Configuration,java.lang.String)>
<org.apache.hadoop.io.TestSequenceFile: void checkSort(org.apache.hadoop.fs.FileSystem,int,int,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.ipc.RPC$Server: void registerProtocolAndImpl(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)>
<org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd: org.apache.hadoop.oncrpc.XDR umnt(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress)>
<org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM: void appAttemptStartContainer(org.apache.hadoop.yarn.security.NMTokenIdentifier)>
<org.apache.hadoop.registry.client.impl.zk.RegistrySecurity: void resetDigestACLs()>
<org.apache.hadoop.hdfs.server.datanode.web.webhdfs.HdfsWriter: void exceptionCaught(io.netty.channel.ChannelHandlerContext,java.lang.Throwable)>
<org.apache.hadoop.hdfs.server.datanode.VolumeScanner: long scanBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long)>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache$StreamMonitor: void run()>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: void handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ResourceEvent)>
<org.apache.hadoop.crypto.CryptoCodec: org.apache.hadoop.crypto.CryptoCodec getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime: org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.LinuxContainerRuntime pickContainerRuntime(java.util.Map)>
<org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp: org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp$RenameResult renameToInt(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,java.lang.String,boolean)>
<org.apache.hadoop.security.UserGroupInformation: java.util.List getGroups()>
<org.apache.hadoop.hdfs.server.federation.resolver.order.HashFirstResolver: java.lang.String getFirstNamespace(java.lang.String,org.apache.hadoop.hdfs.server.federation.resolver.PathLocation)>
<org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter: void loadDirectoriesInINodeSection(java.io.InputStream)>
<org.apache.hadoop.security.TestLdapGroupsMapping: void testLdapReadTimeout()>
<org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$HeartbeatThread: void run()>
<org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void finishResourceLocalization(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LocalizedResourceProto)>
<org.apache.hadoop.mapred.TestMultiFileInputFormat: org.apache.hadoop.fs.Path initFiles(org.apache.hadoop.fs.FileSystem,int,int)>
<org.apache.hadoop.yarn.server.webproxy.ProxyUtils: void sendRedirect(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String)>
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo create(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator,org.apache.hadoop.util.Waitable)>
<org.apache.hadoop.security.SaslInputStream: int readMoreData()>
<org.apache.hadoop.fs.loadGenerator.LoadGenerator: int generateLoadOnNN()>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: void chooseRemoteRack(int,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)>
<org.apache.hadoop.util.Shell: boolean isSetsidSupported()>
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse sendHeartBeat(boolean)>
<org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs: void addSummaryLog(java.lang.String,java.lang.String,java.lang.String,boolean)>
<org.apache.hadoop.mapred.TaskAttemptListenerImpl: boolean ping(org.apache.hadoop.mapred.TaskAttemptID)>
<org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testRBW_RWRReplicas()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: float getNonLabeledQueueCapacity(java.lang.String)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficControlBandwidthHandlerImpl: java.util.List reacquireContainer(org.apache.hadoop.yarn.api.records.ContainerId)>
<org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler: void authenticateWithoutTlsExtension(java.lang.String,java.lang.String)>
<org.apache.hadoop.yarn.security.client.ClientToAMTokenSelector: org.apache.hadoop.security.token.Token selectToken(org.apache.hadoop.io.Text,java.util.Collection)>
<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.MD5MD5CRC32FileChecksum getFileChecksum(java.lang.String,long)>
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber: void run()>
<org.apache.hadoop.mapred.TestBadRecords$BadReducer: void reduce(org.apache.hadoop.io.LongWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)>
<org.apache.hadoop.mapred.Task: void initialize(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.JobID,org.apache.hadoop.mapred.Reporter,boolean)>
<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void assignMapsWithLocality(java.util.List)>
<org.apache.hadoop.mapred.pipes.BinaryProtocol: void start()>
<org.apache.hadoop.security.ssl.SSLFactory: void <init>(org.apache.hadoop.security.ssl.SSLFactory$Mode,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: void storeRMDTMasterKeyState(org.apache.hadoop.security.token.delegation.DelegationKey)>
<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void createPersistentNode(java.lang.String)>
<org.apache.hadoop.crypto.key.kms.KMSClientProvider: org.apache.hadoop.security.token.Token[] addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)>
<org.apache.hadoop.net.NetworkTopologyWithNodeGroup: void remove(org.apache.hadoop.net.Node)>
<org.apache.hadoop.oncrpc.security.CredentialsSys: void <clinit>()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: org.apache.hadoop.yarn.api.records.Resource getComputedResourceLimitForActiveUsers(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)>
<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void stopSources()>
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.crypto.CryptoProtocolVersion chooseProtocolVersion(org.apache.hadoop.hdfs.protocol.EncryptionZone,org.apache.hadoop.crypto.CryptoProtocolVersion[])>
<org.apache.hadoop.net.NetworkTopology: org.apache.hadoop.net.Node chooseRandom(java.lang.String,java.lang.String,java.util.Collection)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: void storeApplicationStateInternal(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData)>
<org.apache.hadoop.yarn.server.nodemanager.DockerContainerExecutor: void writeLaunchEnv(java.io.OutputStream,java.util.Map,java.util.Map,java.util.List,org.apache.hadoop.fs.Path,java.lang.String)>
<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void startCacheCleanerThreadIfNeeded()>
<org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.WRITE3Response write(org.apache.hadoop.oncrpc.XDR,org.jboss.netty.channel.Channel,int,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
<org.apache.hadoop.registry.client.binding.RegistryUtils: java.util.Map statChildren(org.apache.hadoop.registry.client.api.RegistryOperations,java.lang.String)>
<org.apache.hadoop.mapred.LocalContainerLauncher: org.apache.hadoop.mapred.MapOutputFile renameMapOutputForReduce(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId,org.apache.hadoop.mapred.MapOutputFile)>
<org.apache.hadoop.hdfs.util.MD5FileUtils: void saveMD5File(java.io.File,java.lang.String)>
<org.apache.hadoop.io.IOUtils: void cleanup(org.apache.commons.logging.Log,java.io.Closeable[])>
<org.apache.hadoop.service.launcher.AbstractServiceLauncherTestBase: void assertLaunchOutcome(int,java.lang.String,java.lang.String[])>
<org.apache.hadoop.mapreduce.Cluster: void initialize(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hdfs.protocol.datatransfer.Sender: void send(java.io.DataOutputStream,org.apache.hadoop.hdfs.protocol.datatransfer.Op,com.google.protobuf.Message)>
<org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher: void putEntity(org.apache.hadoop.yarn.api.records.timeline.TimelineEntity)>
<org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl: boolean putAll(java.util.List,boolean,boolean)>
<org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMDistributedNodeLabelsHandler: void verifyRMHeartbeatResponseForNodeLabels(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse)>
<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void addNewPendingCached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List)>
<org.apache.hadoop.security.token.delegation.web.DelegationTokenManager: void cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)>
<org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$SnapshotDiffSectionProcessor: void process()>
<org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: void run()>
<org.apache.hadoop.hdfs.server.federation.resolver.MultipleDestinationMountTableResolver: org.apache.hadoop.hdfs.server.federation.resolver.PathLocation getDestinationForPath(java.lang.String)>
<org.apache.hadoop.ipc.Client$Connection: void close()>
<org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd: org.apache.hadoop.oncrpc.XDR umntall(org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress)>
<org.apache.hadoop.metrics2.impl.MetricsConfig: java.lang.String getClassName(java.lang.String)>
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocAndRegisterSlot(org.apache.hadoop.hdfs.ExtendedBlockId)>
<org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices: javax.ws.rs.core.Response getContainerLogsInfo(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,boolean)>
<org.apache.hadoop.security.ProviderUtils: org.apache.hadoop.conf.Configuration excludeIncompatibleCredentialProviders(org.apache.hadoop.conf.Configuration,java.lang.Class)>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache: java.util.Map$Entry getEntryToEvict()>
<org.apache.hadoop.yarn.server.resourcemanager.federation.FederationStateStoreHeartbeat: void run()>
<org.apache.hadoop.util.GenericOptionsParser: void processGeneralOptions(org.apache.commons.cli.CommandLine)>
<org.apache.hadoop.http.HttpRequestLog: org.mortbay.jetty.RequestLog getRequestLog(java.lang.String)>
<org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: boolean remove(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int)>
<org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$SnapshotDiffSectionProcessor: void processDirDiffEntry()>
<org.apache.hadoop.mapred.ClientServiceDelegate: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol getProxy()>
<org.apache.hadoop.mapred.pipes.Application: void waitForAuthentication()>
<org.apache.hadoop.yarn.server.router.webapp.FederationInterceptorREST: javax.ws.rs.core.Response createNewApplication(javax.servlet.http.HttpServletRequest)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer allocate(org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.PendingAsk,org.apache.hadoop.yarn.api.records.Container)>
<org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor: java.lang.String[] getRunCommand(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hdfs.util.IOUtilsClient: void cleanup(org.slf4j.Logger,java.io.Closeable[])>
<org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter: boolean isValidUrl(java.lang.String)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void removeStaleReplicas(java.util.List,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor: org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor$DockerContainerStatus getContainerStatus(java.lang.String,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor)>
<org.apache.hadoop.ha.ZKFailoverController: void verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState)>
<org.apache.hadoop.hdfs.server.datanode.VolumeScanner$ScanResultHandler: void handle(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.io.IOException)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue: void addChildQueue(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: void pruneExpiredPending(long)>
<org.apache.hadoop.ha.ShellCommandFencer: java.lang.String tryGetPid(java.lang.Process)>
<org.apache.hadoop.mapreduce.lib.input.FileInputFormat: java.util.List listStatus(org.apache.hadoop.mapreduce.JobContext)>
<org.apache.hadoop.metrics2.sink.StatsDSink$StatsD: void write(java.lang.String)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue: void updateDemand()>
<org.apache.hadoop.ipc.ClientCache: void stopClient(org.apache.hadoop.ipc.Client)>
<org.apache.hadoop.jmx.JMXJsonServlet: void writeAttribute(org.codehaus.jackson.JsonGenerator,javax.management.ObjectName,javax.management.MBeanAttributeInfo)>
<org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.PATHCONF3Response pathconf(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo: void writeEvent(org.apache.hadoop.mapreduce.jobhistory.HistoryEvent)>
<org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage: org.apache.hadoop.mapreduce.v2.app.job.Job loadJob(org.apache.hadoop.mapreduce.v2.api.records.JobId)>
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void checkAccess(java.io.OutputStream,boolean,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.datatransfer.Op,org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier$AccessMode)>
<org.apache.hadoop.mapreduce.CryptoUtils: java.io.InputStream wrapIfNecessary(org.apache.hadoop.conf.Configuration,java.io.InputStream,long)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: void handleStoreEvent(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent)>
<org.apache.hadoop.security.SaslRpcClient$SaslClientCallbackHandler: void handle(javax.security.auth.callback.Callback[])>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: boolean checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.QueueACL,java.lang.String)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl: void load()>
<org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testRURReplicas()>
<org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.FSSTAT3Response fsstat(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
<org.apache.hadoop.mapred.JobACLsManager: boolean checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.mapreduce.JobACL,java.lang.String,org.apache.hadoop.security.authorize.AccessControlList)>
<org.apache.hadoop.net.NetworkTopology: void add(org.apache.hadoop.net.Node)>
<org.apache.hadoop.security.LdapGroupsMapping: java.util.List doGetGroups(java.lang.String,int)>
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>
<org.apache.hadoop.metrics2.impl.TestMetricsConfig: void testInstances(org.apache.hadoop.metrics2.impl.MetricsConfig)>
<org.apache.hadoop.hdfs.client.impl.BlockReaderLocalLegacy: org.apache.hadoop.hdfs.protocol.BlockLocalPathInfo getBlockPathInfo(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.security.token.Token,boolean,org.apache.hadoop.fs.StorageType)>
<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: void decResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)>
<org.apache.hadoop.mapred.BackupStore$MemoryCache: void createInMemorySegment()>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void markBlockAsCorrupt(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
<org.apache.hadoop.yarn.server.timeline.LogInfo: long parsePath(org.apache.hadoop.yarn.server.timeline.TimelineDataManager,org.apache.hadoop.fs.Path,boolean,org.codehaus.jackson.JsonFactory,org.codehaus.jackson.map.ObjectMapper,org.apache.hadoop.fs.FileSystem)>
<org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor: void copyFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.lang.String)>
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.protocol.LocatedBlock getAdditionalBlock(java.lang.String,long,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],java.lang.String[],java.util.EnumSet)>
<org.apache.hadoop.yarn.util.WindowsBasedProcessTree: java.util.Map createProcessInfo(java.lang.String)>
<org.apache.hadoop.service.CompositeService: void serviceInit(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.yarn.server.timeline.security.TimelineACLsManager: boolean checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.timeline.TimelineDomain)>
<org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void storeContainerUpdateToken(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.security.ContainerTokenIdentifier)>
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager: void activate(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient: org.apache.hadoop.hdfs.server.federation.router.ConnectionContext getConnection(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.String,java.lang.Class)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void queueReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.lang.String)>
<org.apache.hadoop.oncrpc.RpcProgram: boolean doPortMonitoring(java.net.SocketAddress)>
<org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils: org.apache.hadoop.security.UserGroupInformation verifyAdminAccess(org.apache.hadoop.yarn.security.YarnAuthorizationProvider,java.lang.String,java.lang.String,org.apache.commons.logging.Log)>
<org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: void handleContainerStatus(java.util.List)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.Allocation allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.util.List,java.util.List,java.util.List,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask: boolean moveFiles()>
<org.apache.hadoop.util.concurrent.AsyncGetFuture: void callAsyncGet(long,java.util.concurrent.TimeUnit)>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx$COMMIT_STATUS handleSpecialWait(boolean,long,org.jboss.netty.channel.Channel,int,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes)>
<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void selectInputStreams(java.util.Collection,long,boolean)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController: int getClassIdFromFileContents(java.lang.String)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void updateApplicationStateInternal(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData)>
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand[] handleHeartbeat(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,org.apache.hadoop.hdfs.server.protocol.StorageReport[],java.lang.String,long,long,int,int,int,org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary,org.apache.hadoop.hdfs.server.protocol.SlowPeerReports,org.apache.hadoop.hdfs.server.protocol.SlowDiskReports)>
<org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall: org.apache.hadoop.io.retry.CallReturn processWaitTimeAndRetryInfo()>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void addToExcessReplicate(org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.Block)>
<org.apache.hadoop.yarn.webapp.Dispatcher: void removeCookie(javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String)>
<org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMDelegatedNodeLabelsUpdater: org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsMappingProvider createRMNodeLabelsMappingProvider(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)>
<org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor: void beforeExecute(java.lang.Thread,java.lang.Runnable)>
<org.apache.hadoop.io.SequenceFile$Sorter$SortPass: int run(boolean)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: org.apache.hadoop.yarn.api.records.Resource getMaximumAllocationPerQueue(java.lang.String)>
<org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyTokenSecretManager: byte[] retrievePassword(org.apache.hadoop.yarn.security.AMRMTokenIdentifier)>
<org.apache.hadoop.yarn.server.federation.utils.FederationRegistryClient: boolean writeAMRMTokenForUAM(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,org.apache.hadoop.security.token.Token)>
<org.apache.hadoop.hdfs.client.impl.BlockReaderLocal: int read(byte[],int,int)>
<org.apache.hadoop.hdfs.server.datanode.BlockReportTestBase: java.util.ArrayList writeFile(java.lang.String,long,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.MKDIR3Response mkdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor: java.lang.String[] getPrivilegedOperationExecutionCommand(java.util.List,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation)>
<org.apache.hadoop.hdfs.DFSInputStream: void checkInterrupted(java.io.IOException)>
<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair doSaslHandshake(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,java.util.Map,javax.security.auth.callback.CallbackHandler)>
<org.apache.hadoop.util.ApplicationClassLoader: java.net.URL getResource(java.lang.String)>
<org.apache.hadoop.mapred.YarnChild: void main(java.lang.String[])>
<org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl$ResourceRequestInfo remove(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,org.apache.hadoop.yarn.api.records.Resource)>
<org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: java.util.List getContainerStatuses()>
<org.apache.hadoop.mapreduce.util.MRAsyncDiskService$DeleteTask: void run()>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl: void updateCGroupParam(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController,java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache: void scan(long)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppAttemptTransition: org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMStateStoreState transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent)>
<org.apache.hadoop.yarn.webapp.GenericExceptionHandler: javax.ws.rs.core.Response toResponse(java.lang.Exception)>
<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.hdfs.DFSOutputStream create(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,boolean,short,long,org.apache.hadoop.util.Progressable,int,org.apache.hadoop.fs.Options$ChecksumOpt,java.net.InetSocketAddress[])>
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser: void run()>
<org.apache.hadoop.hdfs.nfs.nfs3.Nfs3Utils: void writeChannel(org.jboss.netty.channel.Channel,org.apache.hadoop.oncrpc.XDR,int)>
<org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler: void updateCgroup(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.nfs.NfsExports$CIDRMatch: boolean isIncluded(java.lang.String,java.lang.String)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController: java.util.Map readStats()>
<org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods: void init(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.HttpOpParam,org.apache.hadoop.hdfs.web.resources.Param[])>
<org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.COMMIT3Response commit(org.apache.hadoop.oncrpc.XDR,org.jboss.netty.channel.Channel,int,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: void doSingleWrite(org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx)>
<org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void addStreamsToCollectionFromFiles(java.util.Collection,java.util.Collection,long,long,boolean)>
<org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor: void processXml()>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: long removeLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: java.util.List chooseReplicasToDelete(java.util.Collection,int,java.util.List,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
<org.apache.hadoop.mapred.ShuffleHandler$Shuffle$3: void onRemoval(com.google.common.cache.RemovalNotification)>
<org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager: void removeStoredToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier)>
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: org.apache.hadoop.hdfs.server.datanode.BlockReceiver$Packet waitForAckHead(long)>
<org.apache.hadoop.mapreduce.util.MRAsyncDiskService: void <init>(org.apache.hadoop.fs.FileSystem,java.lang.String[])>
<org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.QueuePriorityContainerCandidateSelector: java.util.Map selectCandidates(java.util.Map,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>
<org.apache.hadoop.io.SequenceFile$Sorter: int sortPass(boolean)>
<org.apache.hadoop.net.unix.DomainSocketWatcher: void close()>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: boolean streamCleanup(long,long)>
<org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker: com.google.protobuf.Message invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])>
<org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore: boolean deleteNextEntity(java.lang.String,byte[],org.apache.hadoop.yarn.server.utils.LeveldbIterator,org.apache.hadoop.yarn.server.utils.LeveldbIterator,boolean)>
<org.apache.hadoop.mapred.BackupStore$MemoryCache: void write(org.apache.hadoop.io.DataInputBuffer,org.apache.hadoop.io.DataInputBuffer)>
<org.apache.hadoop.fs.contract.AbstractContractSeekTest: void testSeekReadClosedFile()>
<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition: void setup(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl)>
<org.apache.hadoop.yarn.client.api.impl.SharedCacheClientImpl: void serviceStart()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue: org.apache.hadoop.yarn.api.records.Resource assignContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode)>
<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair doSaslHandshake(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,java.lang.String,java.util.Map,javax.security.auth.callback.CallbackHandler)>
<org.apache.hadoop.fs.contract.AbstractFSContractTestBase: void teardown()>
<org.apache.hadoop.hdfs.server.namenode.FSDirConcatOp: org.apache.hadoop.fs.FileStatus concat(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,java.lang.String[],boolean)>
<org.apache.hadoop.hdfs.DFSClient: void initThreadsNumForHedgedReads(int)>
<org.apache.hadoop.yarn.server.timeline.EntityCacheItem: org.apache.hadoop.yarn.server.timeline.TimelineStore refreshCache(org.apache.hadoop.yarn.server.timeline.security.TimelineACLsManager,org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStoreMetrics)>
<org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter: void close()>
<org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testFinalizedRbwReplicas()>
<org.apache.hadoop.mapred.CleanupQueue$PathCleanupThread: void run()>
<org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: void add(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.protocol.DatanodeInfo,boolean)>
<org.apache.hadoop.hdfs.DFSInputStream: void seek(long)>
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager$Monitor: boolean exceededNumBlocksPerCheck()>
<org.apache.hadoop.ipc.Client$Connection: void setupConnection(org.apache.hadoop.security.UserGroupInformation)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.net.Node chooseTarget(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet,boolean)>
<org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor: void startLocalizer(org.apache.hadoop.yarn.server.nodemanager.executor.LocalizerStartContext)>
<org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore: long evictOldStartTimes(long)>
<org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler: org.apache.hadoop.security.authentication.server.AuthenticationToken authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>
<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.DfsClientShm requestNewShm(java.lang.String,org.apache.hadoop.hdfs.net.DomainPeer)>
<org.apache.hadoop.hdfs.tools.GetGroups: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void handle(org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent)>
<org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$SnapshotDiffSectionProcessor: void processFileDiffEntry()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: void containerCompleted(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType)>
<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl addAttempt(org.apache.hadoop.mapreduce.v2.api.records.Avataar)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void loadRMAppState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState)>
<org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor: org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor$Registrations registerWithNewSubClusters(java.util.Set)>
<org.apache.hadoop.ha.ActiveStandbyElector: void processWatchEvent(org.apache.zookeeper.ZooKeeper,org.apache.zookeeper.WatchedEvent)>
<org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler: org.apache.hadoop.security.authentication.server.AuthenticationToken authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>
<org.apache.hadoop.security.SaslRpcServer: javax.security.sasl.SaslServer create(org.apache.hadoop.ipc.Server$Connection,java.util.Map,org.apache.hadoop.security.token.SecretManager)>
<org.apache.hadoop.registry.client.impl.zk.RegistrySecurity: org.apache.zookeeper.data.ACL createACLForUser(org.apache.hadoop.security.UserGroupInformation,int)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue: boolean accept(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: org.apache.hadoop.yarn.api.records.Resource computeUserLimitAndSetHeadroom(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.api.records.Resource)>
<org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore: java.util.List getTimelineStoresFromCacheIds(java.util.Set,java.lang.String,java.util.List)>
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void rename2(java.lang.String,java.lang.String,org.apache.hadoop.fs.Options$Rename[])>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten append(java.lang.String,org.apache.hadoop.hdfs.server.datanode.FinalizedReplica,long,long)>
<org.apache.hadoop.mapreduce.JobSubmitter: void populateTokenCache(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)>
<org.apache.hadoop.hdfs.util.LightWeightHashSet: void <init>(int,float,float)>
<org.apache.hadoop.hdfs.server.federation.resolver.PathLocation: java.util.List orderedNamespaces(java.util.List,java.lang.String)>
<org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl: org.apache.hadoop.mapred.RawKeyValueIterator finalMerge(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,java.util.List,java.util.List)>
<org.apache.hadoop.fs.FsUrlConnection: void connect()>
<org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: void handleInternal(org.jboss.netty.channel.ChannelHandlerContext,org.apache.hadoop.oncrpc.RpcInfo)>
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void checkIfClusterIsNowMultiRack(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
<org.apache.hadoop.hdfs.qjournal.MiniJournalCluster: java.net.URI getQuorumJournalURI(java.lang.String)>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: void alterWriteRequest(org.apache.hadoop.nfs.nfs3.request.WRITE3Request,long)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue: void removeChildQueue(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue)>
<org.apache.hadoop.examples.dancing.DancingLinks: void coverColumn(org.apache.hadoop.examples.dancing.DancingLinks$ColumnHeader)>
<org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl$ResourceRequestInfo decResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionTypeRequest,org.apache.hadoop.yarn.api.records.Resource,java.lang.Object)>
<org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule: boolean logout()>
<org.apache.hadoop.registry.client.impl.zk.CuratorService: org.apache.curator.framework.CuratorFramework createCurator()>
<org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: void addPersistedPassword(org.apache.hadoop.security.token.Token)>
<org.apache.hadoop.hdfs.net.DFSNetworkTopology: org.apache.hadoop.net.Node chooseRandomWithStorageTypeTwoTrial(java.lang.String,java.util.Collection,org.apache.hadoop.fs.StorageType)>
<org.apache.hadoop.fs.FSInputStream: int read(long,byte[],int,int)>
<org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater: void reinit(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector: boolean doRecovery()>
<org.apache.hadoop.hdfs.client.impl.BlockReaderLocal: long skip(long)>
<org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap: void addToCorruptReplicasMap(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.lang.String,org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap$Reason)>
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand blockReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,org.apache.hadoop.hdfs.server.protocol.StorageBlockReport[],org.apache.hadoop.hdfs.server.protocol.BlockReportContext)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl: java.lang.String getNextSubDir(java.lang.String,java.io.File)>
<org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall: org.apache.hadoop.io.retry.CallReturn invoke()>
<org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore: org.apache.hadoop.yarn.api.records.timeline.TimelineEntity getEntity(java.lang.String,java.lang.String,java.util.EnumSet)>
<org.apache.hadoop.hdfs.security.token.delegation.DelegationUtilsClient: org.apache.hadoop.security.Credentials getDTfromRemote(org.apache.hadoop.hdfs.web.URLConnectionFactory,java.net.URI,java.lang.String,java.lang.String)>
<org.apache.hadoop.yarn.server.utils.YarnServerSecurityUtils: org.apache.hadoop.security.Credentials parseCredentials(org.apache.hadoop.yarn.api.records.ContainerLaunchContext)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: org.apache.hadoop.yarn.api.records.Resource computeUserLimit(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,boolean)>
<org.apache.hadoop.yarn.ipc.YarnRPC: org.apache.hadoop.yarn.ipc.YarnRPC create(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager: void removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)>
<org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp: org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp$FileState analyzeFileState(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,long,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.LocatedBlock[])>
<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void registerSlot(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,boolean)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime: org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation buildLaunchOp(org.apache.hadoop.yarn.server.nodemanager.containermanager.runtime.ContainerRuntimeContext,java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerRunCommand)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: java.lang.String[] getQueues(java.lang.String)>
<org.apache.hadoop.hdfs.server.namenode.Checkpointer: void doCheckpoint()>
<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void wipeDatanode(org.apache.hadoop.hdfs.protocol.DatanodeID)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processMisReplicatesAsync()>
<org.apache.hadoop.mapred.TestBadRecords: void validateOutput(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RunningJob,java.util.List,java.util.List)>
<org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL: java.net.HttpURLConnection openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)>
<org.apache.hadoop.yarn.server.resourcemanager.timelineservice.RMTimelineCollectorManager: void doPostPut(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollector)>
<org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: void copySucceeded(org.apache.hadoop.mapreduce.TaskAttemptID,org.apache.hadoop.mapreduce.task.reduce.MapHost,long,long,long,org.apache.hadoop.mapreduce.task.reduce.MapOutput)>
<org.apache.hadoop.service.launcher.ServiceLauncher: void verifyConfigurationFilesExist(java.lang.String[])>
<org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler: boolean managementOperation(org.apache.hadoop.security.authentication.server.AuthenticationToken,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>
<org.apache.hadoop.hdfs.server.federation.store.StateStoreConnectionMonitorService: void periodicInvoke()>
<org.apache.hadoop.service.AbstractService: void noteFailure(java.lang.Exception)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: boolean checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.QueueACL,java.lang.String)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams: void <init>(java.io.OutputStream,java.io.OutputStream,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi,org.apache.hadoop.hdfs.server.datanode.FileIoProvider)>
<org.apache.hadoop.hdfs.web.TestWebHdfsTimeouts: void tearDown()>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: void cleanupContainer()>
<org.apache.hadoop.yarn.server.uam.UnmanagedApplicationManager: void allocateAsync(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest,org.apache.hadoop.yarn.util.AsyncCallback)>
<org.apache.hadoop.hdfs.server.federation.resolver.order.HashResolver: java.lang.String extractTempFileName(java.lang.String)>
<org.apache.hadoop.hdfs.server.namenode.FSDirectory: byte[][] constructRemainingPath(byte[][],byte[][],int)>
<org.apache.hadoop.security.KDiag: void loginFromKeytab()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: void dumpSchedulerState()>
<org.apache.hadoop.jmx.JMXJsonServlet: void listBeans(org.codehaus.jackson.JsonGenerator,javax.management.ObjectName,java.lang.String,javax.servlet.http.HttpServletResponse)>
<org.apache.hadoop.hdfs.server.namenode.StreamFile: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>
<org.apache.hadoop.ipc.Server$ConnectionManager: org.apache.hadoop.ipc.Server$Connection register(java.nio.channels.SocketChannel)>
<org.apache.hadoop.net.NetworkTopology: org.apache.hadoop.net.Node chooseRandom(org.apache.hadoop.net.InnerNode,org.apache.hadoop.net.Node,java.util.Collection,int,int)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor: java.lang.String executeDockerCommand(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommand,java.lang.String,java.util.Map,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor,boolean)>
<org.apache.hadoop.mapreduce.lib.input.TestCombineSequenceFileInputFormat: void testFormat()>
<org.apache.hadoop.metrics2.util.MBeans: void unregister(javax.management.ObjectName)>
<org.apache.hadoop.hdfs.server.datanode.BlockSender: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,boolean,org.apache.hadoop.hdfs.server.datanode.DataNode,java.lang.String,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: void setUserLimit(java.lang.String,int)>
<org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService: void removeToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier)>
<org.apache.hadoop.hdfs.DFSInputStream: long readBlockLength(org.apache.hadoop.hdfs.protocol.LocatedBlock)>
<org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: void preallocate()>
<org.apache.hadoop.hdfs.server.namenode.FSImage: void renameImageFileInDir(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,long,boolean)>
<org.apache.hadoop.yarn.webapp.Router: boolean prefixMatches(org.apache.hadoop.yarn.webapp.Router$Dest,java.lang.String)>
<org.apache.hadoop.ha.ActiveStandbyElector: void becomeStandby()>
<org.apache.hadoop.hdfs.client.impl.BlockReaderLocal: boolean fillDataBuf(boolean)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: void completedContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: void tryCommit(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)>
<org.apache.hadoop.net.unix.DomainSocketWatcher: boolean sendCallback(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)>
<org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp: boolean unprotectedRemoveBlock(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INodeFile,org.apache.hadoop.hdfs.protocol.Block)>
<org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: void setattrInternal(org.apache.hadoop.hdfs.DFSClient,java.lang.String,org.apache.hadoop.nfs.nfs3.request.SetAttr3,boolean)>
<org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp: org.apache.hadoop.fs.FileEncryptionInfo getFileEncryptionInfo(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath)>
<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: org.apache.hadoop.metrics2.impl.MetricsSystemImpl$InitMode initMode()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.ActiveUsersManager: void deactivateApplication(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId)>
<org.apache.hadoop.mapred.YARNRunner: java.util.Map setupLocalResources(org.apache.hadoop.conf.Configuration,java.lang.String)>
<org.apache.hadoop.hdfs.tools.DelegationTokenFetcher$1: java.lang.Object run()>
<org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$NMCallbackHandler: void onContainerStarted(org.apache.hadoop.yarn.api.records.ContainerId,java.util.Map)>
<org.apache.hadoop.crypto.key.JavaKeyStoreProvider: void resetKeyStoreState(org.apache.hadoop.fs.Path)>
<org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline: org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams createStreams(boolean,org.apache.hadoop.util.DataChecksum)>
<org.apache.hadoop.mapred.FileInputFormat: org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.mapred.JobConf)>
<org.apache.hadoop.ipc.ClientCache: org.apache.hadoop.ipc.Client getClient(org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,java.lang.Class)>
<org.apache.hadoop.ipc.Server$Connection: void unwrapPacketAndProcessRpcs(byte[])>
<org.apache.hadoop.registry.client.impl.zk.RegistrySecurity: void initSecurity()>
<org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.IntraQueueCandidatesSelector: void preemptFromLeastStarvedApp(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,java.util.Map,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,java.util.Map,java.util.Map)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: boolean accept(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)>
<org.apache.hadoop.crypto.CryptoCodec: java.util.List getCodecClasses(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: org.apache.hadoop.yarn.api.records.Resource assignContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.PendingAsk,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType,boolean,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey)>
<org.apache.hadoop.hdfs.nfs.nfs3.WriteManager: void handleWrite(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.request.WRITE3Request,org.jboss.netty.channel.Channel,int,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes)>
<org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext)>
<org.apache.hadoop.hdfs.client.impl.BlockReaderRemote2: int read(byte[],int,int)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler: int assignContainersOnNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey)>
<org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery$9: void run()>
<org.apache.hadoop.io.retry.RetryInvocationHandler$Call: org.apache.hadoop.io.retry.CallReturn processWaitTimeAndRetryInfo()>
<org.apache.hadoop.mapred.Merger$MergeQueue: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path[],boolean,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.RawComparator,org.apache.hadoop.util.Progressable,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.mapreduce.TaskType)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController: java.lang.String readState()>
<org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler: void handle(javax.security.auth.callback.Callback[])>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType getAllowedLocalityLevelByTime(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,long,long,long)>
<org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.FifoIntraQueuePreemptionPlugin: java.util.Map getResourceDemandFromAppsPerQueue(java.lang.String,java.lang.String)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: void markContainerForKillable(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)>
<org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$NameSectionProcessor: void process()>
<org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$1: boolean removeEldestEntry(java.util.Map$Entry)>
<org.apache.hadoop.service.AbstractService: void stop()>
<org.apache.hadoop.hdfs.client.impl.DfsClientConf$ShortCircuitConf: void <init>(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler: void addApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String,boolean)>
<org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$LogFDsCache$FlushTimerTask: void run()>
<org.apache.hadoop.hdfs.client.impl.LeaseRenewer: void run(int)>
<org.apache.hadoop.hdfs.server.namenode.TestNameNodeRecovery: void testNameNodeRecoveryImpl(org.apache.hadoop.hdfs.server.namenode.TestNameNodeRecovery$Corruptor,boolean)>
<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: java.net.URL getInfoServer()>
<org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMDistributedNodeLabelsHandler: java.util.Set getNodeLabelsForHeartbeat()>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$PublicLocalizer: void addResource(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerResourceRequestEvent)>
<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void removeShm(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm)>
<org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: void addDirectoryToSerialNumberIndex(org.apache.hadoop.fs.Path)>
<org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$CallbackHandlerThread: void run()>
<org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: void startMBeans()>
<org.apache.hadoop.oncrpc.RpcProgram: void messageReceived(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.MessageEvent)>
<org.apache.hadoop.hdfs.DFSInputStream: boolean tokenRefetchNeeded(java.io.IOException,java.net.InetSocketAddress)>
<org.apache.hadoop.hdfs.DFSOutputStream: void closeImpl()>
<org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: void inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: long requestLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
<org.apache.hadoop.hdfs.server.federation.store.StateStoreCacheUpdateService: void periodicInvoke()>
<org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: void handle(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent)>
<org.apache.hadoop.ipc.Server: java.util.List getAuthMethods(org.apache.hadoop.security.token.SecretManager,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.ipc.WritableRpcEngine$Invoker: java.lang.Object invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])>
<org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager: void storeNewToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier,long)>
<org.apache.hadoop.ha.ActiveStandbyElector: void ensureParentZNode()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: void setCapacity(java.lang.String,float)>
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: boolean rename(java.lang.String,java.lang.String)>
<org.apache.hadoop.util.FileBasedIPList: java.lang.String[] readLines(java.lang.String)>
<org.apache.hadoop.io.retry.LossyRetryInvocationHandler: java.lang.Object invokeMethod(java.lang.reflect.Method,java.lang.Object[])>
<org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler: void exceptionCaught(io.netty.channel.ChannelHandlerContext,java.lang.Throwable)>
<org.apache.hadoop.test.LambdaTestUtils: int await(int,java.util.concurrent.Callable,java.util.concurrent.Callable,org.apache.hadoop.test.LambdaTestUtils$TimeoutHandler)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerModule: org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl getCGroupsCpuResourceHandler(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)>
<org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$UserLogDir: void scanIfNeeded(org.apache.hadoop.fs.FileStatus)>
<org.apache.hadoop.mapreduce.task.reduce.Fetcher: void verifyConnection(java.net.URL,java.lang.String,java.lang.String)>
<org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor: void cacheAllocatedContainers(java.util.List,org.apache.hadoop.yarn.server.federation.store.records.SubClusterId)>
<org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor: void recover(java.util.Map)>
<org.apache.hadoop.hdfs.server.namenode.FSEditLog: void close()>
<org.apache.hadoop.ipc.Server$Connection: org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto processSaslToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)>
<org.apache.hadoop.crypto.key.kms.KMSClientProvider$4: org.apache.hadoop.security.token.Token run()>
<org.apache.hadoop.security.SaslRpcClient: void sendSaslMessage(java.io.OutputStream,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.protocol.Block addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)>
<org.apache.hadoop.security.SaslRpcClient: javax.security.sasl.SaslClient createSaslClient(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)>
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void removeDatanode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)>
<org.apache.hadoop.util.Shell$1: void run()>
<org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeDiskMetrics: void detectAndUpdateDiskOutliers(java.util.Map,java.util.Map,java.util.Map)>
<org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: javax.management.AttributeList getAttributes(java.lang.String[])>
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender: void sendLifelineIfDue()>
<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnfinalizedSegments()>
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: boolean mkdirs(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,boolean)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: void releaseContainer(org.apache.hadoop.yarn.api.records.ContainerId,boolean)>
<org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy: void containerBasedPreemptOrKill(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,org.apache.hadoop.yarn.api.records.Resource)>
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void handleLifeline(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,org.apache.hadoop.hdfs.server.protocol.StorageReport[],java.lang.String,long,long,int,int,int,org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment assignContainersToChildQueues(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.PlacementSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)>
<org.apache.hadoop.mapred.TestKeyValueTextInputFormat: void testFormat()>
<org.apache.hadoop.security.UserGroupInformation: void logPrivilegedAction(javax.security.auth.Subject,java.lang.Object)>
<org.apache.hadoop.security.authentication.server.MultiSchemeAuthenticationHandler: org.apache.hadoop.security.authentication.server.AuthenticationHandler initializeAuthHandler(java.lang.String,java.util.Properties)>
<org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: java.util.Map loadStartedResources(org.apache.hadoop.yarn.server.utils.LeveldbIterator,java.lang.String)>
<org.apache.hadoop.yarn.client.api.impl.TimelineWriter: com.sun.jersey.api.client.ClientResponse doPostingObject(java.lang.Object,java.lang.String)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: void killReservedContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: boolean saveNextReplica()>
<org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor: void readVersion()>
<org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: void scanIntermediateDirectory(org.apache.hadoop.fs.Path)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: java.lang.String getContainerPid(org.apache.hadoop.fs.Path)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: void addApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String,boolean)>
<org.apache.hadoop.hdfs.server.datanode.BlockReportTestBase: void waitForTempReplica(org.apache.hadoop.hdfs.protocol.Block,int)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: void allocateResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)>
<org.apache.hadoop.metrics2.lib.MutableMetricsFactory: org.apache.hadoop.metrics2.lib.MutableMetric newForMethod(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.'annotation'.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.PlacementSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)>
<org.apache.hadoop.hdfs.NameNodeProxiesClient: org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider createFailoverProxyProvider(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,boolean,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.hdfs.server.namenode.ha.HAProxyFactory)>
<org.apache.hadoop.yarn.webapp.Controller: void renderJSON(java.lang.Object)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: void setChildQueues(java.util.Collection)>
<org.apache.hadoop.service.CompositeService: void serviceStart()>
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager: boolean isSufficientlyReplicated(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.BlockCollection,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas,boolean)>
<org.apache.hadoop.security.token.Token$PrivateToken: void <init>(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)>
<org.apache.hadoop.security.SaslRpcServer$SaslGssCallbackHandler: void handle(javax.security.auth.callback.Callback[])>
<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanFile(org.apache.hadoop.hdfs.protocol.CacheDirective,org.apache.hadoop.hdfs.server.namenode.INodeFile)>
<org.apache.hadoop.hdfs.server.namenode.TestEditLog: void testEditChecksum()>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void storeRMDTMasterKeyState(org.apache.hadoop.security.token.delegation.DelegationKey)>
<org.apache.hadoop.mapreduce.task.reduce.Fetcher: void copyFromHost(org.apache.hadoop.mapreduce.task.reduce.MapHost)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalCacheCleaner$LocalCacheCleanerStats handleCacheCleanup()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler: int assignContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,int,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType)>
<org.apache.hadoop.util.VersionInfo: void main(java.lang.String[])>
<org.apache.hadoop.hdfs.client.impl.BlockReaderFactory: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo createShortCircuitReplicaInfo()>
<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: java.lang.Object register(java.lang.String,java.lang.String,java.lang.Object)>
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void unref(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica)>
<org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule: boolean login()>
<org.apache.hadoop.hdfs.server.namenode.TestNameNodeRecovery: void runEditLogTest(org.apache.hadoop.hdfs.server.namenode.TestNameNodeRecovery$EditLogTestSetup)>
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo chooseStorage4Block(org.apache.hadoop.fs.StorageType,long)>
<org.apache.hadoop.io.retry.RetryInvocationHandler: org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo handleException(java.lang.reflect.Method,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.io.retry.RetryInvocationHandler$Counters,long,java.lang.Exception)>
<org.apache.hadoop.mapred.TestSortedRanges: void testRemove()>
<org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.FifoIntraQueuePreemptionPlugin: java.util.PriorityQueue createTempAppForResCalculation(org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TempQueuePerPartition,java.util.Collection,org.apache.hadoop.yarn.api.records.Resource,java.util.Map)>
<org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore: org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse put(org.apache.hadoop.yarn.api.records.timeline.TimelineEntities)>
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String,java.lang.String)>
<org.apache.hadoop.fs.FsShellPermissions$Chown: void processPath(org.apache.hadoop.fs.shell.PathData)>
<org.apache.hadoop.hdfs.server.namenode.FSDirSymlinkOp: org.apache.hadoop.fs.FileStatus createSymlinkInt(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,boolean)>
<org.apache.hadoop.util.NativeLibraryChecker: void main(java.lang.String[])>
<org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void storeContainerCompleted(org.apache.hadoop.yarn.api.records.ContainerId,int)>
<org.apache.hadoop.yarn.security.YarnAuthorizationProvider: void destroy()>
<org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: int updateAttrCache(java.lang.Iterable)>
<org.apache.hadoop.hdfs.DFSInputStream: org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair getBestNodeDNAddrPair(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection)>
<org.apache.hadoop.hdfs.server.namenode.NNThroughputBenchmark$TinyDatanode: boolean addBlock(org.apache.hadoop.hdfs.protocol.Block)>
<org.apache.hadoop.yarn.server.timeline.DomainLogInfo: long doParse(org.apache.hadoop.yarn.server.timeline.TimelineDataManager,org.codehaus.jackson.JsonParser,org.codehaus.jackson.map.ObjectMapper,org.apache.hadoop.security.UserGroupInformation,boolean)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue: boolean canAssignToThisQueue(org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)>
<org.apache.hadoop.oncrpc.RpcCall: void <init>(int,org.apache.hadoop.oncrpc.RpcMessage$Type,int,int,int,int,org.apache.hadoop.oncrpc.security.Credentials,org.apache.hadoop.oncrpc.security.Verifier)>
<org.apache.hadoop.registry.client.impl.zk.CuratorService: void serviceInit(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hdfs.tools.DFSHAAdmin: org.apache.hadoop.conf.Configuration addSecurityConfiguration(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.yarn.server.resourcemanager.RMAppManager: void handle(org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEvent)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl: void logLineFromTasksFile(java.io.File)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: void addApplicationOnRecovery(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Priority)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void invalidate(java.lang.String,org.apache.hadoop.hdfs.protocol.Block[],boolean)>
<org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30: void emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)>
<org.apache.hadoop.hdfs.client.impl.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getLegacyBlockReaderLocal()>
<org.apache.hadoop.yarn.server.nodemanager.NodeManager: void reregisterCollectors()>
<org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider: void initFileSystem(java.net.URI)>
<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocSlotFromExistingShm(org.apache.hadoop.hdfs.ExtendedBlockId)>
<org.apache.hadoop.io.retry.RetryInvocationHandler$Call: org.apache.hadoop.io.retry.CallReturn invokeOnce()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: boolean commonCheckContainerAllocation(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ContainerAllocationProposal,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.SchedulerContainer)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: void setQueues(java.lang.String,java.lang.String[])>
<org.apache.hadoop.hdfs.server.namenode.FSImage: long loadEdits(java.lang.Iterable,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>
<org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks$PendingReplicationMonitor: void pendingReplicationCheck()>
<org.apache.hadoop.hdfs.server.datanode.metrics.DataNodePeerMetrics: java.util.Map getOutliers()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: boolean moveReservation(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode)>
<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void addMap(org.apache.hadoop.mapreduce.v2.app.rm.ContainerRequestEvent)>
<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair connectToDN(org.apache.hadoop.hdfs.protocol.DatanodeInfo,int,org.apache.hadoop.hdfs.protocol.LocatedBlock)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: void removeApplicationAttemptInternal(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)>
<org.apache.hadoop.hdfs.server.federation.router.ConnectionPool: void close()>
<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocSlot(org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.net.DomainPeer,org.apache.commons.lang.mutable.MutableBoolean,org.apache.hadoop.hdfs.ExtendedBlockId,java.lang.String)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: void handleContainerExitCode(int,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: int updateCompletedContainers(java.util.List,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.NodeId)>
<org.apache.hadoop.mapred.Task: void reportNextRecordRange(org.apache.hadoop.mapred.TaskUmbilicalProtocol,long)>
<org.apache.hadoop.hdfs.server.namenode.CacheManager: void processCacheReportImpl(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.List)>
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo fetchOrCreate(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processQueuedMessages(java.lang.Iterable)>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx addWritesToCache(org.apache.hadoop.nfs.nfs3.request.WRITE3Request,org.jboss.netty.channel.Channel,int)>
<org.apache.hadoop.hdfs.DFSClient: java.net.SocketAddress getRandomLocalInterfaceAddr()>
<org.apache.hadoop.ipc.Server$Connection: void processOneRpc(java.nio.ByteBuffer)>
<org.apache.hadoop.hdfs.server.namenode.NameNode: void initializeGenericKeys(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: int loadRMDTSecretManagerKeys(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState)>
<org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$Forwarder$1: void operationComplete(io.netty.channel.ChannelFuture)>
<org.apache.hadoop.hdfs.server.datanode.BlockSender: long doSendBlock(java.io.DataOutputStream,java.io.OutputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler)>
<org.apache.hadoop.mapred.YARNRunner: org.apache.hadoop.yarn.api.records.ContainerLaunchContext setupContainerLaunchContextForAM(org.apache.hadoop.conf.Configuration,java.util.Map,java.nio.ByteBuffer,java.util.List)>
<org.apache.hadoop.yarn.server.security.BaseNMTokenSecretManager: byte[] createPassword(org.apache.hadoop.yarn.security.NMTokenIdentifier)>
<org.apache.hadoop.hdfs.qjournal.MiniJournalCluster: void <init>(org.apache.hadoop.hdfs.qjournal.MiniJournalCluster$Builder)>
<org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler: void onSuccess(org.apache.hadoop.hdfs.server.datanode.checker.VolumeCheckResult)>
<org.apache.hadoop.mapred.MapTask: org.apache.hadoop.mapred.MapOutputCollector createSortingCollector(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Task$TaskReporter)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport getAppResourceUsageReport(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)>
<org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.READLINK3Response readlink(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
<org.apache.hadoop.mapred.BackupStore: void reset()>
<org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler: void authenticateWithTlsExtension(java.lang.String,java.lang.String)>
<org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testErrorReplicas()>
<org.apache.hadoop.security.UserGroupInformation: void reloginFromKeytab()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: void markContainerForNonKillable(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)>
<org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void storeContainer(org.apache.hadoop.yarn.api.records.ContainerId,int,long,org.apache.hadoop.yarn.api.protocolrecords.StartContainerRequest)>
<org.apache.hadoop.service.CompositeService: void stop(int,boolean)>
<org.apache.hadoop.io.nativeio.NativeIO$POSIX: java.lang.String getName(org.apache.hadoop.io.nativeio.NativeIO$POSIX$IdCache,int)>
<org.apache.hadoop.examples.terasort.TeraScheduler: void pickBestSplits(org.apache.hadoop.examples.terasort.TeraScheduler$Host)>
<org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolTranslatorPB: void <init>(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.conf.Configuration,int,boolean)>
<org.apache.hadoop.hdfs.server.datanode.VolumeScanner: void markSuspectBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
<org.apache.hadoop.hdfs.web.HftpFileSystem$2: org.apache.hadoop.security.token.Token run()>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void removeRMDTMasterKeyState(org.apache.hadoop.security.token.delegation.DelegationKey)>
<org.apache.hadoop.security.TestLdapGroupsMapping: void testLdapConnectionTimeout()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment allocateContainerOnSingleNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.PlacementSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,boolean)>
<org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: void consume(org.apache.hadoop.metrics2.impl.MetricsBuffer)>
<org.apache.hadoop.hdfs.server.namenode.TestLargeDirectoryDelete$TestThread: void endThread()>
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: void <init>(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$ShmId,java.io.FileInputStream)>
<org.apache.hadoop.hdfs.server.namenode.CacheManager: void processCacheReport(org.apache.hadoop.hdfs.protocol.DatanodeID,java.util.List)>
<org.apache.hadoop.yarn.security.ContainerTokenIdentifier: void write(java.io.DataOutput)>
<org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$ActiveLogParser: void run()>
<org.apache.hadoop.io.compress.zstd.ZStandardCompressor: void reinit(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: void processNodeUpdate(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppNodeUpdateEvent$RMAppNodeUpdateType,org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode)>
<org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler: void createCgroup(java.lang.String,java.lang.String)>
<org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback: void <init>()>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void uncacheBlock(java.lang.String,long)>
<org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager: void sendIBRs(org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol,org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String)>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache: boolean put(org.apache.hadoop.nfs.nfs3.FileHandle,org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx)>
<org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService: void recover()>
<org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testFailedReplicaUpdate()>
<org.apache.hadoop.test.LambdaTestUtils: java.lang.Object eventually(int,java.util.concurrent.Callable,java.util.concurrent.Callable)>
<org.apache.hadoop.crypto.key.JavaKeyStoreProvider: org.apache.hadoop.fs.permission.FsPermission tryLoadIncompleteFlush(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.ipc.Server$Handler: void run()>
<org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: void populateNMTokens(java.util.List)>
<org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: boolean putMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer,long)>
<org.apache.hadoop.util.ShutdownHookManager$1: void run()>
<org.apache.hadoop.hdfs.web.URLConnectionFactory: java.net.URLConnection openConnection(java.net.URL,boolean)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void bumpReplicaGS(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,long)>
<org.apache.hadoop.hdfs.DFSInputStream: long fetchLocatedBlocksAndGetLastBlockLength(boolean)>
<org.apache.hadoop.tracing.TraceAdmin: int run(java.lang.String[])>
<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void assign(java.util.List)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: void allocateContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,boolean)>
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void purge(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica)>
<org.apache.hadoop.mapred.pipes.BinaryProtocol: void abort()>
<org.apache.hadoop.hdfs.server.namenode.ImageServlet: boolean isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt: boolean commonReserve(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.Resource)>
<org.apache.hadoop.hdfs.client.impl.LeaseRenewer: void renew()>
<org.apache.hadoop.mapred.ShuffleHandler$Shuffle: void setResponseHeaders(org.jboss.netty.handler.codec.http.HttpResponse,boolean,long)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void waitVolumeRemoved(int,java.util.concurrent.locks.Condition)>
<org.apache.hadoop.crypto.key.kms.KMSClientProvider: java.lang.Object call(java.net.HttpURLConnection,java.util.Map,int,java.lang.Class,int)>
<org.apache.hadoop.hdfs.server.federation.store.impl.MembershipStoreImpl: org.apache.hadoop.hdfs.server.federation.store.records.MembershipState getRepresentativeQuorum(java.util.Collection)>
<org.apache.hadoop.mapred.SortedRanges$SkipRangeIterator: void doNext()>
<org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: void refreshScheduler(java.lang.String,org.apache.hadoop.yarn.api.records.ReservationDefinition,java.lang.String)>
<org.apache.hadoop.hdfs.TestEncryptionZones: void testDelegationToken()>
<org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindow: long getSum(long)>
<org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd: org.apache.hadoop.oncrpc.XDR dump(org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress)>
<org.apache.hadoop.mapred.TestBadRecords$BadMapper: void map(org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)>
<org.apache.hadoop.util.NativeCodeLoader: void <clinit>()>
<org.apache.hadoop.mapred.pipes.BinaryProtocol: void close()>
<org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testStopWorker(org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery$TestStopWorkerRunnable)>
<org.apache.hadoop.ha.ActiveStandbyElector: boolean reEstablishSession()>
<org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor: void tryStart()>
<org.apache.hadoop.io.file.tfile.Compression$Algorithm: org.apache.hadoop.io.compress.Decompressor getDecompressor()>
<org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.CREATE3Response create(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork scheduleReplication(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int)>
<org.apache.hadoop.hdfs.server.balancer.Dispatcher: long dispatchBlockMoves()>
<org.apache.hadoop.yarn.server.sharedcachemanager.metrics.ClientSCMMetrics: void <init>()>
<org.apache.hadoop.fs.StreamCapabilitiesPolicy: void unbuffer(java.io.InputStream)>
<org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync: void logSync()>
<org.apache.hadoop.io.ReadaheadPool: org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest submitReadahead(java.lang.String,java.io.FileDescriptor,long,long)>
<org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService: void storeTokenMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)>
<org.apache.hadoop.hdfs.server.datanode.VolumeScanner: void enableBlockPoolId(java.lang.String)>
<org.apache.hadoop.security.SaslRpcClient$WrappedInputStream: void readNextRpcPacket()>
<org.apache.hadoop.util.ApplicationClassLoader: java.lang.Class loadClass(java.lang.String,boolean)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: void submitLazyPersistTask(java.lang.String,long,long,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference)>
<org.apache.hadoop.security.authentication.client.AuthenticatedURL: void extractToken(java.net.HttpURLConnection,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)>
<org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor: void run()>
<org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.FifoCandidatesSelector: java.util.Map selectCandidates(java.util.Map,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: boolean delete(java.lang.String,boolean)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.PlacementSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)>
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void trimEvictionMaps()>
<org.apache.hadoop.hdfs.server.namenode.CacheManager: long validateExpiryTime(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,long)>
<org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation: void init(java.lang.Class)>
<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: int incrementCurrentKeyId()>
<org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source: long getBlockList()>
<org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void storeApplication(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto)>
<org.apache.hadoop.net.unix.DomainSocketWatcher$NotificationHandler: boolean handle(org.apache.hadoop.net.unix.DomainSocket)>
<org.apache.hadoop.fs.contract.ContractTestUtils: void noteAction(java.lang.String)>
<org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor: org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest)>
<org.apache.hadoop.hdfs.DFSOutputStream: void computePacketChunkSize(int,int)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: void storeOrUpdateRMDT(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long,boolean)>
<org.apache.hadoop.registry.client.impl.zk.CuratorService: void zkCreate(java.lang.String,org.apache.zookeeper.CreateMode,byte[],java.util.List)>
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void close()>
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>
<org.apache.hadoop.hdfs.server.datanode.DataNode: org.apache.hadoop.hdfs.protocol.BlockLocalPathInfo getBlockLocalPathInfo(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)>
<org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: void join()>
<org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocSlot(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.commons.lang.mutable.MutableBoolean,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId)>
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: java.util.zip.Checksum computePartialChunkCrc(long,long)>
<org.apache.hadoop.hdfs.server.namenode.JournalSet: org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest getEditLogManifest(long)>
<org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor: void run()>
<org.apache.hadoop.security.authentication.client.AuthenticatedURL$AuthCookieHandler: void setAuthCookie(java.net.HttpCookie)>
<org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void removeContainerQueued(org.apache.hadoop.yarn.api.records.ContainerId)>
<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: void addResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType)>
<org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: void handle(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEvent)>
<org.apache.hadoop.yarn.webapp.Router: org.apache.hadoop.yarn.webapp.Router$Dest lookupRoute(org.apache.hadoop.yarn.webapp.WebApp$HTTP,java.lang.String)>
<org.apache.hadoop.yarn.server.resourcemanager.NodesListManager: void decrInactiveNMMetrics(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode)>
<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: byte[][] loadINodeSection(java.io.InputStream)>
<org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: void addTimelineDelegationToken(org.apache.hadoop.yarn.api.records.ContainerLaunchContext)>
<org.apache.hadoop.mapred.ClientCache: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol instantiateHistoryProxy()>
<org.apache.hadoop.ipc.DecayRpcScheduler: int cachedOrComputedPriorityLevel(java.lang.Object)>
<org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: void handle(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEvent)>
<org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem: java.io.OutputStream createOutputStreamWithMode(org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.fs.permission.FsPermission)>
<org.apache.hadoop.net.unix.DomainSocketWatcher: void addNotificationSocket(java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet)>
<org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread: void nextOp()>
<org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp: org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp$RenameResult renameToInt(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.Options$Rename[])>
<org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: void recover(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState)>
<org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule: boolean commit()>
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.util.List getDatanodeListForReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType)>
<org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter: java.lang.String ahsRedirectPath(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp)>
<org.apache.hadoop.hdfs.server.datanode.BlockReportTestBase: void startDNandWait(org.apache.hadoop.fs.Path,boolean)>
<org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService: void storeToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier,java.lang.Long)>
<org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory: void init(org.apache.hadoop.security.ssl.SSLFactory$Mode)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void removeVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>
<org.apache.hadoop.fs.FsShellPermissions$Chmod: void processPath(org.apache.hadoop.fs.shell.PathData)>
<org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl$TestSink: void init(org.apache.commons.configuration.SubsetConfiguration)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: void activateApplication(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId)>
<org.apache.hadoop.ipc.RPC$Server: org.apache.hadoop.ipc.RPC$Server$VerProtocolImpl getHighestSupportedProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)>
<org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem: void setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)>
<org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore: org.apache.hadoop.yarn.api.records.timeline.TimelineEntities getEntities(java.lang.String,java.lang.Long,java.lang.Long,java.lang.Long,java.lang.String,java.lang.Long,org.apache.hadoop.yarn.server.timeline.NameValuePair,java.util.Collection,java.util.EnumSet,org.apache.hadoop.yarn.server.timeline.TimelineDataManager$CheckAcl)>
<org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void storeContainerDiagnostics(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.StringBuilder)>
<org.apache.hadoop.hdfs.server.datanode.DataNode: void <init>(org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.hdfs.server.datanode.checker.StorageLocationChecker,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)>
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void addDatanode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void doTailEdits()>
<org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp$EDEKCacheLoader: void run()>
<org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.READDIRPLUS3Response readdirplus(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
<org.apache.hadoop.fs.shell.Command: void displayError(java.lang.Exception)>
<org.apache.hadoop.mapreduce.CryptoUtils: org.apache.hadoop.fs.FSDataOutputStream wrapIfNecessary(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,boolean)>
<org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector: java.util.List getLatestEditsFiles()>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void rescanPostponedMisreplicatedBlocks()>
<org.apache.hadoop.registry.client.impl.zk.CuratorService: java.util.List zkList(java.lang.String)>
<org.apache.hadoop.hdfs.TestDFSUpgradeFromImage: void testPreserveEditLogs()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.Allocation allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.util.List,java.util.List,java.util.List,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates)>
<org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore: void put(java.lang.String,byte[],boolean)>
<org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem: boolean mkOneDirWithMode(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.permission.FsPermission)>
<org.apache.hadoop.hdfs.DFSInputStream: void hedgedFetchBlockByteRange(org.apache.hadoop.hdfs.protocol.LocatedBlock,long,long,byte[],int,java.util.Map)>
<org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: boolean tryCloseProxy(org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData)>
<org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo addBlock(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[])>
<org.apache.hadoop.mapreduce.CryptoUtils: org.apache.hadoop.fs.FSDataInputStream wrapIfNecessary(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataInputStream)>
<org.apache.hadoop.hdfs.net.DFSTopologyNodeImpl: boolean add(org.apache.hadoop.net.Node)>
<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void updateToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation)>
<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: org.apache.hadoop.metrics2.MetricsSystem init(java.lang.String)>
<org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: void updateInfoCache(java.lang.Iterable)>
<org.apache.hadoop.hdfs.server.namenode.BackupImage: void applyEdits(long,int,byte[])>
<org.apache.hadoop.security.authentication.util.KerberosName: void <clinit>()>
<org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore: org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterPolicyConfigurationResponse getPolicyConfiguration(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterPolicyConfigurationRequest)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.security.LocalizerSecurityInfo$1: java.lang.Class value()>
<org.apache.hadoop.hdfs.PeerCache: void <init>(int,long)>
<org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$Forwarder: void exceptionCaught(io.netty.channel.ChannelHandlerContext,java.lang.Throwable)>
<org.apache.hadoop.mapreduce.lib.partition.InputSampler$RandomSampler: java.lang.Object[] getSample(org.apache.hadoop.mapreduce.InputFormat,org.apache.hadoop.mapreduce.Job)>
<org.apache.hadoop.hdfs.server.federation.router.ConnectionManager$CleanupTask: void run()>
<org.apache.hadoop.registry.server.services.RegistryAdminService: java.util.concurrent.Future submit(java.util.concurrent.Callable)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread: void run()>
<org.apache.hadoop.ipc.Server$Connection: void saslProcess(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)>
<org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore: java.util.List loadPlugIns(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testFinalizedRwrReplicas()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: boolean assignReservedContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processAndHandleReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue: void setEntitlement(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.QueueEntitlement)>
<org.apache.hadoop.fs.FileUtil: int chmod(java.lang.String,java.lang.String,boolean)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: void addApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,boolean,boolean)>
<org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.IntraQueueCandidatesSelector: void initializeUsageAndUserLimitForCompute(org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue,java.util.Map)>
<org.apache.hadoop.yarn.server.federation.store.utils.FederationStateStoreUtils: void setPassword(com.zaxxer.hikari.HikariDataSource,java.lang.String)>
<org.apache.hadoop.hdfs.DFSOutputStream: void writeChunk(byte[],int,int,byte[],int,int)>
<org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat: org.apache.hadoop.mapreduce.RecordReader createDBRecordReader(org.apache.hadoop.mapreduce.lib.db.DBInputFormat$DBInputSplit,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl: java.lang.String createCGroup(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController,java.lang.String)>
<org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster: org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse processTimelineResponseErrors(org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse)>
<org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices: javax.ws.rs.core.Response getContainerLogsInfo(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String)>
<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void processEventForTimelineServer(org.apache.hadoop.mapreduce.jobhistory.HistoryEvent,org.apache.hadoop.mapreduce.v2.api.records.JobId,long)>
<org.apache.hadoop.mapred.LocalJobRunner: void setupChildMapredLocalDirs(org.apache.hadoop.mapred.Task,org.apache.hadoop.mapred.JobConf)>
<org.apache.hadoop.hdfs.DFSOutputStream: void enqueueCurrentPacketFull()>
<org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask: void compute()>
<org.apache.hadoop.ipc.metrics.RetryCacheMetrics: void <init>(org.apache.hadoop.ipc.RetryCache)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner: void writeCredentials(org.apache.hadoop.fs.Path)>
<org.apache.hadoop.fs.FsUrlStreamHandlerFactory: java.net.URLStreamHandler createURLStreamHandler(java.lang.String)>
<org.apache.hadoop.portmap.RpcProgramPortmap: org.apache.hadoop.oncrpc.XDR unset(int,org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.XDR)>
<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void containerAssigned(org.apache.hadoop.yarn.api.records.Container,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor$ContainerRequest)>
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: void pruneStorageMap(org.apache.hadoop.hdfs.server.protocol.StorageReport[])>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.ContainerAllocation assignContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.PendingAsk,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits)>
<org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher: void handle(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent)>
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheCleaner: void run()>
<org.apache.hadoop.mapred.IndexCache: org.apache.hadoop.mapred.IndexRecord getIndexInformation(java.lang.String,int,org.apache.hadoop.fs.Path,java.lang.String)>
<org.apache.hadoop.hdfs.DataStreamer: boolean[] getPinnings(org.apache.hadoop.hdfs.protocol.DatanodeInfo[])>
<org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier: void check(java.lang.String[],java.lang.String[],java.lang.String[],boolean,boolean)>
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.protocol.LastBlockWithStatus append(java.lang.String,java.lang.String,org.apache.hadoop.io.EnumSetWritable)>
<org.apache.hadoop.hdfs.client.impl.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromDomain()>
<org.apache.hadoop.io.IOUtils: void cleanupWithLogger(org.slf4j.Logger,java.io.Closeable[])>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: org.apache.hadoop.yarn.api.records.NodeId getNodeIdToUnreserve(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource)>
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.fs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean,boolean,boolean[])>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: void apply(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)>
<org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$ProcessSmapMemoryInfo: void setMemInfo(java.lang.String,java.lang.String)>
<org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration: void setMemoryPerNode(java.lang.String,int)>
<org.apache.hadoop.security.ssl.SSLFactory: void disableExcludedCiphers(javax.net.ssl.SSLEngine)>
<org.apache.hadoop.fs.contract.AbstractFSContractTestBase: void handleExpectedException(java.lang.Exception)>
<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair receive(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,int,org.apache.hadoop.hdfs.protocol.DatanodeID)>
<org.apache.hadoop.hdfs.web.TokenAspect: void ensureTokenInitialized()>
<org.apache.hadoop.ipc.Client$Connection$1: java.lang.Object run()>
<org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem: boolean delete(org.apache.hadoop.fs.Path,boolean)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: java.lang.String getLeafZnodePath(java.lang.String,java.lang.String,int,boolean)>
<org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ReservedContainerCandidatesSelector: java.util.Map selectCandidates(java.util.Map,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>
<org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore: org.apache.hadoop.yarn.api.records.timeline.TimelineEvents getEntityTimelines(java.lang.String,java.util.SortedSet,java.lang.Long,java.lang.Long,java.lang.Long,java.util.Set)>
<org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys exportKeys()>
<org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$1: void run()>
<org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp: org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo deleteInternal(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,boolean)>
<org.apache.hadoop.yarn.server.federation.resolver.DefaultSubClusterResolverImpl: void load()>
<org.apache.hadoop.conf.Configuration: void handleDeprecation()>
<org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.SETATTR3Response setattr(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
<org.apache.hadoop.yarn.server.security.BaseContainerTokenSecretManager: byte[] retrievePasswordInternal(org.apache.hadoop.yarn.security.ContainerTokenIdentifier,org.apache.hadoop.yarn.server.security.MasterKeyData)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void loadApplicationAttemptState(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData,java.lang.String)>
<org.apache.hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewerForAcl: void createOriginalFSImage()>
<org.apache.hadoop.io.nativeio.NativeIO$Windows: void <clinit>()>
<org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: com.google.common.base.Optional schedule(org.apache.hadoop.hdfs.server.datanode.checker.Checkable,java.lang.Object)>
<org.apache.hadoop.hdfs.server.datanode.DataNode: void shutdown()>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockUnderConstructionFeature: void initializeBlockRecovery(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,long,boolean)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processIncrementalBlockReport(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks)>
<org.apache.hadoop.security.http.CrossOriginFilter: void doCrossFilter(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>
<org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.RENAME3Response rename(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
<org.apache.hadoop.hdfs.server.namenode.LeaseManager: void removeLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,long)>
<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$TooManyFetchFailureTransition: void transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent)>
<org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy: void logToCSV(java.util.List)>
<org.apache.hadoop.hdfs.server.federation.router.ConnectionManager: void cleanup(org.apache.hadoop.hdfs.server.federation.router.ConnectionPool)>
<org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler: boolean validateExpiration(com.nimbusds.jwt.SignedJWT)>
<org.apache.hadoop.registry.server.services.MicroZookeeperService: void serviceInit(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp: org.apache.hadoop.fs.FileStatus mkdirs(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean)>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: void processCommits(long)>
<org.apache.hadoop.security.Groups: org.apache.hadoop.security.Groups getUserToGroupsMappingService(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hdfs.DFSInotifyEventInputStream: org.apache.hadoop.hdfs.inotify.EventBatch take()>
<org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping: void <clinit>()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor: void addNode(java.util.List,org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode)>
<org.apache.hadoop.service.CompositeService: void addService(org.apache.hadoop.service.Service)>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: void trimWriteRequest(org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx,long)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: void setMaximumCapacity(java.lang.String,float)>
<org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager: void removeStoredToken(org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier)>
<org.apache.hadoop.ha.ActiveStandbyElector: boolean becomeActive()>
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.util.Collection listCorruptFileBlocks(java.lang.String,java.lang.String[])>
<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: void containerFailedOnHost(java.lang.String)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: void addApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,boolean,boolean)>
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager$Monitor: boolean exceededNumNodesPerCheck()>
<org.apache.hadoop.io.TestSequenceFile: void mergeTest(org.apache.hadoop.fs.FileSystem,int,int,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$CompressionType,boolean,int,int)>
<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: void checkSaslComplete(org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant,java.util.Map)>
<org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol: org.apache.hadoop.hdfs.protocol.HdfsFileStatus create(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,org.apache.hadoop.io.EnumSetWritable,boolean,short,long,org.apache.hadoop.crypto.CryptoProtocolVersion[])>
<org.apache.hadoop.mapreduce.v2.security.MRDelegationTokenRenewer: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol instantiateHistoryProxy(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress)>
<org.apache.hadoop.yarn.server.nodemanager.DockerContainerExecutor: boolean signalContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerSignalContext)>
<org.apache.hadoop.mapred.ClientServiceDelegate: java.lang.Object invoke(java.lang.String,java.lang.Class,java.lang.Object)>
<org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathResponseRunner: java.lang.Object getResponse(java.net.HttpURLConnection)>
<org.apache.hadoop.hdfs.client.impl.BlockReaderLocal: void close()>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processChosenExcessReplica(java.util.Collection,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)>
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager$Monitor: void processBlocksInternal(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.Iterator,java.util.List,boolean)>
<org.apache.hadoop.ipc.RetryCache: org.apache.hadoop.ipc.RetryCache$CacheEntry waitForCompletion(org.apache.hadoop.ipc.RetryCache$CacheEntry)>
<org.apache.hadoop.service.launcher.ServiceLauncher: org.apache.hadoop.util.ExitUtil$ExitException launchService(org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void handleApplicationAttemptStateOp(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationAttemptStateData,org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$AppAttemptOp)>
<org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage: java.util.Map getAllPartialJobs()>
<org.apache.hadoop.hdfs.DFSOutputStream: void flushOrSync(boolean,java.util.EnumSet)>
<org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker: java.util.Set checkAllVolumes(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi)>
<org.apache.hadoop.fs.FileUtil: java.lang.String[] createJarWithClassPath(java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Map)>
<org.apache.hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewer: void createOriginalFSImage()>
<org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$AttemptDirCache: org.apache.hadoop.fs.Path getAppRootDir(java.lang.String)>
<org.apache.hadoop.hdfs.client.impl.BlockReaderLocalLegacy: org.apache.hadoop.hdfs.client.impl.BlockReaderLocalLegacy newBlockReader(org.apache.hadoop.hdfs.client.impl.DfsClientConf,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeInfo,long,long,org.apache.hadoop.fs.StorageType,org.apache.htrace.core.Tracer)>
<org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter: void ignoreSnapshotName(long)>
<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map associateVolumeIdsWithBlocks(java.util.List,java.util.Map)>
<org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher: void logDecommissioningNodesStatus()>
<org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor: void writeStringTableSection()>
<org.apache.hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewerForXAttr: void createOriginalFSImage()>
<org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: org.apache.hadoop.yarn.server.api.records.NodeStatus getNodeStatus(int)>
<org.apache.hadoop.hdfs.DFSInotifyEventInputStream: org.apache.hadoop.hdfs.inotify.EventBatch poll(long,java.util.concurrent.TimeUnit)>
<org.apache.hadoop.mapreduce.lib.db.DBInputFormat: void closeConnection()>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficControlBandwidthHandlerImpl: java.util.List teardown()>
<org.apache.hadoop.hdfs.DFSInputStream: void close()>
<org.apache.hadoop.yarn.webapp.Dispatcher: void setMoreParams(org.apache.hadoop.yarn.webapp.Controller$RequestContext,java.lang.String,org.apache.hadoop.yarn.webapp.Router$Dest)>
<org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.QueuePriorityContainerCandidateSelector: void intializePriorityDigraph()>
<org.apache.hadoop.conf.Configuration: javax.xml.stream.XMLStreamReader parse(java.net.URL,boolean)>
<org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp: org.apache.hadoop.hdfs.server.namenode.INodesInPath unprotectedRenameTo(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INodesInPath,long)>
<org.apache.hadoop.registry.client.impl.zk.CuratorService: org.apache.zookeeper.data.Stat zkStat(java.lang.String)>
<org.apache.hadoop.yarn.webapp.Router: org.apache.hadoop.yarn.webapp.Router$Dest addWithOptionalDefaultView(org.apache.hadoop.yarn.webapp.WebApp$HTTP,java.lang.String,java.lang.Class,java.lang.String,java.util.List,boolean)>
<org.apache.hadoop.security.UserGroupInformation: void reloginFromTicketCache()>
<org.apache.hadoop.yarn.server.federation.store.metrics.FederationStateStoreClientMetrics: void <init>()>
<org.apache.hadoop.hdfs.server.datanode.DataNode: org.apache.hadoop.hdfs.server.protocol.InterDatanodeProtocol createInterDataNodeProtocolProxy(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.conf.Configuration,int,boolean)>
<org.apache.hadoop.util.ExitUtil: void halt(org.apache.hadoop.util.ExitUtil$HaltException)>
<org.apache.hadoop.hdfs.DataStreamer: void initDataStreaming()>
<org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl: java.io.BufferedWriter getWriter(java.lang.String)>
<org.apache.hadoop.ipc.Server$Connection: org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto buildSaslResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslState,byte[])>
<org.apache.hadoop.mapred.QueueManager: boolean hasAccess(java.lang.String,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.security.UserGroupInformation)>
<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: boolean shutdown()>
<org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager: void updateStoredToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier,long)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void storeApplicationStateInternal(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData)>
<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo: void closeWriter()>
<org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor: javax.xml.stream.events.XMLEvent expectTag(java.lang.String,boolean)>
<org.apache.hadoop.hdfs.TestClientReportBadBlock: void dfsClientReadFileFromPosition(org.apache.hadoop.fs.Path)>
<org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery$8: void run()>
<org.apache.hadoop.hdfs.server.namenode.TestEditLog: void testEditLogFailOverFromCorrupt()>
<org.apache.hadoop.security.SecurityUtil: void setTokenService(org.apache.hadoop.security.token.Token,java.net.InetSocketAddress)>
<org.apache.hadoop.hdfs.client.impl.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader getRemoteBlockReaderFromTcp()>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask: void run()>
<org.apache.hadoop.examples.dancing.DancingLinks: void uncoverColumn(org.apache.hadoop.examples.dancing.DancingLinks$ColumnHeader)>
<org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp: long delete(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,java.util.List,java.util.List,long)>
<org.apache.hadoop.crypto.key.kms.KMSClientProvider: org.apache.hadoop.security.UserGroupInformation getActualUgi()>
<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void addOrUpdateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey,boolean)>
<org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler: void logLineFromTasksFile(java.io.File)>
<org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices: javax.ws.rs.core.Response getLogs(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void ref(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica)>
<org.apache.hadoop.hdfs.server.datanode.DataNode: void checkDiskError()>
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot registerSlot(int,org.apache.hadoop.hdfs.ExtendedBlockId)>
<org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager: void storeNewMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)>
<org.apache.hadoop.mapreduce.lib.db.DataDrivenDBRecordReader: java.lang.String getSelectQuery()>
<org.apache.hadoop.yarn.server.resourcemanager.blacklist.SimpleBlacklistManager: org.apache.hadoop.yarn.api.records.ResourceBlacklistRequest getBlacklistUpdates()>
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>
<org.apache.hadoop.mapreduce.lib.input.FileInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
<org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: void put(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl$ResourceRequestInfo)>
<org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void <init>(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hdfs.server.namenode.ha.RequestHedgingProxyProvider: void logProxyException(java.lang.Exception,java.lang.String)>
<org.apache.hadoop.hdfs.TestClientReportBadBlock: void createAFileWithCorruptedBlockReplicas(org.apache.hadoop.fs.Path,short,int)>
<org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler: void run()>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: void handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent)>
<org.apache.hadoop.fi.ProbabilityModel: float getProbability(java.lang.String)>
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void enqueue(long,boolean,long,org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>
<org.apache.hadoop.hdfs.DataStreamer: void endBlock()>
<org.apache.hadoop.hdfs.web.resources.ExceptionHandler: javax.ws.rs.core.Response toResponse(java.lang.Exception)>
<org.apache.hadoop.hdfs.client.impl.BlockReaderLocalLegacy: long skip(long)>
<org.apache.hadoop.mapred.BackupStore$BackupRamManager: int reserve(int,int)>
<org.apache.hadoop.mapred.lib.MultithreadedMapRunner: void configure(org.apache.hadoop.mapred.JobConf)>
<org.apache.hadoop.yarn.server.resourcemanager.NodesListManager$CachedResolver$ExpireChecker: void run()>
<org.apache.hadoop.service.launcher.ServiceLauncher: int coreServiceLaunch(org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor: java.lang.String executePrivilegedOperation(java.util.List,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation,java.io.File,java.util.Map,boolean,boolean)>
<org.apache.hadoop.crypto.key.kms.KMSClientProvider: boolean containsKmsDt(org.apache.hadoop.security.UserGroupInformation)>
<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: java.net.URL toUrl(org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,org.apache.hadoop.fs.Path,org.apache.hadoop.hdfs.web.resources.Param[])>
<org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.PreemptableResourceCalculator: void calculateResToObtainByPartitionForLeafQueues(java.util.Set,org.apache.hadoop.yarn.api.records.Resource)>
<org.apache.hadoop.metrics2.impl.TestMetricsConfig: void testCommon()>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void loadReservationSystemState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState)>
<org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler: void exceptionCaught(io.netty.channel.ChannelHandlerContext,java.lang.Throwable)>
<org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void removeLocalizedResource(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: java.util.List invalidateWork(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
<org.apache.hadoop.service.launcher.ServiceLauncher: java.util.List parseCommandArgs(org.apache.hadoop.conf.Configuration,java.util.List)>
<org.apache.hadoop.hdfs.net.DFSTopologyNodeImpl: boolean remove(org.apache.hadoop.net.Node)>
<org.apache.hadoop.ipc.Server$Connection: void close()>
<org.apache.hadoop.hdfs.server.datanode.VolumeScanner: void run()>
<org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer: void serviceStop()>
<org.apache.hadoop.registry.client.impl.zk.RegistryOperationsService: org.apache.hadoop.registry.client.types.RegistryPathStatus stat(java.lang.String)>
<org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter: void buildNamespace(java.io.InputStream,java.util.List)>
<org.apache.hadoop.hdfs.server.namenode.FSDirConcatOp: void unprotectedConcat(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INodeFile[],long)>
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: boolean truncate(java.lang.String,long,java.lang.String)>
<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: org.apache.hadoop.security.SaslPropertiesResolver getSaslPropertiesResolver(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.yarn.event.AsyncDispatcher: void dispatch(org.apache.hadoop.yarn.event.Event)>
<org.apache.hadoop.yarn.server.sharedcachemanager.metrics.SharedCacheUploaderMetrics: void <init>()>
<org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void storeContainerPaused(org.apache.hadoop.yarn.api.records.ContainerId)>
<org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: void checkAccess(org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier$AccessMode)>
<org.apache.hadoop.mapreduce.v2.security.client.ClientHSTokenSelector: org.apache.hadoop.security.token.Token selectToken(org.apache.hadoop.io.Text,java.util.Collection)>
<org.apache.hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewerForContentSummary: void createOriginalFSImage()>
<org.apache.hadoop.hdfs.client.impl.BlockReaderLocal: org.apache.hadoop.hdfs.shortcircuit.ClientMmap getClientMmap(java.util.EnumSet)>
<org.apache.hadoop.hdfs.DFSClient: void <init>(java.net.URI,org.apache.hadoop.hdfs.protocol.ClientProtocol,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Statistics)>
<org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp: org.apache.hadoop.hdfs.server.namenode.INodesInPath addFile(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,byte[],org.apache.hadoop.fs.permission.PermissionStatus,short,long,java.lang.String,java.lang.String)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void removeStoredBlock(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
<org.apache.hadoop.mapred.pipes.BinaryProtocol$UplinkReaderThread: void run()>
<org.apache.hadoop.mapred.pipes.Application: void <init>(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter,java.lang.Class,java.lang.Class)>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: long updateNonSequentialWriteInMemory(long)>
<org.apache.hadoop.service.launcher.AbstractLaunchableService: org.apache.hadoop.conf.Configuration bindArgs(org.apache.hadoop.conf.Configuration,java.util.List)>
<org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService: void removeTokenMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)>
<org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: void scanIntermediateDirectory()>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl: void addCredentials()>
<org.apache.hadoop.security.token.delegation.web.DelegationTokenManager: long renewToken(org.apache.hadoop.security.token.Token,java.lang.String)>
<org.apache.hadoop.hdfs.server.datanode.DataNode: void checkBlockToken(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier$AccessMode)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer findNodeToUnreserve(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.Resource)>
<org.apache.hadoop.ha.ActiveStandbyElector: void enterNeutralMode()>
<org.apache.hadoop.registry.client.impl.zk.CuratorService: java.util.List zkGetACLS(java.lang.String)>
<org.apache.hadoop.registry.client.binding.RegistryUtils: java.util.Map extractServiceRecords(org.apache.hadoop.registry.client.api.RegistryOperations,java.lang.String,java.util.Collection)>
<org.apache.hadoop.hdfs.server.namenode.FSImage: void reloadFromImageFile(java.io.File,org.apache.hadoop.hdfs.server.namenode.FSNamesystem)>
<org.apache.hadoop.mapred.BackupStore$FileCache: void write(org.apache.hadoop.io.DataInputBuffer,org.apache.hadoop.io.DataInputBuffer)>
<org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor: org.apache.hadoop.yarn.api.protocolrecords.GetNewApplicationResponse getNewApplication(org.apache.hadoop.yarn.api.protocolrecords.GetNewApplicationRequest)>
<org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler: org.apache.hadoop.security.authentication.server.AuthenticationToken runWithPrincipal(java.lang.String,byte[],org.apache.commons.codec.binary.Base64,javax.servlet.http.HttpServletResponse)>
<org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: java.net.URI getTrackingUri(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,org.apache.hadoop.yarn.server.webproxy.AppReportFetcher$AppReportSource)>
<org.apache.hadoop.hdfs.server.datanode.TestCachingStrategy$Stats: void fadvise(int,int,int)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppAttemptTransition: org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMStateStoreState transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent)>
<org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager: void startMaintenance(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,long)>
<org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void storeContainerLaunched(org.apache.hadoop.yarn.api.records.ContainerId)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void updateRMDelegationTokenState(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long)>
<org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs: void loadDetailLog(org.apache.hadoop.yarn.server.timeline.TimelineDataManager,org.apache.hadoop.yarn.api.records.timeline.TimelineEntityGroupId)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void markBlockReplicasAsCorrupt(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,long,long,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[])>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo chooseFromNextRack(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)>
<org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter: void doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>
<org.apache.hadoop.metrics2.impl.TestSinkQueue: org.apache.hadoop.metrics2.impl.SinkQueue newSleepingConsumerQueue(int,int[])>
<org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$CheckedVolume: boolean isResourceAvailable()>
<org.apache.hadoop.yarn.security.NMTokenSelector: org.apache.hadoop.security.token.Token selectToken(org.apache.hadoop.io.Text,java.util.Collection)>
<org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices: javax.ws.rs.core.Response getLogs(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)>
<org.apache.hadoop.hdfs.server.namenode.FSDirTruncateOp: org.apache.hadoop.hdfs.protocol.Block prepareFileForTruncate(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.protocol.Block)>
<org.apache.hadoop.nfs.NfsExports$ExactMatch: boolean isIncluded(java.lang.String,java.lang.String)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: float internalGetLabeledQueueCapacity(java.lang.String,java.lang.String,java.lang.String,float)>
<org.apache.hadoop.hdfs.tools.federation.RouterAdmin: int run(java.lang.String[])>
<org.apache.hadoop.hdfs.TestPipelines: byte[] writeData(org.apache.hadoop.fs.FSDataOutputStream,int)>
<org.apache.hadoop.ipc.Server$ConnectionManager: boolean close(org.apache.hadoop.ipc.Server$Connection)>
<org.apache.hadoop.mapred.ClientServiceDelegate: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol instantiateAMProxy(java.net.InetSocketAddress)>
<org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer: void cancel(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs: void parseSummaryLogs(org.apache.hadoop.yarn.server.timeline.TimelineDataManager)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: void internalReleaseResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.api.records.Resource)>
<org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp: boolean unprotectedDelete(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INode$ReclaimContext,long)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void cacheBlock(long,java.lang.String,java.lang.String,long,long,java.util.concurrent.Executor)>
<org.apache.hadoop.oncrpc.RegistrationClient$RegistrationClientHandler: boolean validMessageLength(int)>
<org.apache.hadoop.mapred.pipes.BinaryProtocol: void authenticate(java.lang.String,java.lang.String)>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: void waitForDump()>
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void sendAckUpstreamUnprotected(org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck,long,long,long,int)>
<org.apache.hadoop.yarn.server.federation.policies.amrmproxy.LocalityMulticastAMRMProxyPolicy: java.util.Map splitResourceRequests(java.util.List)>
<org.apache.hadoop.yarn.util.ProcfsBasedProcessTree: void updateProcessTree()>
<org.apache.hadoop.yarn.util.ProcfsBasedProcessTree: void constructProcessSMAPInfo(org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$ProcessTreeSmapMemInfo,java.lang.String)>
<org.apache.hadoop.mapred.TaskLog: java.io.File getUserLogDir()>
<org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$EntityLogScanner: void run()>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void loadRMDelegationKeyState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState)>
<org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher: void putEntity(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.api.records.ApplicationId)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processFirstBlockReport(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.BlockListAsLongs)>
<org.apache.hadoop.mapred.BackupStore: void mark()>
<org.apache.hadoop.security.UserGroupInformation: void logoutUserFromKeytab()>
<org.apache.hadoop.hdfs.DataStreamer: void waitForAckedSeqno(long)>
<org.apache.hadoop.io.nativeio.NativeIO$POSIX: void <clinit>()>
<org.apache.hadoop.ha.ZKFailoverController: boolean confirmFormat()>
<org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void removeApplication(org.apache.hadoop.yarn.api.records.ApplicationId)>
<org.apache.hadoop.service.CompositeService: void serviceStop()>
<org.apache.hadoop.oncrpc.SimpleTcpClientHandler: void channelConnected(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelStateEvent)>
<org.apache.hadoop.registry.client.impl.zk.RegistrySecurity: boolean addDigestACL(org.apache.zookeeper.data.ACL)>
<org.apache.hadoop.yarn.server.security.BaseNMTokenSecretManager: byte[] retrivePasswordInternal(org.apache.hadoop.yarn.security.NMTokenIdentifier,org.apache.hadoop.yarn.server.security.MasterKeyData)>
<org.apache.hadoop.portmap.RpcProgramPortmap: org.apache.hadoop.oncrpc.XDR getport(int,org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.XDR)>
<org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall: boolean isDone()>
<org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback: void <init>()>
<org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.yarn.applications.distributedshell.Client: boolean monitorApplication(org.apache.hadoop.yarn.api.records.ApplicationId)>
<org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService: void writeAsync(org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx)>
<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$KillWaitAttemptKilledTransition: org.apache.hadoop.mapreduce.v2.app.job.TaskStateInternal transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent)>
<org.apache.hadoop.hdfs.web.WebHdfsFileSystem$ReadRunner: java.io.InputStream initializeInputStream(java.net.HttpURLConnection)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void findAndMarkBlockAsCorrupt(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo,java.lang.String,java.lang.String)>
<org.apache.hadoop.mapreduce.task.reduce.Fetcher: org.apache.hadoop.mapreduce.TaskAttemptID[] copyMapOutput(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.io.DataInputStream,java.util.Set,boolean)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.LocalitySchedulingPlacementSet: void showRequests()>
<org.apache.hadoop.io.compress.zlib.ZlibCompressor: void reinit(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService: org.apache.hadoop.hdfs.server.federation.resolver.NamenodeStatusReport getNamenodeStatusReport()>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: void handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent)>
<org.apache.hadoop.hdfs.net.DFSNetworkTopology: org.apache.hadoop.net.Node chooseRandomWithStorageType(java.lang.String,java.lang.String,java.util.Collection,org.apache.hadoop.fs.StorageType)>
<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$1: void run()>
<org.apache.hadoop.hdfs.DataStreamer: java.net.Socket createSocketForPipeline(org.apache.hadoop.hdfs.protocol.DatanodeInfo,int,org.apache.hadoop.hdfs.DFSClient)>
<org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: byte[] retrievePassword(org.apache.hadoop.yarn.security.AMRMTokenIdentifier)>
<org.apache.hadoop.metrics2.impl.MetricsConfig: java.lang.ClassLoader getPluginLoader()>
<org.apache.hadoop.yarn.client.api.impl.TimelineConnector: org.apache.hadoop.security.authentication.client.ConnectionConfigurator getConnConfigurator(org.apache.hadoop.security.ssl.SSLFactory)>
<org.apache.hadoop.hdfs.DFSInotifyEventInputStream: org.apache.hadoop.hdfs.inotify.EventBatch poll()>
<org.apache.hadoop.ipc.Client: void stop()>
<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCachedBlockMap()>
<org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.FifoIntraQueuePreemptionPlugin: void computeAppsIdealAllocation(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TempQueuePerPartition,java.util.Map,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,float)>
<org.apache.hadoop.hdfs.server.namenode.FSDirSymlinkOp: org.apache.hadoop.hdfs.server.namenode.INodeSymlink addSymlink(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,boolean)>
<org.apache.hadoop.util.LightWeightGSet: void <init>(int)>
<org.apache.hadoop.security.token.Token: java.lang.Class getClassForIdentifier(org.apache.hadoop.io.Text)>
<org.apache.hadoop.mapred.ShuffleHandler$Shuffle: org.apache.hadoop.mapred.ShuffleHandler$Shuffle$MapOutputInfo getMapOutputInfo(java.lang.String,int,java.lang.String,java.lang.String)>
<org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testRWRReplicas()>
<org.apache.hadoop.hdfs.server.datanode.VolumeScanner$ScanResultHandler: void setup(org.apache.hadoop.hdfs.server.datanode.VolumeScanner)>
<org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation,com.sun.jersey.api.client.Client,java.net.URI)>
<org.apache.hadoop.service.launcher.ServiceLauncher: int loadConfigurationClasses()>
<org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat$OneFileInfo: void <init>(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.conf.Configuration,boolean,java.util.HashMap,java.util.HashMap,java.util.HashMap,java.util.HashMap,long)>
<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey newDataEncryptionKey()>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo chooseLocalRack(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)>
<org.apache.hadoop.ipc.Client$Connection: void receiveRpcResponse()>
<org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp: org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp$RenameResult unprotectedRenameTo(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INodesInPath,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>
<org.apache.hadoop.mapred.TestSortedRanges: void testAdd()>
<org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider: void flush()>
<org.apache.hadoop.util.ExitUtil: void terminate(org.apache.hadoop.util.ExitUtil$ExitException)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: boolean canAssignToUser(org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits)>
<org.apache.hadoop.hdfs.server.datanode.BlockScanner: void addVolumeScanner(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference)>
<org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl: void run()>
<org.apache.hadoop.security.ShellBasedIdMapping: boolean updateMapInternal(com.google.common.collect.BiMap,java.lang.String,java.lang.String,java.lang.String,java.util.Map)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl: void unreference()>
<org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore: org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs getAndSetAppLogs(org.apache.hadoop.yarn.api.records.ApplicationId)>
<org.apache.hadoop.hdfs.server.namenode.BackupImage: boolean tryConvergeJournalSpool()>
<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: java.util.Map validateResponse(org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,java.net.HttpURLConnection,boolean)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: void deactivateApplication(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId)>
<org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker: java.util.Collection getVolumesLowOnSpace()>
<org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager: void updateStoredToken(org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier,long)>
<org.apache.hadoop.ipc.metrics.RpcDetailedMetrics: void <init>(int)>
<org.apache.hadoop.test.MetricsAsserts$1: java.lang.Object answer(org.mockito.invocation.InvocationOnMock)>
<org.apache.hadoop.ipc.metrics.RpcMetrics: void <init>(org.apache.hadoop.ipc.Server,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.io.nativeio.NativeIO: void <clinit>()>
<org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor: void loadNodeChildren(org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$Node,java.lang.String,java.lang.String[])>
<org.apache.hadoop.io.file.tfile.Compression$Algorithm: void returnCompressor(org.apache.hadoop.io.compress.Compressor)>
<org.apache.hadoop.hdfs.server.namenode.BackupImage: void setState(org.apache.hadoop.hdfs.server.namenode.BackupImage$BNState)>
<org.apache.hadoop.mapreduce.JobResourceUploader: void copyLog4jPropertyFile(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path,short)>
<org.apache.hadoop.hdfs.util.ByteArrayManager: void logDebugMessage()>
<org.apache.hadoop.yarn.security.client.RMDelegationTokenSelector: org.apache.hadoop.security.token.Token selectToken(org.apache.hadoop.io.Text,java.util.Collection)>
<org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: void updateNodeLabelsFromNMReport(java.util.Set,org.apache.hadoop.yarn.api.records.NodeId)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.io.File validateBlockFile(java.lang.String,long)>
<org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient: java.util.Map invokeConcurrent(java.util.Collection,org.apache.hadoop.hdfs.server.federation.router.RemoteMethod,boolean,boolean,long,java.lang.Class)>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache: void cleanAll()>
<org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy$RetryAction shouldRetry(java.lang.Exception,int,int,boolean)>
<org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1: void run()>
<org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractSchedulerPlanFollower: void synchronizePlan(org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,boolean)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.ContainerAllocation doAllocation(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.ContainerAllocation,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)>
<org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods: java.net.URI redirectURI(org.apache.hadoop.hdfs.server.federation.router.Router,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,long,java.lang.String,org.apache.hadoop.hdfs.web.resources.Param[])>
<org.apache.hadoop.ipc.Server$Connection: void authorizeConnection()>
<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: boolean replaceExpiredDelegationToken()>
<org.apache.hadoop.io.TestArrayFile: void writeTest(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.RandomDatum[],java.lang.String)>
<org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.LOOKUP3Response lookup(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void removeReservationState(java.lang.String,java.lang.String)>
<org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void cleanupKeysWithPrefix(java.lang.String)>
<org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: void <init>(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,java.lang.String)>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx$Dumper: void run()>
<org.apache.hadoop.util.ShutdownHookManager: void shutdownExecutor(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: void nodeUpdate(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode)>
<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean loadFSImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>
<org.apache.hadoop.hdfs.server.namenode.FSImage: boolean recoverTransitionRead(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean processReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,org.apache.hadoop.hdfs.server.protocol.BlockReportContext)>
<org.apache.hadoop.ha.ActiveStandbyElector: void createConnection()>
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: boolean truncate(java.lang.String,long,java.lang.String,java.lang.String,long)>
<org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$1: void updateTimelineCollectorData(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: void removeReservationState(java.lang.String,java.lang.String)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: void recoverTrackerResources(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTracker,org.apache.hadoop.yarn.server.nodemanager.recovery.NMStateStoreService$LocalResourceTrackerState)>
<org.apache.hadoop.mapred.CleanupQueue: boolean deletePath(org.apache.hadoop.mapred.CleanupQueue$PathDeletionContext)>
<org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void removeContainerPaused(org.apache.hadoop.yarn.api.records.ContainerId)>
<org.apache.hadoop.mapred.YARNRunner: void killJob(org.apache.hadoop.mapreduce.JobID)>
<org.apache.hadoop.hdfs.server.federation.router.PeriodicService$1: void run()>
<org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync: void enqueueEdit(org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync$Edit)>
<org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor: void removeFinishedContainersFromCache(java.util.List)>
<org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter: org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse putEntities(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.records.timeline.TimelineEntityGroupId,org.apache.hadoop.yarn.api.records.timeline.TimelineEntity[])>
<org.apache.hadoop.mapred.BackupStore$BackupRamManager: void unreserve(int)>
<org.apache.hadoop.ipc.Server$RpcCall: java.lang.Void run()>
<org.apache.hadoop.security.authentication.client.KerberosAuthenticator: void authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)>
<org.apache.hadoop.mapreduce.v2.hs.JobHistory: java.util.Map getAllJobs(org.apache.hadoop.yarn.api.records.ApplicationId)>
<org.apache.hadoop.mapreduce.TestLocalRunner$StressMapper: void cleanup(org.apache.hadoop.mapreduce.Mapper$Context)>
<org.apache.hadoop.hdfs.DataStreamer: void queuePacket(org.apache.hadoop.hdfs.DFSPacket)>
<org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue: org.apache.hadoop.io.SequenceFile$Sorter$RawKeyValueIterator merge()>
<org.apache.hadoop.hdfs.TestLargeBlock: void checkFullFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,long)>
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startActiveServices()>
<org.apache.hadoop.crypto.key.kms.KMSClientProvider: long renewDelegationToken(org.apache.hadoop.security.token.Token)>
<org.apache.hadoop.hdfs.client.impl.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader build()>
<org.apache.hadoop.hdfs.server.federation.router.ConnectionPool: void <init>(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.UserGroupInformation,int,int,java.lang.Class)>
<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void <init>(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.yarn.util.FSDownload: void changePermissions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.ha.SshFenceByTcpPort: int execCommand(com.jcraft.jsch.Session,java.lang.String)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void loadRMAppStateFromAppNode(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState,java.lang.String,java.lang.String)>
<org.apache.hadoop.registry.cli.RegistryCli: java.lang.String analyzeException(java.lang.String,java.lang.Exception,java.util.List)>
<org.apache.hadoop.security.JniBasedUnixGroupsMapping: java.util.List getGroups(java.lang.String)>
<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair getEncryptedStreams(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey)>
<org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler: boolean deleteCgroup(java.lang.String)>
<org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair getEncryptedStreams(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream)>
<org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator: boolean hasDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)>
<org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: void <init>(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.RMContext,org.apache.hadoop.yarn.server.resourcemanager.scheduler.YarnScheduler,org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService,org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp,org.apache.hadoop.yarn.server.resourcemanager.blacklist.BlacklistManager)>
<org.apache.hadoop.mapred.SortedRanges: void add(org.apache.hadoop.mapred.SortedRanges$Range)>
<org.apache.hadoop.ipc.Server: void wrapWithSasl(org.apache.hadoop.ipc.Server$RpcCall)>
<org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService: void storeTokenMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.PlacementSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)>
<org.apache.hadoop.registry.client.impl.zk.CuratorService: byte[] zkRead(java.lang.String)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: void recoverApplication(org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto)>
<org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$EntityLogFD: void writeEntities(java.util.List)>
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void <init>(org.apache.hadoop.hdfs.net.Peer,org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.datanode.DataXceiverServer)>
<org.apache.hadoop.hdfs.server.balancer.Dispatcher: boolean shouldIgnore(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>
<org.apache.hadoop.net.NetworkTopologyWithNodeGroup: void add(org.apache.hadoop.net.Node)>
<org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: boolean markMovedIfGoodBlock(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock,org.apache.hadoop.fs.StorageType)>
<org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void addOrUpdateReservationState(org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto,java.lang.String,java.lang.String,org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction,boolean)>
<org.apache.hadoop.hdfs.server.federation.resolver.MembershipNamenodeResolver: java.util.List getRecentRegistrationForQuery(org.apache.hadoop.hdfs.server.federation.store.protocol.GetNamenodeRegistrationsRequest,boolean,boolean)>
<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.hdfs.DFSOutputStream callAppend(java.lang.String,java.util.EnumSet,org.apache.hadoop.util.Progressable,java.lang.String[])>
<org.apache.hadoop.io.TestArrayFile: org.apache.hadoop.io.RandomDatum[] generate(int)>
<org.apache.hadoop.hdfs.server.federation.resolver.order.HashResolver: java.lang.String getFirstNamespace(java.lang.String,org.apache.hadoop.hdfs.server.federation.resolver.PathLocation)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler: void addApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,boolean,boolean)>
<org.apache.hadoop.yarn.security.NMTokenIdentifier: void write(java.io.DataOutput)>
<org.apache.hadoop.security.authentication.util.RolloverSignerSecretProvider: void rollSecret()>
<org.apache.hadoop.hdfs.server.datanode.BlockScanner: void markSuspectBlock(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
<org.apache.hadoop.hdfs.nfs.nfs3.WriteManager: int commitBeforeRead(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.FileHandle,long)>
<org.apache.hadoop.io.retry.RetryUtils: org.apache.hadoop.io.retry.RetryPolicy getDefaultRetryPolicy(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String,java.lang.String)>
<org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink: void init(org.apache.commons.configuration.SubsetConfiguration)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: void storeApplicationAttemptStateInternal(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationAttemptStateData)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController: boolean checkIfAlreadyBootstrapped(java.lang.String)>
<org.apache.hadoop.ipc.DecayRpcScheduler: void addResponseTime(java.lang.String,int,int,int)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.Allocation allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.util.List,java.util.List,java.util.List,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates)>
<org.apache.hadoop.nfs.NfsExports: void <init>(int,long,java.lang.String)>
<org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: void init()>
<org.apache.hadoop.yarn.security.ContainerTokenSelector: org.apache.hadoop.security.token.Token selectToken(org.apache.hadoop.io.Text,java.util.Collection)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo chooseRandom(int,java.lang.String,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)>
<org.apache.hadoop.hdfs.server.datanode.BlockReportTestBase: void blockReport_02()>
<org.apache.hadoop.mapred.LocalJobRunner$Job: java.util.concurrent.ExecutorService createReduceExecutor()>
<org.apache.hadoop.hdfs.server.datanode.BlockScanner$Servlet: void doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void checkRemoveParentZnode(java.lang.String,int)>
<org.apache.hadoop.ipc.Client$Connection: void <init>(org.apache.hadoop.ipc.Client,org.apache.hadoop.ipc.Client$ConnectionId,int)>
<org.apache.hadoop.security.LdapGroupsMapping: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.registry.server.services.RegistryAdminService: void verifyRealmValidity()>
<org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore: org.apache.hadoop.yarn.server.federation.store.records.GetApplicationHomeSubClusterResponse getApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.GetApplicationHomeSubClusterRequest)>
<org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl: org.apache.hadoop.hdfs.server.federation.store.records.QueryResult get(java.lang.Class)>
<org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer: long renew(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source: void dispatchBlocks(long)>
<org.apache.hadoop.io.TestArrayFile: void readTest(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.RandomDatum[],java.lang.String,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hdfs.DFSClient: boolean primitiveMkdir(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,boolean)>
<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FileSystem get(java.net.URI,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.yarn.webapp.Controller: void renderText(java.lang.String)>
<org.apache.hadoop.security.authentication.client.KerberosAuthenticator: void doSpnegoSequence(org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)>
<org.apache.hadoop.io.TestSequenceFile: void readTest(org.apache.hadoop.fs.FileSystem,int,int,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: boolean checkLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,long,long)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime: boolean useWhitelistEnv(java.util.Map)>
<org.apache.hadoop.service.AbstractService: void serviceInit(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.REMOVE3Response remove(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
<org.apache.hadoop.io.file.tfile.Compression$Algorithm: org.apache.hadoop.io.compress.Compressor getCompressor()>
<org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService: void storeToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier,java.lang.Long)>
<org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore: org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterInfoResponse getSubCluster(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterInfoRequest)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: org.apache.hadoop.yarn.api.records.Resource getUserAMResourceLimitPerPartition(java.lang.String,java.lang.String)>
<org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerTracker: java.lang.String getJson()>
<org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.FSINFO3Response fsinfo(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
<org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher: void publishLocalizationEvent(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEvent)>
<org.apache.hadoop.hdfs.server.datanode.VolumeScanner: void disableBlockPoolId(java.lang.String)>
<org.apache.hadoop.mapred.TestCombineTextInputFormat: void testFormat()>
<org.apache.hadoop.yarn.server.nodemanager.scheduler.DistributedScheduler: org.apache.hadoop.yarn.server.api.protocolrecords.DistributedSchedulingAllocateResponse allocateForDistributedScheduling(org.apache.hadoop.yarn.server.api.protocolrecords.DistributedSchedulingAllocateRequest)>
<org.apache.hadoop.metrics2.util.TestMetricsCache: void testGet()>
<org.apache.hadoop.mapred.ShuffleHandler$Shuffle$1: org.apache.hadoop.mapred.ShuffleHandler$AttemptPathInfo load(org.apache.hadoop.mapred.ShuffleHandler$AttemptPathIdentifier)>
<org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker: boolean checkVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi,org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$Callback)>
<org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager: void storeNewMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)>
<org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor: org.apache.hadoop.yarn.server.api.protocolrecords.DistributedSchedulingAllocateResponse allocateForDistributedScheduling(org.apache.hadoop.yarn.server.api.protocolrecords.DistributedSchedulingAllocateRequest)>
<org.apache.hadoop.yarn.server.nodemanager.DockerContainerExecutor: int launchContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.FileDeletionTask: void run()>
<org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: boolean signalContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerSignalContext)>
<org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService: void execute(java.lang.Runnable)>
<org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer: void checkOperation(org.apache.hadoop.hdfs.server.namenode.NameNode$OperationCategory)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue: void updateDemand()>
<org.apache.hadoop.security.UserGroupInformation: void logUserInfo(org.slf4j.Logger,java.lang.String,org.apache.hadoop.security.UserGroupInformation)>
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: int receivePacket()>
<org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs: long scanForLogs()>
<org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd: org.apache.hadoop.oncrpc.XDR nullOp(org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress)>
<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: int incrementDelegationTokenSeqNum()>
<org.apache.hadoop.hdfs.DFSInputStream: java.nio.ByteBuffer tryReadZeroCopy(int,java.util.EnumSet)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: boolean reservationExceedsThreshold(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType)>
<org.apache.hadoop.ha.ActiveStandbyElector: void terminateConnection()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: void reserveResource(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)>
<org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE enterState(org.apache.hadoop.service.Service$STATE)>
<org.apache.hadoop.hdfs.DFSUtil: java.lang.String[] getSuffixIDs(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.DFSUtil$AddressMatcher)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl: void deleteCGroup(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController,java.lang.String)>
<org.apache.hadoop.hdfs.server.datanode.BlockReportTestBase: java.util.ArrayList locatedToBlocks(java.util.List,java.util.List)>
<org.apache.hadoop.hdfs.client.impl.BlockReaderFactory: org.apache.hadoop.hdfs.BlockReader tryToCreateExternalBlockReader()>
<org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo: void delete()>
<org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider: java.io.OutputStream getOutputStreamForKeystore()>
<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void handleEvent(org.apache.hadoop.mapreduce.jobhistory.JobHistoryEvent)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt: void showRequests()>
<org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: byte[] createPassword(org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier)>
<org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider: void flush()>
<org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testNotMatchedReplicaID()>
<org.apache.hadoop.hdfs.server.datanode.metrics.OutlierDetector: java.util.Map getOutliers(java.util.Map)>
<org.apache.hadoop.hdfs.server.balancer.KeyManager: org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey newDataEncryptionKey()>
<org.apache.hadoop.ipc.DecayRpcScheduler: boolean shouldBackOff(org.apache.hadoop.ipc.Schedulable)>
<org.apache.hadoop.mapred.IndexCache: org.apache.hadoop.mapred.IndexCache$IndexInformation readIndexFileToCache(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)>
<org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx: void trimWrite(int)>
<org.apache.hadoop.net.unix.DomainSocketWatcher$2: void run()>
<org.apache.hadoop.io.compress.CodecPool: org.apache.hadoop.io.compress.Decompressor getDecompressor(org.apache.hadoop.io.compress.CompressionCodec)>
<org.apache.hadoop.crypto.key.kms.KMSClientProvider$3: java.lang.Void run()>
<org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService$ApplicationEventHandler: void handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent)>
<org.apache.hadoop.yarn.server.federation.store.utils.FederationStateStoreUtils: void setProperty(com.zaxxer.hikari.HikariDataSource,java.lang.String,java.lang.String)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: org.apache.hadoop.yarn.api.records.Resource assignContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode,boolean)>
<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceStop()>
<org.apache.hadoop.mapred.TestTextInputFormat: void testSplitableCodecs2()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: void updateMetricsForAllocatedContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.api.records.Container)>
<org.apache.hadoop.mapreduce.task.reduce.EventFetcher: void run()>
<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.ContainerAllocation preCheckForPlacementSet(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.PlacementSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey)>
<org.apache.hadoop.hdfs.HAUtilClient: void cloneDelegationTokenForLogicalUri(org.apache.hadoop.security.UserGroupInformation,java.net.URI,java.util.Collection)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator: boolean shouldAllocOrReserveNewContainer(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.Resource)>
<org.apache.hadoop.fs.FsShell: int run(java.lang.String[])>
<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void snapshotMetrics(org.apache.hadoop.metrics2.impl.MetricsSourceAdapter,org.apache.hadoop.metrics2.impl.MetricsBufferBuilder)>
<org.apache.hadoop.hdfs.server.common.Storage: void nativeCopyFileUnbuffered(java.io.File,java.io.File,boolean)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: void releaseResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)>
<org.apache.hadoop.mapreduce.lib.input.TestCombineTextInputFormat: void testFormat()>
<org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: boolean isStale()>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx$Dumper: void dump()>
<org.apache.hadoop.security.SaslRpcClient$WrappedOutputStream: void write(byte[],int,int)>
<org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter: void outputINodes(java.io.InputStream)>
<org.apache.hadoop.ipc.Server$ConnectionManager$1: void run()>
<org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler: boolean validateAudiences(com.nimbusds.jwt.SignedJWT)>
<org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp: org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp$RenameResult renameTo(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INodesInPath,boolean)>
<org.apache.hadoop.hdfs.server.datanode.DataXceiver: void replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.fs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.AllocationBasedResourceUtilizationTracker: boolean hasResourcesAvailable(long,long,int)>
<org.apache.hadoop.yarn.server.federation.store.utils.FederationStateStoreUtils: void setUsername(com.zaxxer.hikari.HikariDataSource,java.lang.String)>
<org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices$2: void write(java.io.OutputStream)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: void recover()>
<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)>
<org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration: void setVcoresPerNode(java.lang.String,int)>
<org.apache.hadoop.fs.RawLocalFileSystem: boolean rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.service.launcher.ServiceLauncher: void noteException(org.apache.hadoop.util.ExitUtil$ExitException)>
<org.apache.hadoop.registry.server.services.RegistryAdminService$AsyncPurge: java.lang.Integer call()>
<org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.NamespaceInfo retrieveNamespaceInfo()>
<org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void unregisterSlot(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId)>
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void closeFile(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile)>
<org.apache.hadoop.ipc.Server$Connection: void processRpcOutOfBandRequest(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,org.apache.hadoop.ipc.RpcWritable$Buffer)>
<org.apache.hadoop.mapred.FileInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>
<org.apache.hadoop.crypto.OpensslCipher: void <clinit>()>
<org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp: org.apache.hadoop.hdfs.protocol.HdfsFileStatus startFile(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.fs.permission.PermissionStatus,java.lang.String,java.lang.String,java.util.EnumSet,boolean,short,long,org.apache.hadoop.fs.FileEncryptionInfo,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,boolean)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl: org.apache.hadoop.hdfs.protocol.ExtendedBlock nextBlock()>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: boolean validateReplicationWork(org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork)>
<org.apache.hadoop.security.SecurityUtil: void setTokenServiceUseIp(boolean)>
<org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: org.apache.hadoop.mapreduce.task.reduce.MapHost getHost()>
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.protocol.LocatedBlock getAdditionalDatanode(java.lang.String,long,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],java.lang.String[],org.apache.hadoop.hdfs.protocol.DatanodeInfo[],int,java.lang.String)>
<org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void selectInputStreams(java.util.Collection,long,boolean)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner: org.apache.hadoop.security.Credentials getSystemCredentialsSentFromRM(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizerContext)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void removeRMDelegationTokenState(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue: void setFairShare(org.apache.hadoop.yarn.api.records.Resource)>
<org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo: void shutDownTimer()>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode: void adjustBlockTotals(int,int)>
<org.apache.hadoop.security.ssl.ReloadingX509TrustManager: javax.net.ssl.X509TrustManager loadTrustManager()>
<org.apache.hadoop.hdfs.server.federation.resolver.order.RouterResolver: void updateSubclusterMapping()>
<org.apache.hadoop.hdfs.server.federation.router.RouterHeartbeatService: void updateStateStore()>
<org.apache.hadoop.metrics2.util.MBeans: javax.management.ObjectName register(java.lang.String,java.lang.String,java.lang.Object)>
<org.apache.hadoop.yarn.server.resourcemanager.webapp.NodesPage$NodesBlock: void render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block)>
<org.apache.hadoop.registry.client.impl.zk.CuratorService: void zkUpdate(java.lang.String,byte[])>
<org.apache.hadoop.io.compress.bzip2.Bzip2Compressor: void reinit(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hdfs.web.HftpFileSystem: java.net.URL getNamenodeURL(java.lang.String,java.lang.String)>
<org.apache.hadoop.hdfs.DFSClient: void clearDataEncryptionKey()>
<org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService: int loadTokenMasterKeys(org.apache.hadoop.mapreduce.v2.hs.HistoryServerStateStoreService$HistoryServerState)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue: boolean fitsInMaxShare(org.apache.hadoop.yarn.api.records.Resource)>
<org.apache.hadoop.io.SequenceFile$Reader: void getCurrentValue(org.apache.hadoop.io.Writable)>
<org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: boolean add(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int,int,int)>
<org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl: java.io.BufferedReader getReader(java.lang.String)>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void removeApp(java.lang.String,boolean,java.util.Set)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: void completedContainerInternal(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType)>
<org.apache.hadoop.mapred.BackupStore$BackupRamManager: int reserve(int)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void removeBRLeaseIfNeeded(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.BlockReportContext)>
<org.apache.hadoop.hdfs.server.namenode.NNThroughputBenchmark$TinyDatanode: void sendHeartbeat()>
<org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM: byte[] retrievePassword(org.apache.hadoop.yarn.security.NMTokenIdentifier)>
<org.apache.hadoop.mapred.pipes.BinaryProtocol: void endOfInput()>
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: java.nio.MappedByteBuffer loadMmapInternal()>
<org.apache.hadoop.yarn.server.timeline.EntityCacheItem: void forceRelease()>
<org.apache.hadoop.io.SequenceFile$Sorter: int mergePass(org.apache.hadoop.fs.Path)>
<org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: void update(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int,int,int,int,int)>
<org.apache.hadoop.mapred.MapTask$MapOutputBuffer: void mergeParts()>
<org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader load(java.lang.String)>
<org.apache.hadoop.hdfs.DataStreamer$LastExceptionInStreamer: void check(boolean)>
<org.apache.hadoop.hdfs.net.DFSTopologyNodeImpl: void childRemoveStorage(java.lang.String,org.apache.hadoop.fs.StorageType)>
<org.apache.hadoop.hdfs.DFSOutputStream: void <init>(org.apache.hadoop.hdfs.DFSClient,java.lang.String,java.util.EnumSet,org.apache.hadoop.util.Progressable,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.util.DataChecksum)>
<org.apache.hadoop.hdfs.TestClientReportBadBlock: void dfsClientReadFile(org.apache.hadoop.fs.Path)>
<org.apache.hadoop.ha.SshFenceByTcpPort$LogAdapter: void log(int,java.lang.String)>
<org.apache.hadoop.mapreduce.task.reduce.Fetcher: java.net.URL getMapOutputURL(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.util.Collection)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerAppUtils: boolean isPlaceBlacklisted(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.commons.logging.Log)>
<org.apache.hadoop.crypto.key.kms.KMSClientProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData: org.apache.hadoop.yarn.api.ContainerManagementProtocol newProxy(org.apache.hadoop.yarn.ipc.YarnRPC,java.lang.String,org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.Token)>
<org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker: com.google.protobuf.Message getReturnMessage(java.lang.reflect.Method,org.apache.hadoop.ipc.RpcWritable$Buffer)>
<org.apache.hadoop.ipc.Server$Responder: boolean processResponse(java.util.LinkedList,boolean)>
<org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService: void updateState()>
<org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager: void removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)>
<org.apache.hadoop.security.token.delegation.web.DelegationTokenManager: org.apache.hadoop.security.token.Token createToken(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.String)>
<org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher: void putEntity(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.api.records.ApplicationId)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: void updateActiveUsersResourceUsage(java.lang.String)>
<org.apache.hadoop.hdfs.server.balancer.NameNodeConnector: boolean shouldContinue(long)>
<org.apache.hadoop.yarn.client.api.impl.TimelineWriter: com.sun.jersey.api.client.ClientResponse doPosting(java.lang.Object,java.lang.String)>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: void signalContainer(org.apache.hadoop.yarn.api.records.SignalContainerCommand)>
<org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.NFS3Response nullProcedure()>
<org.apache.hadoop.nfs.NfsExports$RegexMatch: boolean isIncluded(java.lang.String,java.lang.String)>
<org.apache.hadoop.mapred.TestTextInputFormat: void testFormat()>
<org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp: org.apache.hadoop.hdfs.protocol.LastBlockWithStatus appendFile(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.lang.String,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,java.lang.String,boolean,boolean)>
<org.apache.hadoop.registry.client.impl.zk.CuratorService: boolean zkMkPath(java.lang.String,org.apache.zookeeper.CreateMode,boolean,java.util.List)>
<org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolTranslatorPB: org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolPB createClientDatanodeProtocolProxy(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.conf.Configuration,int,boolean,org.apache.hadoop.hdfs.protocol.LocatedBlock)>
<org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ReservedContainerCandidatesSelector: org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ReservedContainerCandidatesSelector$NodeForPreemption getPreemptionCandidatesOnNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.api.records.Resource,java.util.Map,java.util.Map,org.apache.hadoop.yarn.api.records.Resource,boolean)>
<org.apache.hadoop.security.authentication.server.AuthenticationFilter: void doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)>
<org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator: void authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)>
<org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager: void storeNewToken(org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier,long)>
<org.apache.hadoop.security.LdapGroupsMapping: java.util.List getGroups(java.lang.String)>
<org.apache.hadoop.ipc.Client$Connection: void setupIOstreams(java.util.concurrent.atomic.AtomicBoolean)>
<org.apache.hadoop.hdfs.server.federation.resolver.order.LocalResolver: java.lang.String chooseFirstNamespace(java.lang.String,org.apache.hadoop.hdfs.server.federation.resolver.PathLocation)>
<org.apache.hadoop.hdfs.server.datanode.DataNode: void checkReadAccess(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.io.File[] copyBlockFiles(java.io.File,java.io.File,java.io.File,java.io.File,boolean,int,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.yarn.server.nodemanager.DeletionService: void delete(org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.DeletionTask)>
<org.apache.hadoop.security.UserGroupInformation: void loginUserFromSubject(javax.security.auth.Subject)>
<org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: void addProxyToCache(java.lang.String,org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData)>
<org.apache.hadoop.service.launcher.ServiceLauncher: org.apache.hadoop.service.Service instantiateService(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.ipc.Client$Connection: void handleConnectionFailure(int,java.io.IOException)>
<org.apache.hadoop.hdfs.server.datanode.DataNode$4: void call(java.util.Set,java.util.Set)>
<org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService: int loadTokens(org.apache.hadoop.mapreduce.v2.hs.HistoryServerStateStoreService$HistoryServerState)>
<org.apache.hadoop.registry.client.impl.zk.CuratorService: java.lang.String dumpRegistryRobustly(boolean)>
<org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$AttemptDirCache: org.apache.hadoop.fs.Path createApplicationDir(org.apache.hadoop.yarn.api.records.ApplicationId)>
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.protocol.HdfsFileStatus startFileInt(java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,java.lang.String,java.lang.String,java.util.EnumSet,boolean,short,long,org.apache.hadoop.crypto.CryptoProtocolVersion[],boolean)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[] chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.Set,long,java.util.List,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet)>
<org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: void proxyLink(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.net.URI,javax.servlet.http.Cookie,java.lang.String,org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet$HTTP)>
<org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService: void fenceOldActive(byte[])>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue: boolean assignContainerPreCheck(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler: void nodeUpdate(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode)>
<org.apache.hadoop.mapred.BackupStore$FileCache: void createInDiskSegment()>
<org.apache.hadoop.hdfs.server.datanode.web.webhdfs.ExceptionHandler: io.netty.handler.codec.http.DefaultFullHttpResponse exceptionCaught(java.lang.Throwable)>
<org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter: void doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)>
<org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter: void output(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary,java.io.FileInputStream,java.util.ArrayList)>
<org.apache.hadoop.yarn.nodelabels.NonAppendableFSNodeLabelStore: void recover()>
<org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods: java.net.URI redirectURI(javax.ws.rs.core.Response$ResponseBuilder,org.apache.hadoop.hdfs.server.namenode.NameNode,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,long,long,java.lang.String,org.apache.hadoop.hdfs.web.resources.Param[])>
<org.apache.hadoop.yarn.util.YarnVersionInfo: void main(java.lang.String[])>
<org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void mergePaths(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: java.util.List getResources()>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: void processOverWrite(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.request.WRITE3Request,org.jboss.netty.channel.Channel,int,org.apache.hadoop.security.IdMappingServiceProvider)>
<org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp: void persistNewBlock(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile)>
<org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo: void moveToDone()>
<org.apache.hadoop.hdfs.TestPipelines: void pipeline_01()>
<org.apache.hadoop.hdfs.server.namenode.NNStorage: void reportErrorsOnDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl: void printReferenceTraceInfo(java.lang.String)>
<org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand cacheReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,java.util.List)>
<org.apache.hadoop.hdfs.client.impl.BlockReaderLocalLegacy: int read(byte[],int,int)>
<org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.lang.String getCorruptFiles()>
<org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp: boolean deleteAllowed(org.apache.hadoop.hdfs.server.namenode.INodesInPath)>
<org.apache.hadoop.util.LogAdapter: void debug(java.lang.Throwable)>
<org.apache.hadoop.mapreduce.JobSubmitter: org.apache.hadoop.mapreduce.JobStatus submitJobInternal(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.mapreduce.Cluster)>
<org.apache.hadoop.ha.ActiveStandbyElector: void joinElection(byte[])>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void removeBlocksAndUpdateSafemodeTotal(org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo)>
<org.apache.hadoop.fs.RawLocalFileSystem: boolean mkOneDirWithMode(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.permission.FsPermission)>
<org.apache.hadoop.io.compress.CodecPool: org.apache.hadoop.io.compress.Compressor getCompressor(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: void dumpOpCounts(java.util.EnumMap)>
<org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map queryDatanodesForHdfsBlocksMetadata(org.apache.hadoop.conf.Configuration,java.util.Map,int,int,boolean,org.apache.htrace.core.Tracer,org.apache.htrace.core.SpanId)>
<org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.SYMLINK3Response symlink(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
<org.apache.hadoop.security.CompositeGroupsMapping: java.util.List getGroups(java.lang.String)>
<org.apache.hadoop.hdfs.client.impl.BlockReaderRemote2: void readTrailingEmptyPacket()>
<org.apache.hadoop.yarn.server.nodemanager.api.impl.pb.NMProtoUtils: org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.DeletionTask convertProtoToDeletionTask(org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto,org.apache.hadoop.yarn.server.nodemanager.DeletionService)>
<org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder: org.apache.hadoop.io.FastByteComparisons$Comparer getBestComparer()>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: void receivedNewWriteInternal(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.request.WRITE3Request,org.jboss.netty.channel.Channel,int,org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService,org.apache.hadoop.security.IdMappingServiceProvider)>
<org.apache.hadoop.metrics2.impl.MetricsSystemImpl: void registerSource(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource)>
<org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void downloadEditsToStorage(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog,org.apache.hadoop.hdfs.server.namenode.NNStorage)>
<org.apache.hadoop.hdfs.qjournal.server.Journal: void journal(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,long,int,byte[])>
<org.apache.hadoop.hdfs.server.datanode.BlockReportTestBase: void printStats()>
<org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver: void doRead(java.nio.channels.ReadableByteChannel,java.io.InputStream)>
<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void run()>
<org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: void updateMetricsForRejoinedNode(org.apache.hadoop.yarn.api.records.NodeState)>
<org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker: org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)>
<org.apache.hadoop.security.LdapGroupsMapping: void goUpGroupHierarchy(java.util.Set,int,java.util.Set)>
<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void handleVolumeFailures(java.util.Set)>
<org.apache.hadoop.yarn.server.nodemanager.NodeManager: org.apache.hadoop.yarn.server.nodemanager.nodelabels.NodeLabelsProvider createNodeLabelsProvider(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.fs.FileUtil: boolean compareFs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileSystem)>
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.fs.StorageType,java.io.DataInputStream,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,long,long,long,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean,boolean)>
<org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.ACCESS3Response access(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
<org.apache.hadoop.mapred.YarnChild: void configureTask(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Task,org.apache.hadoop.security.Credentials,org.apache.hadoop.security.token.Token)>
<org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx$COMMIT_STATUS checkCommitInternal(long,org.jboss.netty.channel.Channel,int,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes,boolean)>
<org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: void decResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionTypeRequest,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.client.api.AMRMClient$ContainerRequest)>
<org.apache.hadoop.mapred.SortedRanges: void add(long,long)>
<org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks: boolean decrement(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
<org.apache.hadoop.metrics2.lib.MutableMetricsFactory: org.apache.hadoop.metrics2.lib.MutableMetric newForField(java.lang.reflect.Field,org.apache.hadoop.metrics2.'annotation'.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry)>
<org.apache.hadoop.ipc.Server$Responder: void doRunLoop()>
<org.apache.hadoop.fs.FileUtil: int symLink(java.lang.String,java.lang.String)>
<org.apache.hadoop.hdfs.server.namenode.BackupImage: void journal(long,int,byte[])>
<org.apache.hadoop.hdfs.web.WebHdfsFileSystem: org.apache.hadoop.security.token.Token getDelegationToken()>
<org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter: void flush()>
<org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$JobListCache: org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo addIfAbsent(org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo)>
<org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: void rescanCacheDirectives()>
<org.apache.hadoop.io.SequenceFile$Reader: java.lang.Object getCurrentValue(java.lang.Object)>
<org.apache.hadoop.security.SaslRpcClient: org.apache.hadoop.security.token.Token getServerToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)>
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: boolean addNoChecksumAnchor()>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: org.apache.hadoop.yarn.api.records.Resource assignContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode)>
<org.apache.hadoop.io.IOUtils: void closeSocket(java.net.Socket)>
<org.apache.hadoop.yarn.server.resourcemanager.NodesListManager: void printConfiguredHosts()>
<org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: int demoteOldEvictableMmaped(long)>
<org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processOverReplicatedBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,short,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
<org.apache.hadoop.yarn.security.client.TimelineDelegationTokenSelector: org.apache.hadoop.security.token.Token selectToken(org.apache.hadoop.io.Text,java.util.Collection)>
<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt: org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer allocate(org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.Container)>
<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>
<org.apache.hadoop.hdfs.TestEncryptionZonesWithKMS: void testDelegationToken()>
<org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploader: void deleteTempFile(org.apache.hadoop.fs.Path)>
<org.apache.hadoop.conf.Configuration: org.apache.hadoop.conf.Configuration$Resource loadResource(java.util.Properties,org.apache.hadoop.conf.Configuration$Resource,boolean)>
<org.apache.hadoop.yarn.client.api.async.AMRMClientAsync: void waitFor(com.google.common.base.Supplier,int,int)>
<org.apache.hadoop.hdfs.server.datanode.DataNode: java.io.FileInputStream[] requestShortCircuitFdsForRead(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,int)>
<org.apache.hadoop.registry.server.services.MicroZookeeperService: void serviceStart()>
<org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.QuotaUsage getQuotaUsage(java.lang.String)>
<org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testNoReplicaUnderRecovery()>
<org.apache.hadoop.hdfs.server.datanode.VolumeScanner: boolean calculateShouldScan(java.lang.String,long,long,long,long)>
<org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$KillNewTransition: void transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent)>
<org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor: void beforeExecute(java.lang.Thread,java.lang.Runnable)>
<org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.GETATTR3Response getattr(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
<org.apache.hadoop.hdfs.server.federation.router.ConnectionManager$ConnectionCreator: void run()>
<org.apache.hadoop.mapred.Queue: org.apache.hadoop.mapred.JobQueueInfo getJobQueueInfo()>
<org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: void removeRMDTMasterKeyState(org.apache.hadoop.security.token.delegation.DelegationKey)>
<org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager$TopWindow snapshot(long)>