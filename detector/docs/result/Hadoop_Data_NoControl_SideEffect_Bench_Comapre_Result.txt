2020-08-25 12:40:48 [INFO] - start to calc Recall:
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void removeRMDTMasterKeyState(org.apache.hadoop.security.token.delegation.DelegationKey)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.DataNode: void handleVolumeFailures(java.util.Set)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment allocateContainerOnSingleNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.PlacementSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,boolean)>
Start to analyze method: <org.apache.hadoop.metrics2.impl.MetricsConfig: java.lang.Object getProperty(java.lang.String)>
Start to analyze method: <org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: void consume(org.apache.hadoop.metrics2.impl.MetricsBuffer)>
Unit: tmp$25967488 = new java.lang.StringBuilder AT LINE 184 is not found in our analysis.
Unit: specialinvoke tmp$25967488.<java.lang.StringBuilder: void <init>()>() AT LINE 184 is not found in our analysis.
Unit: $stack42 = virtualinvoke tmp$25967488.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("Pushing record ") AT LINE 184 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: void receivedNewWrite(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.request.WRITE3Request,org.jboss.netty.channel.Channel,int,org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService,org.apache.hadoop.security.IdMappingServiceProvider)>
Unit: goto [?= return] AT LINE 466 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry$NewShmInfo createNewMemorySegment(java.lang.String,org.apache.hadoop.net.unix.DomainSocket)>
Start to analyze method: <org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo: void flush()>
Start to analyze method: <org.apache.hadoop.security.SaslRpcServer: void <init>(org.apache.hadoop.security.SaslRpcServer$AuthMethod)>
Start to analyze method: <org.apache.hadoop.hdfs.tools.DFSAdmin: int run(java.lang.String[])>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM: void appFinished(org.apache.hadoop.yarn.api.records.ApplicationId)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: void activateApplications()>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler: void createCgroup(java.lang.String,java.lang.String)>
Start to analyze method: <org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback: void <init>()>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache: void uncacheBlock(java.lang.String,long)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager: void sendIBRs(org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol,org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache: boolean put(org.apache.hadoop.nfs.nfs3.FileHandle,org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testFailedReplicaUpdate()>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: void markContainerForPreemption(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)>
Start to analyze method: <org.apache.hadoop.ipc.Server$Handler: void run()>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: void printChildQueues()>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx$COMMIT_STATUS checkCommit(org.apache.hadoop.hdfs.DFSClient,long,org.jboss.netty.channel.Channel,int,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes,boolean)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.io.File moveBlockFiles(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi,org.apache.hadoop.hdfs.protocol.Block,java.io.File,java.io.File)>
Start to analyze method: <org.apache.hadoop.registry.client.impl.zk.CuratorService: void zkDelete(java.lang.String,boolean,org.apache.curator.framework.api.BackgroundCallback)>
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: void populateNMTokens(java.util.List)>
Unit: $stack15 = virtualinvoke this.<org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: org.apache.hadoop.yarn.client.api.NMTokenCache getNMTokenCache()>() AT LINE 463 is not found in our analysis.
Unit: $stack16 = virtualinvoke $stack15.<org.apache.hadoop.yarn.client.api.NMTokenCache: boolean containsToken(java.lang.String)>(nodeId) AT LINE 463 is not found in our analysis.
Unit: if $stack16 == 0 goto $stack18 = <org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: org.apache.commons.logging.Log LOG> AT LINE 463 is not found in our analysis.
Unit: goto [?= $stack12 = virtualinvoke this.<org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: org.apache.hadoop.yarn.client.api.NMTokenCache getNMTokenCache()>()] AT LINE 464 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.conf.Configuration: void reloadExistingConfigurations()>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: void <init>(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerContext,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void bumpReplicaGS(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,long)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore: void loadRMDTSecretManagerState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState)>
Start to analyze method: <org.apache.hadoop.yarn.util.ProcfsBasedProcessTree: long getSmapBasedRssMemorySize(int)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler: void recover()>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testRBWReplicas()>
Start to analyze method: <org.apache.hadoop.fs.FileSystem: void loadFileSystems()>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void assign(java.util.List)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: void allocateContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,boolean)>
Start to analyze method: <org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void purge(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica)>
Unit: if removedFromInfoMap == 0 goto (branch) AT LINE 651 is not found in our analysis.
Unit: if evictionMapName == null goto $stack20 = <org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: org.slf4j.Logger LOG> AT LINE 654 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)>
Unit: goto [?= message#26 = 0] AT LINE 991 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.NameNode: void copyEditLogSegmentsToSharedDir(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.util.Collection,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.conf.Configuration)>
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: void removePendingChangeRequests(java.util.List)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt: boolean commonReserve(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.Resource)>
Start to analyze method: <org.apache.hadoop.mapred.ShuffleHandler$Shuffle: void setResponseHeaders(org.jboss.netty.handler.codec.http.HttpResponse,boolean,long)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void storeRMDelegationTokenState(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.security.AppPriorityACLsManager: void addPrioirityACLs(java.util.List,java.lang.String)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void waitVolumeRemoved(int,java.util.concurrent.locks.Condition)>
Start to analyze method: <org.apache.hadoop.security.UserGroupInformation$1: void run()>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: java.util.List loadCompletedResources(org.apache.hadoop.yarn.server.utils.LeveldbIterator,java.lang.String)>
Start to analyze method: <org.apache.hadoop.crypto.key.kms.KMSClientProvider: java.lang.Object call(java.net.HttpURLConnection,java.util.Map,int,java.lang.Class,int)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType getAllowedLocalityLevel(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,int,double,double)>
Start to analyze method: <org.apache.hadoop.ha.ActiveStandbyElector: void monitorActiveStatus()>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindow: long getSum(long)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd: org.apache.hadoop.oncrpc.XDR dump(org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress)>
Start to analyze method: <org.apache.hadoop.util.NativeCodeLoader: void <clinit>()>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void storeContainerQueued(org.apache.hadoop.yarn.api.records.ContainerId)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: void checkPermission(org.apache.hadoop.hdfs.server.namenode.INodesInPath,boolean,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,boolean)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testFinalizedReplicas()>
Start to analyze method: <org.apache.hadoop.ha.ActiveStandbyElector: boolean reEstablishSession()>
Start to analyze method: <org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.BlockStorageLocation[] getBlockStorageLocations(java.util.List)>
Unit: $stack39 = staticinvoke <com.google.common.base.Joiner: com.google.common.base.Joiner on(java.lang.String)>("\n") AT LINE 960 is not found in our analysis.
Unit: $stack40 = virtualinvoke $stack39.<com.google.common.base.Joiner: com.google.common.base.Joiner$MapJoiner withKeyValueSeparator(java.lang.String)>("=") AT LINE 960 is not found in our analysis.
Unit: $stack41 = virtualinvoke $stack40.<com.google.common.base.Joiner$MapJoiner: java.lang.String join(java.util.Map)>(hdfsLoc#11) AT LINE 960 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.ipc.Client$Connection$3: void run()>
Start to analyze method: <org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor: void tryStart()>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testRecoveryInProgressException()>
Start to analyze method: <org.apache.hadoop.io.file.tfile.Compression$Algorithm: org.apache.hadoop.io.compress.Decompressor getDecompressor()>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.CREATE3Response create(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: java.lang.String[] getGroupsForUser(java.lang.String)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp: boolean completeFile(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy: org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi doChooseVolume(java.util.List,long)>
Unit: goto [?= return volume] AT LINE 170 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.balancer.Dispatcher: long dispatchBlockMoves()>
Start to analyze method: <org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31: void emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: org.apache.hadoop.hdfs.server.datanode.ReplicaInfo selectReplicaToDelete(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,org.apache.hadoop.hdfs.server.datanode.ReplicaInfo)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync: void logSync()>
Start to analyze method: <org.apache.hadoop.io.ReadaheadPool: org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest submitReadahead(java.lang.String,java.io.FileDescriptor,long,long)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService: void storeTokenMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)>
Start to analyze method: <org.apache.hadoop.security.SaslRpcClient$WrappedInputStream: void readNextRpcPacket()>
Start to analyze method: <org.apache.hadoop.util.ApplicationClassLoader: java.lang.Class loadClass(java.lang.String,boolean)>
Unit: if c == null goto (branch) AT LINE 188 is not found in our analysis.
Unit: if c == null goto (branch) AT LINE 195 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService: void submitLazyPersistTask(java.lang.String,long,long,long,java.io.File,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference)>
Start to analyze method: <org.apache.hadoop.registry.server.services.RegistryAdminService: int purge(java.lang.String,org.apache.hadoop.registry.server.services.RegistryAdminService$NodeSelector,org.apache.hadoop.registry.server.services.RegistryAdminService$PurgePolicy,org.apache.curator.framework.api.BackgroundCallback)>
Start to analyze method: <org.apache.hadoop.ipc.WritableRpcEngine$Server$WritableRpcInvoker: org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)>
Unit: if exception == null goto $stack73 = <org.apache.hadoop.ipc.Server: org.slf4j.Logger LOG> AT LINE 547 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.FifoCandidatesSelector: java.util.Map selectCandidates(java.util.Map,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: boolean delete(java.lang.String,boolean)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskContiguous: void recover()>
Unit: if info != null goto $stack68 = <org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker: org.slf4j.Logger LOG> AT LINE 137 is not found in our analysis.
Unit: goto [?= (branch)] AT LINE 138 is not found in our analysis.
Unit: goto [?= (branch)] AT LINE 131 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.PlacementSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)>
Start to analyze method: <org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void trimEvictionMaps()>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor$ContainerRequest assignWithoutLocality(org.apache.hadoop.yarn.api.records.Container)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition: org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState transition(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent)>
Unit: $stack43 = interfaceinvoke msg#18.<java.util.Map$Entry: java.lang.Object getValue()>() AT LINE 1096 is not found in our analysis.
Unit: $stack44 = (java.lang.Long) $stack43 AT LINE 1096 is not found in our analysis.
Unit: $stack45 = virtualinvoke $stack44.<java.lang.Long: long longValue()>() AT LINE 1096 is not found in our analysis.
Unit: $stack46 = staticinvoke <org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: org.apache.hadoop.yarn.util.Clock access$1200(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl)>(app) AT LINE 1096 is not found in our analysis.
Unit: $stack47 = interfaceinvoke $stack46.<org.apache.hadoop.yarn.util.Clock: long getTime()>() AT LINE 1096 is not found in our analysis.
Unit: remainingTime = $stack45 - $stack47 AT LINE 1096 is not found in our analysis.
Unit: $stack57 = remainingTime cmp 0L AT LINE 1097 is not found in our analysis.
Unit: if $stack57 <= 0 goto $stack61 = 0L AT LINE 1097 is not found in our analysis.
Unit: goto [?= $stack58 = virtualinvoke $stack56.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>($stack61)] AT LINE 1102 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source: long getBlockList()>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void storeApplication(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl: void save()>
Start to analyze method: <org.apache.hadoop.net.unix.DomainSocketWatcher$NotificationHandler: boolean handle(org.apache.hadoop.net.unix.DomainSocket)>
Start to analyze method: <org.apache.hadoop.fs.contract.ContractTestUtils: void noteAction(java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: void handleNMContainerStatus(org.apache.hadoop.yarn.server.api.protocolrecords.NMContainerStatus,org.apache.hadoop.yarn.api.records.NodeId)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: void setAppCollectorsMapToResponse(java.util.List,org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor: org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: void storeOrUpdateRMDT(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long,boolean)>
Start to analyze method: <org.apache.hadoop.security.SecurityUtil: java.net.InetAddress getByName(java.lang.String)>
Unit: $stack6 = new org.apache.hadoop.util.StopWatch AT LINE 532 is not found in our analysis.
Unit: specialinvoke $stack6.<org.apache.hadoop.util.StopWatch: void <init>()>() AT LINE 532 is not found in our analysis.
Unit: lookupTimer = virtualinvoke $stack6.<org.apache.hadoop.util.StopWatch: org.apache.hadoop.util.StopWatch start()>() AT LINE 532 is not found in our analysis.
Unit: $stack8 = <org.apache.hadoop.security.SecurityUtil: org.apache.hadoop.security.SecurityUtil$HostResolver hostResolver> AT LINE 533 is not found in our analysis.
Unit: result = interfaceinvoke $stack8.<org.apache.hadoop.security.SecurityUtil$HostResolver: java.net.InetAddress getByName(java.lang.String)>(hostname) AT LINE 533 is not found in our analysis.
Unit: $stack10 = virtualinvoke lookupTimer.<org.apache.hadoop.util.StopWatch: org.apache.hadoop.util.StopWatch stop()>() AT LINE 534 is not found in our analysis.
Unit: $stack11 = <java.util.concurrent.TimeUnit: java.util.concurrent.TimeUnit MILLISECONDS> AT LINE 534 is not found in our analysis.
Unit: elapsedMs = virtualinvoke $stack10.<org.apache.hadoop.util.StopWatch: long now(java.util.concurrent.TimeUnit)>($stack11) AT LINE 534 is not found in our analysis.
Unit: $stack13 = <org.apache.hadoop.security.SecurityUtil: int slowLookupThresholdMs> AT LINE 536 is not found in our analysis.
Unit: $stack14 = (long) $stack13 AT LINE 536 is not found in our analysis.
Unit: $stack15 = elapsedMs cmp $stack14 AT LINE 536 is not found in our analysis.
Unit: if $stack15 < 0 goto $stack16 = <org.apache.hadoop.security.SecurityUtil: org.slf4j.Logger LOG> AT LINE 536 is not found in our analysis.
Unit: $stack28 = <org.apache.hadoop.security.SecurityUtil: org.slf4j.Logger LOG> AT LINE 537 is not found in our analysis.
Unit: $stack27 = new java.lang.StringBuilder AT LINE 537 is not found in our analysis.
Unit: specialinvoke $stack27.<java.lang.StringBuilder: void <init>()>() AT LINE 537 is not found in our analysis.
Unit: $stack29 = virtualinvoke $stack27.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("Slow name lookup for ") AT LINE 537 is not found in our analysis.
Unit: $stack30 = virtualinvoke $stack29.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(hostname) AT LINE 537 is not found in our analysis.
Unit: $stack31 = virtualinvoke $stack30.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(". Took ") AT LINE 537 is not found in our analysis.
Unit: $stack32 = virtualinvoke $stack31.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(elapsedMs) AT LINE 537 is not found in our analysis.
Unit: $stack33 = virtualinvoke $stack32.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" ms.") AT LINE 537 is not found in our analysis.
Unit: $stack34 = virtualinvoke $stack33.<java.lang.StringBuilder: java.lang.String toString()>() AT LINE 537 is not found in our analysis.
Unit: interfaceinvoke $stack28.<org.slf4j.Logger: void warn(java.lang.String)>($stack34) AT LINE 537 is not found in our analysis.
Unit: goto [?= return result] AT LINE 537 is not found in our analysis.
Unit: $stack16 = <org.apache.hadoop.security.SecurityUtil: org.slf4j.Logger LOG> AT LINE 539 is not found in our analysis.
Unit: $stack17 = interfaceinvoke $stack16.<org.slf4j.Logger: boolean isTraceEnabled()>() AT LINE 539 is not found in our analysis.
Unit: if $stack17 == 0 goto return result AT LINE 539 is not found in our analysis.
Unit: return result AT LINE 543 is not found in our analysis.
Unit: $stack37 = <org.apache.hadoop.security.SecurityUtil: org.apache.hadoop.security.SecurityUtil$HostResolver hostResolver> AT LINE 545 is not found in our analysis.
Unit: $stack38 = interfaceinvoke $stack37.<org.apache.hadoop.security.SecurityUtil$HostResolver: java.net.InetAddress getByName(java.lang.String)>(hostname) AT LINE 545 is not found in our analysis.
Unit: return $stack38 AT LINE 545 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: int loadRMApp(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState,org.apache.hadoop.yarn.server.utils.LeveldbIterator,java.lang.String,byte[])>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: void handle(org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent)>
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: void addResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionTypeRequest,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.client.api.AMRMClient$ContainerRequest,boolean,java.lang.String)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: boolean checkAndStartWrite(org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService,org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx)>
Start to analyze method: <org.apache.hadoop.registry.client.impl.zk.CuratorService: void zkCreate(java.lang.String,org.apache.zookeeper.CreateMode,byte[],java.util.List)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void close()>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.DataNode: org.apache.hadoop.hdfs.protocol.BlockLocalPathInfo getBlockLocalPathInfo(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)>
Unit: goto [?= $stack11 = this.<org.apache.hadoop.hdfs.server.datanode.DataNode: org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetrics metrics>] AT LINE 1834 is not found in our analysis.
Unit: if info == null goto $stack12 = <org.apache.hadoop.hdfs.server.datanode.DataNode: org.slf4j.Logger LOG> AT LINE 1832 is not found in our analysis.
Unit: $stack22 = <org.apache.hadoop.hdfs.server.datanode.DataNode: org.slf4j.Logger LOG> AT LINE 1833 is not found in our analysis.
Unit: $stack23 = interfaceinvoke $stack22.<org.slf4j.Logger: boolean isTraceEnabled()>() AT LINE 1833 is not found in our analysis.
Unit: if $stack23 == 0 goto $stack11 = this.<org.apache.hadoop.hdfs.server.datanode.DataNode: org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetrics metrics> AT LINE 1833 is not found in our analysis.
Unit: goto [?= $stack11 = this.<org.apache.hadoop.hdfs.server.datanode.DataNode: org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetrics metrics>] AT LINE 1834 is not found in our analysis.
Unit: $stack12 = <org.apache.hadoop.hdfs.server.datanode.DataNode: org.slf4j.Logger LOG> AT LINE 1839 is not found in our analysis.
Unit: $stack13 = interfaceinvoke $stack12.<org.slf4j.Logger: boolean isTraceEnabled()>() AT LINE 1839 is not found in our analysis.
Unit: if $stack13 == 0 goto $stack11 = this.<org.apache.hadoop.hdfs.server.datanode.DataNode: org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetrics metrics> AT LINE 1839 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem: void setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.BlockReceiver: java.util.zip.Checksum computePartialChunkCrc(long,long)>
Start to analyze method: <org.apache.hadoop.yarn.server.uam.UnmanagedApplicationManager$AMRequestHandlerThread: void run()>
Unit: $stack38 = virtualinvoke response.<org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse: java.util.List getAllocatedContainers()>() AT LINE 612 is not found in our analysis.
Unit: if $stack38 != null goto $stack39 = virtualinvoke response.<org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse: java.util.List getAllocatedContainers()>() AT LINE 612 is not found in our analysis.
Unit: goto [?= $stack42 = virtualinvoke $stack37.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>($stack41)] AT LINE 617 is not found in our analysis.
Unit: $stack55 = virtualinvoke request.<org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest: java.util.List getAskList()>() AT LINE 589 is not found in our analysis.
Unit: if $stack55 != null goto $stack56 = virtualinvoke request.<org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest: java.util.List getAskList()>() AT LINE 589 is not found in our analysis.
Unit: goto [?= $stack59 = virtualinvoke $stack54.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>($stack58)] AT LINE 599 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.mapred.ShuffleHandler$Shuffle: void populateHeaders(java.util.List,java.lang.String,java.lang.String,int,org.jboss.netty.handler.codec.http.HttpRequest,org.jboss.netty.handler.codec.http.HttpResponse,boolean,java.util.Map)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.JournalSet: org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest getEditLogManifest(long)>
Start to analyze method: <org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor: void run()>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: void serviceInit(org.apache.hadoop.conf.Configuration)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void removeContainerQueued(org.apache.hadoop.yarn.api.records.ContainerId)>
Start to analyze method: <org.apache.hadoop.crypto.key.JavaKeyStoreProvider: org.apache.hadoop.fs.permission.FsPermission loadAndReturnPerm(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: void addResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: boolean accept(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void onCompleteLazyPersist(java.lang.String,long,long,java.io.File[],org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskContiguous: void syncBlock(java.util.List)>
Start to analyze method: <org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: java.util.List getMapsForHost(org.apache.hadoop.mapreduce.task.reduce.MapHost)>
Start to analyze method: <org.apache.hadoop.io.file.tfile.Compression$Algorithm: void returnDecompressor(org.apache.hadoop.io.compress.Decompressor)>
Start to analyze method: <org.apache.hadoop.service.AbstractService: void start()>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.DataXceiver: void blockChecksum(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: void allocateContainersToNode(org.apache.hadoop.yarn.api.records.NodeId,boolean)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService: void updateToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier,java.lang.Long)>
Start to analyze method: <org.apache.hadoop.registry.server.services.DeleteCompletionCallback: void processResult(org.apache.curator.framework.CuratorFramework,org.apache.curator.framework.api.CuratorEvent)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: void removeRMDelegationTokenState(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: void handle(org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent)>
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: void addTimelineDelegationToken(org.apache.hadoop.yarn.api.records.ContainerLaunchContext)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: void handle(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEvent)>
Start to analyze method: <org.apache.hadoop.mapred.lib.MultithreadedMapRunner: void run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)>
Start to analyze method: <org.apache.hadoop.io.TestWritableUtils: void testValue(int,int)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem: java.io.OutputStream createOutputStreamWithMode(org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.fs.permission.FsPermission)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer: void handleAppSubmitEvent(org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$AbstractDelegationTokenRenewerAppEvent)>
Unit: itor = virtualinvoke tokenConf.<org.apache.hadoop.conf.Configuration: java.util.Iterator iterator()>() AT LINE 469 is not found in our analysis.
Unit: $stack90 = interfaceinvoke itor.<java.util.Iterator: boolean hasNext()>() AT LINE 470 is not found in our analysis.
Unit: if $stack90 == 0 goto (branch) AT LINE 470 is not found in our analysis.
Unit: $stack91 = interfaceinvoke itor.<java.util.Iterator: java.lang.Object next()>() AT LINE 471 is not found in our analysis.
Unit: entry = (java.util.Map$Entry) $stack91 AT LINE 471 is not found in our analysis.
Unit: $stack93 = <org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer: org.apache.commons.logging.Log LOG> AT LINE 472 is not found in our analysis.
Unit: $stack92 = new java.lang.StringBuilder AT LINE 472 is not found in our analysis.
Unit: specialinvoke $stack92.<java.lang.StringBuilder: void <init>()>() AT LINE 472 is not found in our analysis.
Unit: $stack94 = interfaceinvoke entry.<java.util.Map$Entry: java.lang.Object getKey()>() AT LINE 472 is not found in our analysis.
Unit: $stack95 = (java.lang.String) $stack94 AT LINE 472 is not found in our analysis.
Unit: $stack96 = virtualinvoke $stack92.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($stack95) AT LINE 472 is not found in our analysis.
Unit: $stack97 = virtualinvoke $stack96.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" ===> ") AT LINE 472 is not found in our analysis.
Unit: $stack98 = interfaceinvoke entry.<java.util.Map$Entry: java.lang.Object getValue()>() AT LINE 472 is not found in our analysis.
Unit: $stack99 = (java.lang.String) $stack98 AT LINE 472 is not found in our analysis.
Unit: $stack100 = virtualinvoke $stack97.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($stack99) AT LINE 472 is not found in our analysis.
Unit: $stack101 = virtualinvoke $stack100.<java.lang.StringBuilder: java.lang.String toString()>() AT LINE 472 is not found in our analysis.
Unit: interfaceinvoke $stack93.<org.apache.commons.logging.Log: void info(java.lang.Object)>($stack101) AT LINE 472 is not found in our analysis.
Unit: goto [?= $stack64 = new org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenToRenew] AT LINE 470 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.net.unix.DomainSocketWatcher: void addNotificationSocket(java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet)>
Start to analyze method: <org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread: void nextOp()>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp: org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp$RenameResult renameToInt(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.Options$Rename[])>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: void recover(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState)>
Start to analyze method: <org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule: boolean commit()>
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: java.util.List getDatanodeListForReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime: void addCGroupParentIfRequired(java.lang.String,java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerRunCommand)>
Unit: goto [?= return] AT LINE 379 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.top.TopAuditLogger: void logAuditEvent(boolean,java.lang.String,java.net.InetAddress,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.fs.FileStatus)>
Unit: if null != status goto virtualinvoke t#2.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("perm=") AT LINE 70 is not found in our analysis.
Unit: goto [?= $stack41 = <org.apache.hadoop.hdfs.server.namenode.top.TopAuditLogger: org.slf4j.Logger LOG>] AT LINE 71 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.ha.ActiveStandbyElector: void processResult(int,java.lang.String,java.lang.Object,java.lang.String)>
Start to analyze method: <org.apache.hadoop.mapred.ShuffleHandler$Shuffle: void verifyRequest(java.lang.String,org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.handler.codec.http.HttpRequest,org.jboss.netty.handler.codec.http.HttpResponse,java.net.URL)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void storeContainerKilled(org.apache.hadoop.yarn.api.records.ContainerId)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.BlockReportTestBase: void startDNandWait(org.apache.hadoop.fs.Path,boolean)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream: org.apache.hadoop.hdfs.server.namenode.FSEditLogOp nextOpImpl(boolean)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService: void storeToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier,java.lang.Long)>
Start to analyze method: <org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory: void init(org.apache.hadoop.security.ssl.SSLFactory$Mode)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment allocateContainersOnMultiNodes(org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.PlacementSet)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList: void removeVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: void activateApplication(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId)>
Start to analyze method: <org.apache.hadoop.ipc.RPC$Server: org.apache.hadoop.ipc.RPC$Server$VerProtocolImpl getHighestSupportedProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor: void setScriptExecutable(org.apache.hadoop.fs.Path,java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem: void setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.READDIR3Response readdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
Start to analyze method: <org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider: void initFileSystem(java.net.URI)>
Unit: $stack16 = this.<org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider: java.io.File file> AT LINE 131 is not found in our analysis.
Unit: $stack17 = virtualinvoke $stack16.<java.io.File: boolean exists()>() AT LINE 131 is not found in our analysis.
Unit: if $stack17 == 0 goto $stack18 = <org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider: org.slf4j.Logger LOG> AT LINE 131 is not found in our analysis.
Unit: $stack26 = <org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider: org.slf4j.Logger LOG> AT LINE 133 is not found in our analysis.
Unit: $stack27 = interfaceinvoke $stack26.<org.slf4j.Logger: boolean isTraceEnabled()>() AT LINE 133 is not found in our analysis.
Unit: if $stack27 == 0 goto (branch) AT LINE 133 is not found in our analysis.
Unit: $stack28 = this.<org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider: java.io.File file> AT LINE 134 is not found in our analysis.
Unit: $stack29 = virtualinvoke $stack28.<java.io.File: boolean canRead()>() AT LINE 134 is not found in our analysis.
Unit: if $stack29 == 0 goto $stack30 = this.<org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider: java.io.File file> AT LINE 134 is not found in our analysis.
Unit: $stack30 = this.<org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider: java.io.File file> AT LINE 137 is not found in our analysis.
Unit: $stack31 = virtualinvoke $stack30.<java.io.File: boolean canWrite()>() AT LINE 137 is not found in our analysis.
Unit: if $stack31 == 0 goto (branch) AT LINE 137 is not found in our analysis.
Unit: goto [?= (branch)] AT LINE 138 is not found in our analysis.
Unit: $stack28 = this.<org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider: java.io.File file> AT LINE 134 is not found in our analysis.
Unit: $stack29 = virtualinvoke $stack28.<java.io.File: boolean canRead()>() AT LINE 134 is not found in our analysis.
Unit: if $stack29 == 0 goto $stack30 = this.<org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider: java.io.File file> AT LINE 134 is not found in our analysis.
Unit: $stack30 = this.<org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider: java.io.File file> AT LINE 137 is not found in our analysis.
Unit: $stack31 = virtualinvoke $stack30.<java.io.File: boolean canWrite()>() AT LINE 137 is not found in our analysis.
Unit: if $stack31 == 0 goto (branch) AT LINE 137 is not found in our analysis.
Unit: goto [?= (branch)] AT LINE 138 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.protocol.LocatedBlocks createLocatedBlocks(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo[],long,boolean,long,long,boolean,boolean,org.apache.hadoop.fs.FileEncryptionInfo)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader: void load(java.io.File)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void storeContainerDiagnostics(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.StringBuilder)>
Start to analyze method: <org.apache.hadoop.yarn.server.timeline.security.TimelineACLsManager: boolean checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.ApplicationAccessType,org.apache.hadoop.yarn.api.records.timeline.TimelineEntity)>
Unit: if callerUGI != null goto $stack57 = virtualinvoke callerUGI.<org.apache.hadoop.security.UserGroupInformation: java.lang.String getShortUserName()>() AT LINE 107 is not found in our analysis.
Unit: goto [?= $stack58 = virtualinvoke $stack56.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($stack57)] AT LINE 113 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher: void publishApplicationEvent(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: boolean anyContainerInFinalState(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)>
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void addDatanode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: void doTailEdits()>
Unit: goto [?= $stack32 = ioe#3 cmp 0L] AT LINE 268 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void adjustCrcChannelPosition(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams,int)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.READDIRPLUS3Response readdirplus(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
Start to analyze method: <org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy: void preemptOrkillSelectedContainerAfterWait(java.util.Map,long)>
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter: void writeDomain(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.records.timeline.TimelineDomain)>
Start to analyze method: <org.apache.hadoop.mapreduce.CryptoUtils: org.apache.hadoop.fs.FSDataOutputStream wrapIfNecessary(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,boolean)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor: org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterResponse finishApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterRequest)>
Start to analyze method: <org.apache.hadoop.service.launcher.ServiceLauncher: void launchServiceAndExit(java.util.List)>
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void rescanPostponedMisreplicatedBlocks()>
Unit: goto [?= i = i + 1] AT LINE 2252 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.registry.client.impl.zk.CuratorService: java.util.List zkList(java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.Allocation allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.util.List,java.util.List,java.util.List,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem: boolean mkOneDirWithMode(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.permission.FsPermission)>
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: boolean tryCloseProxy(org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.PlacementSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)>
Unit: virtualinvoke application.<org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: void showRequests()>() AT LINE 1146 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo addBlock(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[])>
Start to analyze method: <org.apache.hadoop.mapreduce.CryptoUtils: org.apache.hadoop.fs.FSDataInputStream wrapIfNecessary(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataInputStream)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp: void persistBlocks(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile,boolean)>
Start to analyze method: <org.apache.hadoop.hdfs.DataStreamer: boolean createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],long,boolean)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.security.LocalizerTokenSelector: org.apache.hadoop.security.token.Token selectToken(org.apache.hadoop.io.Text,java.util.Collection)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy: void editSchedule()>
Start to analyze method: <org.apache.hadoop.registry.client.impl.zk.RegistrySecurity: org.apache.zookeeper.data.ACL createACLfromUsername(java.lang.String,int)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: void addDirectoryToJobListCache(org.apache.hadoop.fs.Path)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.BackupImage: void applyEdits(long,int,byte[])>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: void updateResourceUsagePerUser(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,boolean)>
Start to analyze method: <org.apache.hadoop.ipc.Client$Connection: void run()>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void blockReceivedAndDeleted(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks[])>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$RMAppStateFileProcessor: void processChildNode(java.lang.String,java.lang.String,byte[])>
Start to analyze method: <org.apache.hadoop.mapreduce.lib.map.MultithreadedMapper: void run(org.apache.hadoop.mapreduce.Mapper$Context)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenCancelThread: void run()>
Start to analyze method: <org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore: org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterPolicyConfigurationResponse getPolicyConfiguration(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterPolicyConfigurationRequest)>
Unit: goto [?= $stack38 = stopTime - startTime] AT LINE 856 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor: void updateNode(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode)>
Unit: goto [?= virtualinvoke writeLock.<java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock: void unlock()>()] AT LINE 256 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$JobListCache: void delete(org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo)>
Start to analyze method: <org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: java.lang.Object getAttribute(java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: void storeReservationState(org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto,java.lang.String,java.lang.String)>
Start to analyze method: <org.apache.hadoop.metrics.ganglia.GangliaContext31: void emitMetric(java.lang.String,java.lang.String,java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer: void removeApplicationFromRenewal(org.apache.hadoop.yarn.api.records.ApplicationId)>
Start to analyze method: <org.apache.hadoop.security.Groups: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Timer)>
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void addToInvalidates(org.apache.hadoop.hdfs.protocol.Block)>
Unit: $stack23 = new java.lang.StringBuilder AT LINE 1361 is not found in our analysis.
Unit: specialinvoke $stack23.<java.lang.StringBuilder: void <init>()>() AT LINE 1361 is not found in our analysis.
Unit: goto [?= datanodes = $stack23] AT LINE 1361 is not found in our analysis.
Unit: $stack23 = null AT LINE 1361 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.util.FSDownload: org.apache.hadoop.fs.Path call()>
Start to analyze method: <org.apache.hadoop.yarn.client.api.AMRMClient: void waitFor(com.google.common.base.Supplier,int,int)>
Start to analyze method: <org.apache.hadoop.registry.server.services.RegistryAdminService: java.util.concurrent.Future submit(java.util.concurrent.Callable)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread: void run()>
Unit: $stack302 = this.<org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread: org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl this$0> AT LINE 399 is not found in our analysis.
Unit: $stack303 = $stack302.<org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: java.util.Map trackingContainers> AT LINE 399 is not found in our analysis.
Unit: $stack304 = interfaceinvoke $stack303.<java.util.Map: java.util.Collection values()>() AT LINE 399 is not found in our analysis.
Unit: i$ = interfaceinvoke $stack304.<java.util.Collection: java.util.Iterator iterator()>() AT LINE 399 is not found in our analysis.
Unit: $stack306 = interfaceinvoke i$.<java.util.Iterator: boolean hasNext()>() AT LINE 399 is not found in our analysis.
Unit: if $stack306 == 0 goto $stack307 = staticinvoke <org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: org.slf4j.Logger access$100()>() AT LINE 399 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: void startContainerInternal(org.apache.hadoop.yarn.security.ContainerTokenIdentifier,org.apache.hadoop.yarn.api.protocolrecords.StartContainerRequest)>
Start to analyze method: <org.apache.hadoop.ipc.Server$Connection: void saslProcess(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.BlockReportTestBase: void blockReport_01()>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testFinalizedRwrReplicas()>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: boolean assignReservedContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue: void setEntitlement(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.QueueEntitlement)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: int loadRMDTSecretManagerTokens(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.ActiveUsersManager: void activateApplication(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.READ3Response read(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
Start to analyze method: <org.apache.hadoop.yarn.util.RackResolver: org.apache.hadoop.net.Node coreResolve(java.lang.String)>
Unit: goto [?= $stack10 = new org.apache.hadoop.net.NodeBase] AT LINE 106 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: void addApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,boolean,boolean)>
Unit: goto [?= $stack36 = this.<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock writeLock>] AT LINE 543 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.IntraQueueCandidatesSelector: void initializeUsageAndUserLimitForCompute(org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue,java.util.Map)>
Start to analyze method: <org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void addOrUpdateToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation,boolean)>
Unit: if isUpdate == 0 goto $stack46 = "Storing " AT LINE 894 is not found in our analysis.
Unit: goto [?= $stack40 = virtualinvoke $stack38.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($stack46)] AT LINE 898 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerAppReport getSchedulerAppInfo(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl: java.lang.String createCGroup(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController,java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor: void createDir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,java.lang.String)>
Start to analyze method: <org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica: void close()>
Start to analyze method: <org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void processEventForTimelineServer(org.apache.hadoop.mapreduce.jobhistory.HistoryEvent,org.apache.hadoop.mapreduce.v2.api.records.JobId,long)>
Unit: goto [?= (branch)] AT LINE 1054 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.conf.HAUtil: java.lang.String getConfValueForRMInstance(java.lang.String,org.apache.hadoop.conf.Configuration)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask: void compute()>
Start to analyze method: <org.apache.hadoop.ipc.metrics.RetryCacheMetrics: void <init>(org.apache.hadoop.ipc.RetryCache)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner: void writeCredentials(org.apache.hadoop.fs.Path)>
Unit: $stack47 = virtualinvoke credentials.<org.apache.hadoop.security.Credentials: java.util.Collection getAllTokens()>() AT LINE 1301 is not found in our analysis.
Unit: i$ = interfaceinvoke $stack47.<java.util.Collection: java.util.Iterator iterator()>() AT LINE 1301 is not found in our analysis.
Unit: $stack49 = interfaceinvoke i$.<java.util.Iterator: boolean hasNext()>() AT LINE 1301 is not found in our analysis.
Unit: if $stack49 == 0 goto $stack28 = staticinvoke <org.apache.hadoop.security.UserGroupInformation: boolean isSecurityEnabled()>() AT LINE 1301 is not found in our analysis.
Unit: $stack50 = interfaceinvoke i$.<java.util.Iterator: java.lang.Object next()>() AT LINE 1306 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.portmap.RpcProgramPortmap: org.apache.hadoop.oncrpc.XDR unset(int,org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.XDR)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void containerAssigned(org.apache.hadoop.yarn.api.records.Container,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor$ContainerRequest)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.ContainerAllocation assignContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.PendingAsk,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits)>
Start to analyze method: <org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheCleaner: void run()>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse registerNodeManager(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest)>
Start to analyze method: <org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor: void loadNodeChildrenHelper(org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$Node,java.lang.String,java.lang.String[])>
Start to analyze method: <org.apache.hadoop.nfs.NfsExports: org.apache.hadoop.nfs.NfsExports$Match getMatch(java.lang.String)>
Start to analyze method: <org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier: void check(java.lang.String[],java.lang.String[],java.lang.String[],boolean,boolean)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.protocol.LastBlockWithStatus append(java.lang.String,java.lang.String,org.apache.hadoop.io.EnumSetWritable)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: org.apache.hadoop.yarn.api.records.NodeId getNodeIdToUnreserve(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.DataXceiver: void writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.fs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean,boolean,boolean[])>
Unit: $stack225 = <org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status: org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status SUCCESS> AT LINE 846 is not found in our analysis.
Unit: if mirrorInStatus == $stack225 goto (branch) AT LINE 846 is not found in our analysis.
Unit: goto [?= (branch)] AT LINE 846 is not found in our analysis.
Unit: $stack151 = <org.apache.hadoop.hdfs.server.datanode.DataXceiver: org.slf4j.Logger LOG> AT LINE 851 is not found in our analysis.
Unit: $stack152 = interfaceinvoke $stack151.<org.slf4j.Logger: boolean isDebugEnabled()>() AT LINE 851 is not found in our analysis.
Unit: if $stack152 != 0 goto $stack154 = <org.apache.hadoop.hdfs.server.datanode.DataXceiver: org.slf4j.Logger LOG> AT LINE 851 is not found in our analysis.
Unit: $stack165 = <org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status: org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status SUCCESS> AT LINE 865 is not found in our analysis.
Unit: if mirrorInStatus == $stack165 goto $stack161 = staticinvoke <org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto: org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto$Builder newBuilder()>() AT LINE 865 is not found in our analysis.
Unit: $stack134 = <org.apache.hadoop.hdfs.server.datanode.DataXceiver: org.slf4j.Logger LOG> AT LINE 872 is not found in our analysis.
Unit: $stack135 = interfaceinvoke $stack134.<org.slf4j.Logger: boolean isTraceEnabled()>() AT LINE 872 is not found in our analysis.
Unit: if $stack135 == 0 goto $stack136 = <org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status: org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status SUCCESS> AT LINE 872 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.client.HdfsUtils: boolean isHealthy(java.net.URI)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: void apply(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)>
Start to analyze method: <org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$ProcessSmapMemoryInfo: void setMemInfo(java.lang.String,java.lang.String)>
Start to analyze method: <org.apache.hadoop.portmap.RpcProgramPortmap: org.apache.hadoop.oncrpc.XDR set(int,org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.XDR)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: void updateNonActiveUsersResourceUsage(java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor: org.apache.hadoop.fs.Path localizeClasspathJar(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.lang.String)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: void executeWriteBack()>
Start to analyze method: <org.apache.hadoop.ipc.Client$Connection$1: java.lang.Object run()>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem: boolean delete(org.apache.hadoop.fs.Path,boolean)>
Start to analyze method: <org.apache.hadoop.security.SaslRpcClient: org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth selectSaslClient(java.util.List)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ReservedContainerCandidatesSelector: java.util.Map selectCandidates(java.util.Map,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>
Start to analyze method: <org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys exportKeys()>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$1: void run()>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp: org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo deleteInternal(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,boolean)>
Unit: $stack36 = virtualinvoke iip.<org.apache.hadoop.hdfs.server.namenode.INodesInPath: java.lang.String getPath()>() AT LINE 180 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.federation.resolver.DefaultSubClusterResolverImpl: void load()>
Start to analyze method: <org.apache.hadoop.mapred.ShuffleHandler$Shuffle: void messageReceived(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.MessageEvent)>
Start to analyze method: <org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$NMCallbackHandler: void onContainerStopped(org.apache.hadoop.yarn.api.records.ContainerId)>
Start to analyze method: <org.apache.hadoop.yarn.server.security.BaseContainerTokenSecretManager: byte[] createPassword(org.apache.hadoop.yarn.security.ContainerTokenIdentifier)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: void handleInitContainerResources(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ContainerLocalizationRequestEvent)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: boolean moveReservedContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.SETATTR3Response setattr(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: void recover(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState)>
Start to analyze method: <org.apache.hadoop.yarn.server.security.BaseContainerTokenSecretManager: byte[] retrievePasswordInternal(org.apache.hadoop.yarn.security.ContainerTokenIdentifier,org.apache.hadoop.yarn.server.security.MasterKeyData)>
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: int computeReplicationWorkForBlocks(java.util.List)>
Unit: priority#10 = interfaceinvoke work.<java.util.List: java.util.Iterator iterator()>() AT LINE 1638 is not found in our analysis.
Unit: $stack29 = interfaceinvoke priority#10.<java.util.Iterator: boolean hasNext()>() AT LINE 1638 is not found in our analysis.
Unit: if $stack29 == 0 goto $stack31 = <org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.slf4j.Logger blockLog> AT LINE 1638 is not found in our analysis.
Unit: $stack39 = interfaceinvoke priority#10.<java.util.Iterator: java.lang.Object next()>() AT LINE 1655 is not found in our analysis.
Unit: i$#11 = (org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork) $stack39 AT LINE 1655 is not found in our analysis.
Unit: block#20 = virtualinvoke i$#11.<org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[] getTargets()>() AT LINE 1639 is not found in our analysis.
Unit: if block#20 == null goto (branch) AT LINE 1640 is not found in our analysis.
Unit: $stack42 = lengthof block#20 AT LINE 1649 is not found in our analysis.
Unit: if $stack42 == 0 goto (branch) AT LINE 1649 is not found in our analysis.
Unit: arr$#28 = block#20 AT LINE 1642 is not found in our analysis.
Unit: len$#29 = lengthof arr$#28 AT LINE 1642 is not found in our analysis.
Unit: i$#30 = 0 AT LINE 1642 is not found in our analysis.
Unit: if i$#30 >= len$#29 goto $stack45 = <org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.slf4j.Logger blockLog> AT LINE 1642 is not found in our analysis.
Unit: i$#30 = i$#30 + 1 AT LINE 1642 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand cacheReport()>
Start to analyze method: <org.apache.hadoop.ipc.DecayRpcScheduler: void updateAverageResponseTime(boolean)>
Start to analyze method: <org.apache.hadoop.security.http.CrossOriginFilter: void doCrossFilter(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.RENAME3Response rename(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.LeaseManager: void removeLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,long)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void removeContainer(org.apache.hadoop.yarn.api.records.ContainerId)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: java.util.Map parseCredentials(java.util.Map)>
Unit: $stack13 = interfaceinvoke map.<java.util.Map: java.util.Set entrySet()>() AT LINE 755 is not found in our analysis.
Unit: i$ = interfaceinvoke $stack13.<java.util.Set: java.util.Iterator iterator()>() AT LINE 755 is not found in our analysis.
Unit: $stack15 = interfaceinvoke i$.<java.util.Iterator: boolean hasNext()>() AT LINE 755 is not found in our analysis.
Unit: if $stack15 == 0 goto return map AT LINE 755 is not found in our analysis.
Unit: $stack17 = interfaceinvoke i$.<java.util.Iterator: java.lang.Object next()>() AT LINE 760 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.ipc.Server: void registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,org.apache.hadoop.ipc.RPC$RpcInvoker)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp: org.apache.hadoop.fs.FileStatus mkdirs(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: void processCommits(long)>
Start to analyze method: <org.apache.hadoop.security.Groups: org.apache.hadoop.security.Groups getUserToGroupsMappingService(org.apache.hadoop.conf.Configuration)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.LeaseManager: boolean checkLeases()>
Unit: if completed == 0 goto $stack76 = <org.apache.hadoop.hdfs.server.namenode.LeaseManager: org.apache.commons.logging.Log LOG> AT LINE 581 is not found in our analysis.
Unit: goto [?= (branch)] AT LINE 582 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: void removeApplicationStateInternal(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor: void addNode(java.util.List,org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode)>
Start to analyze method: <org.apache.hadoop.service.CompositeService: void addService(org.apache.hadoop.service.Service)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: void trimWriteRequest(org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx,long)>
Start to analyze method: <org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue: void addCall(org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall)>
Start to analyze method: <org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager: void removeStoredToken(org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier)>
Start to analyze method: <org.apache.hadoop.ha.ActiveStandbyElector: boolean becomeActive()>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSNamesystem: java.util.Collection listCorruptFileBlocks(java.lang.String,java.lang.String[])>
Start to analyze method: <org.apache.hadoop.yarn.server.security.ApplicationACLsManager: boolean checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.ApplicationAccessType,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId)>
Unit: goto [?= $stack18 = this.<org.apache.hadoop.yarn.server.security.ApplicationACLsManager: org.apache.hadoop.yarn.security.AdminACLsManager adminAclsManager>] AT LINE 116 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: void containerFailedOnHost(java.lang.String)>
Start to analyze method: <org.apache.hadoop.security.SaslRpcClient: java.lang.String getServerPrincipal(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: void addApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,boolean,boolean)>
Unit: goto [?= $stack37 = this.<org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock writeLock>] AT LINE 844 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.Nfs3Utils: void writeChannelCommit(org.jboss.netty.channel.Channel,org.apache.hadoop.oncrpc.XDR,int)>
Start to analyze method: <org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor: void kill(org.apache.hadoop.util.Daemon)>
Start to analyze method: <org.apache.hadoop.ha.ActiveStandbyElector: void processResult(int,java.lang.String,java.lang.Object,org.apache.zookeeper.data.Stat)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl: boolean checkClosed()>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender: void sendLifeline()>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: org.apache.hadoop.yarn.api.records.Resource getHeadroom()>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.security.MRDelegationTokenRenewer: org.apache.hadoop.mapreduce.v2.api.MRClientProtocol instantiateHistoryProxy(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.DockerContainerExecutor: boolean signalContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerSignalContext)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.util.ProcessIdFileReader: java.lang.String getProcessId(org.apache.hadoop.fs.Path)>
Unit: if processId == null goto $stack25 = "null" AT LINE 103 is not found in our analysis.
Unit: goto [?= $stack20 = virtualinvoke $stack19.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($stack25)] AT LINE 106 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData getProxy(java.lang.String,org.apache.hadoop.yarn.api.records.ContainerId)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.util.MRApps: java.lang.ClassLoader createJobClassLoader(org.apache.hadoop.conf.Configuration)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd: org.apache.hadoop.oncrpc.XDR mnt(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress)>
Start to analyze method: <org.apache.hadoop.ipc.RetryCache: org.apache.hadoop.ipc.RetryCache$CacheEntry waitForCompletion(org.apache.hadoop.ipc.RetryCache$CacheEntry)>
Start to analyze method: <org.apache.hadoop.registry.client.impl.zk.RegistrySecurity: void bindJVMtoJAASFile(java.io.File)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void handleApplicationAttemptStateOp(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationAttemptStateData,org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$AppAttemptOp)>
Unit: goto [?= return] AT LINE 929 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$AttemptDirCache: org.apache.hadoop.fs.Path getAppRootDir(java.lang.String)>
Start to analyze method: <org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter: void ignoreSnapshotName(long)>
Start to analyze method: <org.apache.hadoop.hdfs.BlockStorageLocationUtil: java.util.Map associateVolumeIdsWithBlocks(java.util.List,java.util.Map)>
Unit: goto [?= j = j + 1] AT LINE 276 is not found in our analysis.
Unit: goto [?= j = j + 1] AT LINE 256 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher: void logDecommissioningNodesStatus()>
Unit: $stack12 = this.<org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher: java.util.HashMap decomNodes> AT LINE 386 is not found in our analysis.
Unit: $stack13 = virtualinvoke $stack12.<java.util.HashMap: int size()>() AT LINE 386 is not found in our analysis.
Unit: if $stack13 != 0 goto $stack14 = this.<org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher: org.apache.hadoop.yarn.util.MonotonicClock mclock> AT LINE 386 is not found in our analysis.
Unit: $stack16 = this.<org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher: java.util.HashMap decomNodes> AT LINE 389 is not found in our analysis.
Unit: $stack17 = virtualinvoke $stack16.<java.util.HashMap: java.util.Collection values()>() AT LINE 389 is not found in our analysis.
Unit: i$ = interfaceinvoke $stack17.<java.util.Collection: java.util.Iterator iterator()>() AT LINE 389 is not found in our analysis.
Unit: $stack19 = interfaceinvoke i$.<java.util.Iterator: boolean hasNext()>() AT LINE 389 is not found in our analysis.
Unit: if $stack19 == 0 goto return AT LINE 389 is not found in our analysis.
Unit: $stack20 = interfaceinvoke i$.<java.util.Iterator: java.lang.Object next()>() AT LINE 418 is not found in our analysis.
Unit: d = (org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher$DecommissioningNodeContext) $stack20 AT LINE 418 is not found in our analysis.
Unit: $stack22 = staticinvoke <org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher$DecommissioningNodeContext: org.apache.hadoop.yarn.api.records.NodeId access$1200(org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher$DecommissioningNodeContext)>(d) AT LINE 391 is not found in our analysis.
Unit: s = virtualinvoke this.<org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher: org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher$DecommissioningNodeStatus checkDecommissioningStatus(org.apache.hadoop.yarn.api.records.NodeId)>($stack22) AT LINE 391 is not found in our analysis.
Unit: $stack44 = <org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher$DecommissioningNodeStatus: org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher$DecommissioningNodeStatus WAIT_APP> AT LINE 399 is not found in our analysis.
Unit: if s == $stack44 goto $stack45 = newarray (java.lang.Object)[1] AT LINE 399 is not found in our analysis.
Unit: $stack99 = <org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher$DecommissioningNodeStatus: org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher$DecommissioningNodeStatus WAIT_CONTAINER> AT LINE 415 is not found in our analysis.
Unit: if s != $stack99 goto $stack52 = staticinvoke <org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher$DecommissioningNodeContext: java.util.Set access$500(org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher$DecommissioningNodeContext)>(d) AT LINE 415 is not found in our analysis.
Unit: $stack52 = staticinvoke <org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher$DecommissioningNodeContext: java.util.Set access$500(org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher$DecommissioningNodeContext)>(d) AT LINE 403 is not found in our analysis.
Unit: i$ = interfaceinvoke $stack52.<java.util.Set: java.util.Iterator iterator()>() AT LINE 403 is not found in our analysis.
Unit: $stack54 = interfaceinvoke i$.<java.util.Iterator: boolean hasNext()>() AT LINE 403 is not found in our analysis.
Unit: if $stack54 == 0 goto $stack56 = <org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher: org.apache.commons.logging.Log LOG> AT LINE 403 is not found in our analysis.
Unit: $stack62 = interfaceinvoke i$.<java.util.Iterator: java.lang.Object next()>() AT LINE 417 is not found in our analysis.
Unit: aid = (org.apache.hadoop.yarn.api.records.ApplicationId) $stack62 AT LINE 417 is not found in our analysis.
Unit: $stack68 = this.<org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher: org.apache.hadoop.yarn.server.resourcemanager.RMContext rmContext> AT LINE 405 is not found in our analysis.
Unit: $stack69 = interfaceinvoke $stack68.<org.apache.hadoop.yarn.server.resourcemanager.RMContext: java.util.concurrent.ConcurrentMap getRMApps()>() AT LINE 405 is not found in our analysis.
Unit: $stack70 = interfaceinvoke $stack69.<java.util.concurrent.ConcurrentMap: java.lang.Object get(java.lang.Object)>(aid) AT LINE 405 is not found in our analysis.
Unit: rmApp = (org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp) $stack70 AT LINE 405 is not found in our analysis.
Unit: if rmApp == null goto (branch) AT LINE 406 is not found in our analysis.
Unit: $stack77 = interfaceinvoke rmApp.<org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp: java.lang.String getApplicationType()>() AT LINE 407 is not found in our analysis.
Unit: if $stack77 != null goto $stack78 = interfaceinvoke rmApp.<org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp: java.lang.String getApplicationType()>() AT LINE 407 is not found in our analysis.
Unit: goto [?= $stack72[1] = $stack78] AT LINE 415 is not found in our analysis.
Unit: return AT LINE 418 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor: void writeStringTableSection()>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: org.apache.hadoop.yarn.server.api.records.NodeStatus getNodeStatus(int)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficControlBandwidthHandlerImpl: java.util.List teardown()>
Start to analyze method: <org.apache.hadoop.hdfs.server.common.JspHelper: org.apache.hadoop.security.UserGroupInformation getUGI(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,boolean)>
Start to analyze method: <org.apache.hadoop.io.retry.RetryInvocationHandler: void log(java.lang.reflect.Method,boolean,int,long,java.lang.Exception)>
Unit: return AT LINE 398 is not found in our analysis.
Unit: $stack10 = new java.lang.StringBuilder AT LINE 401 is not found in our analysis.
Unit: specialinvoke $stack10.<java.lang.StringBuilder: void <init>()>() AT LINE 401 is not found in our analysis.
Unit: $stack11 = new java.lang.StringBuilder AT LINE 401 is not found in our analysis.
Unit: specialinvoke $stack11.<java.lang.StringBuilder: void <init>()>() AT LINE 401 is not found in our analysis.
Unit: $stack12 = virtualinvoke $stack11.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(ex) AT LINE 401 is not found in our analysis.
Unit: $stack13 = virtualinvoke $stack12.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(", while invoking ") AT LINE 401 is not found in our analysis.
Unit: $stack14 = virtualinvoke $stack13.<java.lang.StringBuilder: java.lang.String toString()>() AT LINE 401 is not found in our analysis.
Unit: $stack15 = virtualinvoke $stack10.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($stack14) AT LINE 401 is not found in our analysis.
Unit: $stack16 = this.<org.apache.hadoop.io.retry.RetryInvocationHandler: org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor proxyDescriptor> AT LINE 401 is not found in our analysis.
Unit: $stack17 = virtualinvoke $stack16.<org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor: org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo getProxyInfo()>() AT LINE 401 is not found in our analysis.
Unit: $stack18 = virtualinvoke method.<java.lang.reflect.Method: java.lang.String getName()>() AT LINE 401 is not found in our analysis.
Unit: $stack19 = virtualinvoke $stack17.<org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo: java.lang.String getString(java.lang.String)>($stack18) AT LINE 401 is not found in our analysis.
Unit: b = virtualinvoke $stack15.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($stack19) AT LINE 401 is not found in our analysis.
Unit: if failovers <= 0 goto (branch) AT LINE 404 is not found in our analysis.
Unit: $stack39 = virtualinvoke b.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" after ") AT LINE 405 is not found in our analysis.
Unit: $stack40 = virtualinvoke $stack39.<java.lang.StringBuilder: java.lang.StringBuilder append(int)>(failovers) AT LINE 405 is not found in our analysis.
Unit: virtualinvoke $stack40.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" failover attempts") AT LINE 405 is not found in our analysis.
Unit: if isFailover == 0 goto $stack35 = ". Retrying " AT LINE 407 is not found in our analysis.
Unit: $stack35 = ". Trying to failover " AT LINE 410 is not found in our analysis.
Unit: goto [?= virtualinvoke b.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($stack35)] AT LINE 410 is not found in our analysis.
Unit: $stack35 = ". Retrying " AT LINE 410 is not found in our analysis.
Unit: virtualinvoke b.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($stack35) AT LINE 407 is not found in our analysis.
Unit: $stack22 = delay cmp 0L AT LINE 408 is not found in our analysis.
Unit: if $stack22 <= 0 goto $stack32 = "immediately." AT LINE 408 is not found in our analysis.
Unit: $stack28 = new java.lang.StringBuilder AT LINE 415 is not found in our analysis.
Unit: specialinvoke $stack28.<java.lang.StringBuilder: void <init>()>() AT LINE 415 is not found in our analysis.
Unit: $stack29 = virtualinvoke $stack28.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("after sleeping for ") AT LINE 415 is not found in our analysis.
Unit: $stack30 = virtualinvoke $stack29.<java.lang.StringBuilder: java.lang.StringBuilder append(long)>(delay) AT LINE 415 is not found in our analysis.
Unit: $stack31 = virtualinvoke $stack30.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("ms.") AT LINE 415 is not found in our analysis.
Unit: $stack32 = virtualinvoke $stack31.<java.lang.StringBuilder: java.lang.String toString()>() AT LINE 415 is not found in our analysis.
Unit: goto [?= virtualinvoke b.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($stack32)] AT LINE 415 is not found in our analysis.
Unit: $stack32 = "immediately." AT LINE 415 is not found in our analysis.
Unit: virtualinvoke b.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($stack32) AT LINE 408 is not found in our analysis.
Unit: if info == 0 goto $stack24 = <org.apache.hadoop.io.retry.RetryInvocationHandler: org.slf4j.Logger LOG> AT LINE 410 is not found in our analysis.
Unit: $stack26 = <org.apache.hadoop.io.retry.RetryInvocationHandler: org.slf4j.Logger LOG> AT LINE 411 is not found in our analysis.
Unit: $stack27 = virtualinvoke b.<java.lang.StringBuilder: java.lang.String toString()>() AT LINE 411 is not found in our analysis.
Unit: interfaceinvoke $stack26.<org.slf4j.Logger: void info(java.lang.String)>($stack27) AT LINE 411 is not found in our analysis.
Unit: goto [?= return] AT LINE 411 is not found in our analysis.
Unit: return AT LINE 415 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.QueuePriorityContainerCandidateSelector: void intializePriorityDigraph()>
Unit: goto [?= (branch)] AT LINE 152 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.fs.RawLocalFileSystem: boolean handleEmptyDstDirectoryOnWindows(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.Path,java.io.File)>
Start to analyze method: <org.apache.hadoop.conf.Configuration: javax.xml.stream.XMLStreamReader parse(java.net.URL,boolean)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp: org.apache.hadoop.hdfs.server.namenode.INodesInPath unprotectedRenameTo(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INodesInPath,long)>
Start to analyze method: <org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: void stopThreads()>
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$AttemptDirCache: org.apache.hadoop.fs.Path createAttemptDir(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)>
Start to analyze method: <org.apache.hadoop.registry.client.impl.zk.CuratorService: org.apache.zookeeper.data.Stat zkStat(java.lang.String)>
Start to analyze method: <org.apache.hadoop.security.UserGroupInformation: void reloginFromTicketCache()>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker: void checkNotSymlink(org.apache.hadoop.hdfs.server.namenode.INode,byte[][],int)>
Start to analyze method: <org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$NMCallbackHandler: void onContainerStatusReceived(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.ContainerStatus)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.DataNode: org.apache.hadoop.hdfs.server.protocol.InterDatanodeProtocol createInterDataNodeProtocolProxy(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.conf.Configuration,int,boolean)>
Start to analyze method: <org.apache.hadoop.mapred.YARNRunner: java.util.List generateResourceRequests()>
Unit: i$ = interfaceinvoke amResourceRequests.<java.util.List: java.util.Iterator iterator()>() AT LINE 716 is not found in our analysis.
Unit: $stack33 = interfaceinvoke i$.<java.util.Iterator: boolean hasNext()>() AT LINE 716 is not found in our analysis.
Unit: if $stack33 == 0 goto return amResourceRequests AT LINE 716 is not found in our analysis.
Unit: $stack35 = interfaceinvoke i$.<java.util.Iterator: java.lang.Object next()>() AT LINE 722 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.DataStreamer: void initDataStreaming()>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp: org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp$RenameResult renameTo(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,boolean,org.apache.hadoop.fs.Options$Rename[])>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: void handle(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent)>
Start to analyze method: <org.apache.hadoop.ipc.Server$Connection: org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto buildSaslResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslState,byte[])>
Unit: if replyToken == null goto $stack24 = null AT LINE 1927 is not found in our analysis.
Unit: goto [?= $stack17 = virtualinvoke $stack16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>($stack24)] AT LINE 1933 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.mapred.QueueManager: boolean hasAccess(java.lang.String,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.security.UserGroupInformation)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager: void updateStoredToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier,long)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void storeApplicationStateInternal(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData)>
Start to analyze method: <org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor: javax.xml.stream.events.XMLEvent expectTag(java.lang.String,boolean)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.RMDIR3Response rmdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: void evictBlocks(long)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx: long dumpData(java.io.FileOutputStream,java.io.RandomAccessFile)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void loadDelegationTokenFromNode(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState,java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter: java.util.Set getProxyAddresses()>
Start to analyze method: <org.apache.hadoop.security.SecurityUtil: void setTokenService(org.apache.hadoop.security.token.Token,java.net.InetSocketAddress)>
Unit: goto [?= return] AT LINE 426 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp: long delete(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,java.util.List,java.util.List,long)>
Unit: $stack38 = virtualinvoke iip.<org.apache.hadoop.hdfs.server.namenode.INodesInPath: java.lang.String getPath()>() AT LINE 55 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testZeroLenReplicas()>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM: void setNodeId(org.apache.hadoop.yarn.api.records.NodeId)>
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.client.api.NMTokenCache)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx offerNextToWrite()>
Start to analyze method: <org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void addOrUpdateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey,boolean)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.protocol.HdfsFileStatus create(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,org.apache.hadoop.io.EnumSetWritable,boolean,short,long,org.apache.hadoop.crypto.CryptoProtocolVersion[])>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler: void logLineFromTasksFile(java.io.File)>
Unit: $stack10 = new java.io.BufferedReader AT LINE 257 is not found in our analysis.
Unit: $stack11 = new java.io.InputStreamReader AT LINE 257 is not found in our analysis.
Unit: $stack12 = new java.io.FileInputStream AT LINE 257 is not found in our analysis.
Unit: $stack13 = new java.lang.StringBuilder AT LINE 257 is not found in our analysis.
Unit: specialinvoke $stack13.<java.lang.StringBuilder: void <init>()>() AT LINE 257 is not found in our analysis.
Unit: $stack14 = virtualinvoke $stack13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(cgf) AT LINE 257 is not found in our analysis.
Unit: $stack15 = virtualinvoke $stack14.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/tasks") AT LINE 257 is not found in our analysis.
Unit: $stack16 = virtualinvoke $stack15.<java.lang.StringBuilder: java.lang.String toString()>() AT LINE 257 is not found in our analysis.
Unit: specialinvoke $stack12.<java.io.FileInputStream: void <init>(java.lang.String)>($stack16) AT LINE 257 is not found in our analysis.
Unit: specialinvoke $stack11.<java.io.InputStreamReader: void <init>(java.io.InputStream,java.lang.String)>($stack12, "UTF-8") AT LINE 257 is not found in our analysis.
Unit: specialinvoke $stack10.<java.io.BufferedReader: void <init>(java.io.Reader)>($stack11) AT LINE 257 is not found in our analysis.
Unit: inl = $stack10 AT LINE 257 is not found in our analysis.
Unit: l4 = null AT LINE 257 is not found in our analysis.
Unit: str = virtualinvoke inl.<java.io.BufferedReader: java.lang.String readLine()>() AT LINE 260 is not found in our analysis.
Unit: if str == null goto (branch) AT LINE 261 is not found in our analysis.
Unit: if inl == null goto (branch) AT LINE 264 is not found in our analysis.
Unit: if l4 == null goto virtualinvoke inl.<java.io.BufferedReader: void close()>() AT LINE 268 is not found in our analysis.
Unit: virtualinvoke inl.<java.io.BufferedReader: void close()>() AT LINE 266 is not found in our analysis.
Unit: goto [?= (branch)] AT LINE 266 is not found in our analysis.
Unit: virtualinvoke inl.<java.io.BufferedReader: void close()>() AT LINE 268 is not found in our analysis.
Unit: goto [?= (branch)] AT LINE 268 is not found in our analysis.
Unit: goto [?= return] AT LINE 266 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void ref(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.DFSClientCache: org.apache.hadoop.security.UserGroupInformation getUserGroupInformation(java.lang.String,org.apache.hadoop.security.UserGroupInformation)>
Start to analyze method: <org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot registerSlot(int,org.apache.hadoop.hdfs.ExtendedBlockId)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$ScheduleTransition: org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState transition(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: org.apache.hadoop.yarn.api.records.Resource getComputedResourceLimitForAllUsers(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)>
Start to analyze method: <org.apache.hadoop.util.concurrent.ExecutorHelper: void logThrowableFromAfterExecute(java.lang.Runnable,java.lang.Throwable)>
Start to analyze method: <org.apache.hadoop.mapreduce.security.TokenCache: org.apache.hadoop.security.Credentials loadTokens(java.lang.String,org.apache.hadoop.mapred.JobConf)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService: void removeTokenMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp: org.apache.hadoop.hdfs.server.namenode.INodesInPath createSingleDirectory(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,byte[],org.apache.hadoop.fs.permission.PermissionStatus)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor: void removeNode(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode)>
Unit: if node == null goto $stack19 = <org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor: org.apache.commons.logging.Log LOG> AT LINE 204 is not found in our analysis.
Unit: goto [?= return] AT LINE 205 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher: void publishContainerEvent(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent)>
Start to analyze method: <org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager: void storeNewMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.blacklist.SimpleBlacklistManager: org.apache.hadoop.yarn.api.records.ResourceBlacklistRequest getBlacklistUpdates()>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.DataXceiver: void run()>
Start to analyze method: <org.apache.hadoop.mapreduce.lib.input.FileInputFormat: java.util.List getSplits(org.apache.hadoop.mapreduce.JobContext)>
Unit: $stack72 = virtualinvoke file.<org.apache.hadoop.fs.FileStatus: long getBlockSize()>() AT LINE 427 is not found in our analysis.
Unit: $stack73 = staticinvoke <java.lang.Math: long min(long,long)>($stack72, minSize) AT LINE 427 is not found in our analysis.
Unit: $stack74 = length cmp $stack73 AT LINE 427 is not found in our analysis.
Unit: if $stack74 <= 0 goto $stack65 = blkLocations[0] AT LINE 427 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.ipc.RPC$Server: void registerProtocolAndImpl(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd: org.apache.hadoop.oncrpc.XDR umnt(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress)>
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: void put(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl$ResourceRequestInfo)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM: void appAttemptStartContainer(org.apache.hadoop.yarn.security.NMTokenIdentifier)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: void handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent)>
Start to analyze method: <org.apache.hadoop.registry.client.impl.zk.RegistrySecurity: void resetDigestACLs()>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.web.webhdfs.HdfsWriter: void exceptionCaught(io.netty.channel.ChannelHandlerContext,java.lang.Throwable)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void enqueue(long,boolean,long,org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status)>
Start to analyze method: <org.apache.hadoop.hdfs.web.resources.ExceptionHandler: javax.ws.rs.core.Response toResponse(java.lang.Exception)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache$StreamMonitor: void run()>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: void handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ResourceEvent)>
Unit: $stack25 = this.<org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: org.apache.hadoop.fs.Path localPath> AT LINE 206 is not found in our analysis.
Unit: if $stack25 == null goto $stack37 = "" AT LINE 206 is not found in our analysis.
Unit: goto [?= $stack26 = virtualinvoke $stack24.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($stack37)] AT LINE 213 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.mapred.lib.MultithreadedMapRunner: void configure(org.apache.hadoop.mapred.JobConf)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime: org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.LinuxContainerRuntime pickContainerRuntime(java.util.Map)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp: org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp$RenameResult renameToInt(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,java.lang.String,boolean)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.NodesListManager$CachedResolver$ExpireChecker: void run()>
Start to analyze method: <org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter: void loadDirectoriesInINodeSection(java.io.InputStream)>
Unit: $stack21 = i % 10000 AT LINE 563 is not found in our analysis.
Unit: if $stack21 != 0 goto $stack17 = virtualinvoke p.<org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode: boolean hasDirectory()>() AT LINE 563 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void finishResourceLocalization(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LocalizedResourceProto)>
Start to analyze method: <org.apache.hadoop.mapred.TestMultiFileInputFormat: org.apache.hadoop.fs.Path initFiles(org.apache.hadoop.fs.FileSystem,int,int)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor: java.lang.String executePrivilegedOperation(java.util.List,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation,java.io.File,java.util.Map,boolean,boolean)>
Start to analyze method: <org.apache.hadoop.yarn.server.webproxy.ProxyUtils: void sendRedirect(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String)>
Start to analyze method: <org.apache.hadoop.security.SaslInputStream: int readMoreData()>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.PreemptableResourceCalculator: void calculateResToObtainByPartitionForLeafQueues(java.util.Set,org.apache.hadoop.yarn.api.records.Resource)>
Start to analyze method: <org.apache.hadoop.fs.loadGenerator.LoadGenerator: int generateLoadOnNN()>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void loadReservationSystemState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void removeLocalizedResource(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.fs.Path)>
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks: java.util.List invalidateWork(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
Start to analyze method: <org.apache.hadoop.util.Shell: boolean isSetsidSupported()>
Unit: if shexec == null goto $stack16 = "(null executor)" AT LINE 807 is not found in our analysis.
Unit: goto [?= $stack12 = virtualinvoke $stack11.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>($stack16)] AT LINE 811 is not found in our analysis.
Unit: goto [?= return setsidSupported] AT LINE 807 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.BPServiceActor: org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse sendHeartBeat(boolean)>
Start to analyze method: <org.apache.hadoop.mapred.TaskAttemptListenerImpl: boolean ping(org.apache.hadoop.mapred.TaskAttemptID)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testRBW_RWRReplicas()>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficControlBandwidthHandlerImpl: java.util.List reacquireContainer(org.apache.hadoop.yarn.api.records.ContainerId)>
Start to analyze method: <org.apache.hadoop.registry.client.impl.zk.RegistryOperationsService: org.apache.hadoop.registry.client.types.RegistryPathStatus stat(java.lang.String)>
Start to analyze method: <org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter: void buildNamespace(java.io.InputStream,java.util.List)>
Unit: $stack35 = count % 10000 AT LINE 593 is not found in our analysis.
Unit: if $stack35 != 0 goto parentId = virtualinvoke e.<org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry: long getParent()>() AT LINE 593 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirConcatOp: void unprotectedConcat(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INodeFile[],long)>
Unit: $stack38 = virtualinvoke targetIIP.<org.apache.hadoop.hdfs.server.namenode.INodesInPath: java.lang.String getPath()>() AT LINE 231 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: boolean truncate(java.lang.String,long,java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.event.AsyncDispatcher: void dispatch(org.apache.hadoop.yarn.event.Event)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void storeContainerPaused(org.apache.hadoop.yarn.api.records.ContainerId)>
Start to analyze method: <org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: void checkAccess(org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier$AccessMode)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.security.client.ClientHSTokenSelector: org.apache.hadoop.security.token.Token selectToken(org.apache.hadoop.io.Text,java.util.Collection)>
Start to analyze method: <org.apache.hadoop.hdfs.DFSClient: org.apache.hadoop.fs.MD5MD5CRC32FileChecksum getFileChecksum(java.lang.String,long)>
Unit: if i != 0 goto $stack116 = <org.apache.hadoop.hdfs.DFSClient: org.slf4j.Logger LOG> AT LINE 1858 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber: void run()>
Start to analyze method: <org.apache.hadoop.hdfs.DFSClient: void <init>(java.net.URI,org.apache.hadoop.hdfs.protocol.ClientProtocol,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Statistics)>
Unit: $stack96 = lengthof localInterfaces AT LINE 369 is not found in our analysis.
Unit: if 0 == $stack96 goto $stack48 = virtualinvoke conf.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("dfs.client.cache.drop.behind.reads") AT LINE 369 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp: org.apache.hadoop.hdfs.server.namenode.INodesInPath addFile(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,byte[],org.apache.hadoop.fs.permission.PermissionStatus,short,long,java.lang.String,java.lang.String)>
Start to analyze method: <org.apache.hadoop.mapred.Task: void initialize(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.JobID,org.apache.hadoop.mapred.Reporter,boolean)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: long updateNonSequentialWriteInMemory(long)>
Start to analyze method: <org.apache.hadoop.service.launcher.AbstractLaunchableService: org.apache.hadoop.conf.Configuration bindArgs(org.apache.hadoop.conf.Configuration,java.util.List)>
Unit: i$ = interfaceinvoke args.<java.util.List: java.util.Iterator iterator()>() AT LINE 62 is not found in our analysis.
Unit: $stack13 = interfaceinvoke i$.<java.util.Iterator: boolean hasNext()>() AT LINE 62 is not found in our analysis.
Unit: if $stack13 == 0 goto return config AT LINE 62 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void assignMapsWithLocality(java.util.List)>
Unit: goto [?= (branch)] AT LINE 1405 is not found in our analysis.
Unit: goto [?= (branch)] AT LINE 1439 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService: void removeTokenMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl: void addCredentials()>
Start to analyze method: <org.apache.hadoop.security.ssl.SSLFactory: void <init>(org.apache.hadoop.security.ssl.SSLFactory$Mode,org.apache.hadoop.conf.Configuration)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: void storeRMDTMasterKeyState(org.apache.hadoop.security.token.delegation.DelegationKey)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.DataNode: void checkBlockToken(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier$AccessMode)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer findNodeToUnreserve(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.Resource)>
Start to analyze method: <org.apache.hadoop.net.NetworkTopologyWithNodeGroup: void remove(org.apache.hadoop.net.Node)>
Start to analyze method: <org.apache.hadoop.oncrpc.security.CredentialsSys: void <clinit>()>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: org.apache.hadoop.yarn.api.records.Resource getComputedResourceLimitForActiveUsers(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)>
Start to analyze method: <org.apache.hadoop.ha.ActiveStandbyElector: void enterNeutralMode()>
Start to analyze method: <org.apache.hadoop.registry.client.impl.zk.CuratorService: java.util.List zkGetACLS(java.lang.String)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.crypto.CryptoProtocolVersion chooseProtocolVersion(org.apache.hadoop.hdfs.protocol.EncryptionZone,org.apache.hadoop.crypto.CryptoProtocolVersion[])>
Unit: goto [?= i$ = i$ + 1] AT LINE 2199 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: void storeApplicationStateInternal(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppAttemptTransition: org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMStateStoreState transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.DockerContainerExecutor: void writeLaunchEnv(java.io.OutputStream,java.util.Map,java.util.Map,java.util.List,org.apache.hadoop.fs.Path,java.lang.String)>
Unit: $stack43 = new java.io.ByteArrayOutputStream AT LINE 394 is not found in our analysis.
Unit: specialinvoke $stack43.<java.io.ByteArrayOutputStream: void <init>()>() AT LINE 394 is not found in our analysis.
Unit: i$#42 = $stack43 AT LINE 394 is not found in our analysis.
Unit: $stack44 = new java.io.PrintStream AT LINE 396 is not found in our analysis.
Unit: specialinvoke $stack44.<java.io.PrintStream: void <init>(java.io.OutputStream,boolean,java.lang.String)>(i$#42, 0, "UTF-8") AT LINE 396 is not found in our analysis.
Unit: env#43 = $stack44 AT LINE 396 is not found in our analysis.
Unit: i$#31 = null AT LINE 396 is not found in our analysis.
Unit: virtualinvoke sb.<org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch$ShellScriptBuilder: void write(java.io.PrintStream)>(env#43) AT LINE 397 is not found in our analysis.
Unit: if env#43 == null goto $stack46 = <org.apache.hadoop.yarn.server.nodemanager.DockerContainerExecutor: org.slf4j.Logger LOG> AT LINE 398 is not found in our analysis.
Unit: if i$#31 == null goto virtualinvoke env#43.<java.io.PrintStream: void close()>() AT LINE 402 is not found in our analysis.
Unit: virtualinvoke env#43.<java.io.PrintStream: void close()>() AT LINE 402 is not found in our analysis.
Unit: goto [?= $stack46 = <org.apache.hadoop.yarn.server.nodemanager.DockerContainerExecutor: org.slf4j.Logger LOG>] AT LINE 402 is not found in our analysis.
Unit: virtualinvoke env#43.<java.io.PrintStream: void close()>() AT LINE 402 is not found in our analysis.
Unit: goto [?= $stack46 = <org.apache.hadoop.yarn.server.nodemanager.DockerContainerExecutor: org.slf4j.Logger LOG>] AT LINE 402 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>
Unit: goto [?= return numEdits] AT LINE 298 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void storeContainerLaunched(org.apache.hadoop.yarn.api.records.ContainerId)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void updateRMDelegationTokenState(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.WRITE3Response write(org.apache.hadoop.oncrpc.XDR,org.jboss.netty.channel.Channel,int,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$CheckedVolume: boolean isResourceAvailable()>
Start to analyze method: <org.apache.hadoop.yarn.security.NMTokenSelector: org.apache.hadoop.security.token.Token selectToken(org.apache.hadoop.io.Text,java.util.Collection)>
Start to analyze method: <org.apache.hadoop.mapred.LocalContainerLauncher: org.apache.hadoop.mapred.MapOutputFile renameMapOutputForReduce(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId,org.apache.hadoop.mapred.MapOutputFile)>
Start to analyze method: <org.apache.hadoop.nfs.NfsExports$ExactMatch: boolean isIncluded(java.lang.String,java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: float internalGetLabeledQueueCapacity(java.lang.String,java.lang.String,java.lang.String,float)>
Start to analyze method: <org.apache.hadoop.hdfs.util.MD5FileUtils: void saveMD5File(java.io.File,java.lang.String)>
Start to analyze method: <org.apache.hadoop.service.launcher.AbstractServiceLauncherTestBase: void assertLaunchOutcome(int,java.lang.String,java.lang.String[])>
Unit: arr$ = args AT LINE 158 is not found in our analysis.
Unit: len$ = lengthof arr$ AT LINE 158 is not found in our analysis.
Unit: i$ = 0 AT LINE 158 is not found in our analysis.
Unit: if i$ >= len$ goto staticinvoke <org.apache.hadoop.service.launcher.ServiceLauncher: void serviceMain(java.lang.String[])>(args) AT LINE 158 is not found in our analysis.
Unit: i$ = i$ + 1 AT LINE 158 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher: void putEntity(org.apache.hadoop.yarn.api.records.timeline.TimelineEntity)>
Start to analyze method: <org.apache.hadoop.hdfs.TestPipelines: byte[] writeData(org.apache.hadoop.fs.FSDataOutputStream,int)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMDistributedNodeLabelsHandler: void verifyRMHeartbeatResponseForNodeLabels(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse)>
Unit: goto [?= return] AT LINE 1213 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.ipc.Server$ConnectionManager: boolean close(org.apache.hadoop.ipc.Server$Connection)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: void run()>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: void internalReleaseResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.api.records.Resource)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp: boolean unprotectedDelete(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INode$ReclaimContext,long)>
Start to analyze method: <org.apache.hadoop.ipc.Client$Connection: void close()>
Start to analyze method: <org.apache.hadoop.oncrpc.RegistrationClient$RegistrationClientHandler: boolean validMessageLength(int)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd: org.apache.hadoop.oncrpc.XDR umntall(org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress)>
Start to analyze method: <org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot allocAndRegisterSlot(org.apache.hadoop.hdfs.ExtendedBlockId)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: void waitForDump()>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache: java.util.Map$Entry getEntryToEvict()>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void sendAckUpstreamUnprotected(org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck,long,long,long,int)>
Start to analyze method: <org.apache.hadoop.util.GenericOptionsParser: void processGeneralOptions(org.apache.commons.cli.CommandLine)>
Start to analyze method: <org.apache.hadoop.yarn.server.federation.policies.amrmproxy.LocalityMulticastAMRMProxyPolicy: java.util.Map splitResourceRequests(java.util.List)>
Start to analyze method: <org.apache.hadoop.yarn.util.ProcfsBasedProcessTree: void updateProcessTree()>
Start to analyze method: <org.apache.hadoop.yarn.util.ProcfsBasedProcessTree: void constructProcessSMAPInfo(org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$ProcessTreeSmapMemInfo,java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void loadRMDelegationKeyState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher: void putEntity(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.api.records.ApplicationId)>
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processFirstBlockReport(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.BlockListAsLongs)>
Start to analyze method: <org.apache.hadoop.security.UserGroupInformation: void logoutUserFromKeytab()>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer allocate(org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.PendingAsk,org.apache.hadoop.yarn.api.records.Container)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void removeApplication(org.apache.hadoop.yarn.api.records.ApplicationId)>
Start to analyze method: <org.apache.hadoop.service.CompositeService: void serviceStop()>
Start to analyze method: <org.apache.hadoop.oncrpc.SimpleTcpClientHandler: void channelConnected(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelStateEvent)>
Start to analyze method: <org.apache.hadoop.registry.client.impl.zk.RegistrySecurity: boolean addDigestACL(org.apache.zookeeper.data.ACL)>
Start to analyze method: <org.apache.hadoop.yarn.server.security.BaseNMTokenSecretManager: byte[] retrivePasswordInternal(org.apache.hadoop.yarn.security.NMTokenIdentifier,org.apache.hadoop.yarn.server.security.MasterKeyData)>
Start to analyze method: <org.apache.hadoop.portmap.RpcProgramPortmap: org.apache.hadoop.oncrpc.XDR getport(int,org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.XDR)>
Unit: goto [?= $stack19 = staticinvoke <org.apache.hadoop.portmap.PortmapResponse: org.apache.hadoop.oncrpc.XDR intReply(org.apache.hadoop.oncrpc.XDR,int,int)>(out, xid, res)] AT LINE 135 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor: java.lang.String[] getRunCommand(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor: org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor$DockerContainerStatus getContainerStatus(java.lang.String,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor)>
Start to analyze method: <org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback: void <init>()>
Start to analyze method: <org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet: boolean isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue: void addChildQueue(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue)>
Start to analyze method: <org.apache.hadoop.mapreduce.lib.input.FileInputFormat: java.util.List listStatus(org.apache.hadoop.mapreduce.JobContext)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue: void updateDemand()>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService: void writeAsync(org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx)>
Start to analyze method: <org.apache.hadoop.ipc.ClientCache: void stopClient(org.apache.hadoop.ipc.Client)>
Start to analyze method: <org.apache.hadoop.hdfs.web.WebHdfsFileSystem$ReadRunner: java.io.InputStream initializeInputStream(java.net.HttpURLConnection)>
Start to analyze method: <org.apache.hadoop.mapreduce.task.reduce.Fetcher: org.apache.hadoop.mapreduce.TaskAttemptID[] copyMapOutput(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.io.DataInputStream,java.util.Set,boolean)>
Start to analyze method: <org.apache.hadoop.io.compress.zlib.ZlibCompressor: void reinit(org.apache.hadoop.conf.Configuration)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: void handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.PATHCONF3Response pathconf(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: byte[] retrievePassword(org.apache.hadoop.yarn.security.AMRMTokenIdentifier)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage: org.apache.hadoop.mapreduce.v2.app.job.Job loadJob(org.apache.hadoop.mapreduce.v2.api.records.JobId)>
Start to analyze method: <org.apache.hadoop.metrics2.impl.MetricsConfig: java.lang.ClassLoader getPluginLoader()>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.DataXceiver: void checkAccess(java.io.OutputStream,boolean,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.datatransfer.Op,org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier$AccessMode)>
Start to analyze method: <org.apache.hadoop.mapreduce.CryptoUtils: java.io.InputStream wrapIfNecessary(org.apache.hadoop.conf.Configuration,java.io.InputStream,long)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: void handleStoreEvent(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent)>
Start to analyze method: <org.apache.hadoop.security.SaslRpcClient$SaslClientCallbackHandler: void handle(javax.security.auth.callback.Callback[])>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: boolean checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.QueueACL,java.lang.String)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testRURReplicas()>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.FSSTAT3Response fsstat(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
Start to analyze method: <org.apache.hadoop.mapred.JobACLsManager: boolean checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.mapreduce.JobACL,java.lang.String,org.apache.hadoop.security.authorize.AccessControlList)>
Start to analyze method: <org.apache.hadoop.ipc.Client: void stop()>
Start to analyze method: <org.apache.hadoop.security.LdapGroupsMapping: java.util.List doGetGroups(java.lang.String,int)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.FifoIntraQueuePreemptionPlugin: void computeAppsIdealAllocation(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TempQueuePerPartition,java.util.Map,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,float)>
Unit: $stack50 = virtualinvoke tq.<org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TempQueuePerPartition: java.util.Collection getApps()>() AT LINE 187 is not found in our analysis.
Unit: i$ = interfaceinvoke $stack50.<java.util.Collection: java.util.Iterator iterator()>() AT LINE 187 is not found in our analysis.
Unit: $stack52 = interfaceinvoke i$.<java.util.Iterator: boolean hasNext()>() AT LINE 187 is not found in our analysis.
Unit: if $stack52 == 0 goto return AT LINE 187 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirSymlinkOp: org.apache.hadoop.hdfs.server.namenode.INodeSymlink addSymlink(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,boolean)>
Start to analyze method: <org.apache.hadoop.util.LightWeightGSet: void <init>(int)>
Start to analyze method: <org.apache.hadoop.mapred.ShuffleHandler$Shuffle: org.apache.hadoop.mapred.ShuffleHandler$Shuffle$MapOutputInfo getMapOutputInfo(java.lang.String,int,java.lang.String,java.lang.String)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: void decResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testRWRReplicas()>
Start to analyze method: <org.apache.hadoop.metrics2.lib.MutableMetricsFactory: org.apache.hadoop.metrics2.lib.MutableMetric newForMethod(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.'annotation'.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry)>
Unit: tmp$1415606880 = new java.lang.StringBuilder AT LINE 93 is not found in our analysis.
Unit: specialinvoke tmp$1415606880.<java.lang.StringBuilder: void <init>()>() AT LINE 93 is not found in our analysis.
Unit: $stack20 = virtualinvoke tmp$1415606880.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("method ") AT LINE 93 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor: void copyFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation,com.sun.jersey.api.client.Client,java.net.URI)>
Unit: $stack72 = this.<org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter: java.util.Set summaryEntityTypes> AT LINE 188 is not found in our analysis.
Unit: if $stack72 == null goto $stack73 = <org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter: org.apache.commons.logging.Log LOG> AT LINE 188 is not found in our analysis.
Unit: $stack75 = this.<org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter: java.util.Set summaryEntityTypes> AT LINE 195 is not found in our analysis.
Unit: $stack76 = interfaceinvoke $stack75.<java.util.Set: boolean isEmpty()>() AT LINE 195 is not found in our analysis.
Unit: if $stack76 != 0 goto $stack73 = <org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter: org.apache.commons.logging.Log LOG> AT LINE 195 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat$OneFileInfo: void <init>(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.conf.Configuration,boolean,java.util.HashMap,java.util.HashMap,java.util.HashMap,java.util.HashMap,long)>
Start to analyze method: <org.apache.hadoop.ipc.Client$Connection: void receiveRpcResponse()>
Start to analyze method: <org.apache.hadoop.service.CompositeService: void serviceInit(org.apache.hadoop.conf.Configuration)>
Start to analyze method: <org.apache.hadoop.yarn.server.timeline.security.TimelineACLsManager: boolean checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.timeline.TimelineDomain)>
Unit: if callerUGI != null goto $stack29 = virtualinvoke callerUGI.<org.apache.hadoop.security.UserGroupInformation: java.lang.String getShortUserName()>() AT LINE 152 is not found in our analysis.
Unit: goto [?= $stack30 = virtualinvoke $stack28.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($stack29)] AT LINE 157 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void storeContainerUpdateToken(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.security.ContainerTokenIdentifier)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp: org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp$RenameResult unprotectedRenameTo(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INodesInPath,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[])>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: boolean canAssignToUser(org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits)>
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void queueReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.lang.String)>
Start to analyze method: <org.apache.hadoop.oncrpc.RpcProgram: boolean doPortMonitoring(java.net.SocketAddress)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils: org.apache.hadoop.security.UserGroupInformation verifyAdminAccess(org.apache.hadoop.yarn.security.YarnAuthorizationProvider,java.lang.String,java.lang.String,org.apache.commons.logging.Log)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl: void unreference()>
Unit: $stack7 = this.<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl: org.apache.hadoop.util.CloseableReferenceCount reference> AT LINE 220 is not found in our analysis.
Unit: $stack8 = virtualinvoke $stack7.<org.apache.hadoop.util.CloseableReferenceCount: int getReferenceCount()>() AT LINE 220 is not found in our analysis.
Unit: if $stack8 > 0 goto specialinvoke this.<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl: void checkReference()>() AT LINE 220 is not found in our analysis.
Unit: specialinvoke this.<org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl: void printReferenceTraceInfo(java.lang.String)>("desc") AT LINE 217 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: void handleContainerStatus(java.util.List)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.Allocation allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.util.List,java.util.List,java.util.List,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates)>
Unit: virtualinvoke application.<org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: void showRequests()>() AT LINE 991 is not found in our analysis.
Unit: virtualinvoke application.<org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: void showRequests()>() AT LINE 1001 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask: boolean moveFiles()>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx$COMMIT_STATUS handleSpecialWait(boolean,long,org.jboss.netty.channel.Channel,int,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void selectInputStreams(java.util.Collection,long,boolean)>
Unit: if inProgressOk == 0 goto $stack28 = " (excluding inProgress) " AT LINE 343 is not found in our analysis.
Unit: goto [?= $stack17 = virtualinvoke $stack16.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($stack28)] AT LINE 349 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: void deactivateApplication(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker: java.util.Collection getVolumesLowOnSpace()>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController: int getClassIdFromFileContents(java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager: void updateStoredToken(org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier,long)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void updateApplicationStateInternal(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData)>
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand[] handleHeartbeat(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,org.apache.hadoop.hdfs.server.protocol.StorageReport[],java.lang.String,long,long,int,int,int,org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary,org.apache.hadoop.hdfs.server.protocol.SlowPeerReports,org.apache.hadoop.hdfs.server.protocol.SlowDiskReports)>
Start to analyze method: <org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor: void loadNodeChildren(org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$Node,java.lang.String,java.lang.String[])>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMDelegatedNodeLabelsUpdater: org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsMappingProvider createRMNodeLabelsMappingProvider(org.apache.hadoop.conf.Configuration)>
Start to analyze method: <org.apache.hadoop.io.file.tfile.Compression$Algorithm: void returnCompressor(org.apache.hadoop.io.compress.Compressor)>
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.BackupImage: void setState(org.apache.hadoop.hdfs.server.namenode.BackupImage$BNState)>
Start to analyze method: <org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor: void beforeExecute(java.lang.Thread,java.lang.Runnable)>
Start to analyze method: <org.apache.hadoop.io.SequenceFile$Sorter$SortPass: int run(boolean)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: void updateNodeLabelsFromNMReport(java.util.Set,org.apache.hadoop.yarn.api.records.NodeId)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.io.File validateBlockFile(java.lang.String,long)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: org.apache.hadoop.yarn.api.records.Resource getMaximumAllocationPerQueue(java.lang.String)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache: void cleanAll()>
Start to analyze method: <org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy: org.apache.hadoop.io.retry.RetryPolicy$RetryAction shouldRetry(java.lang.Exception,int,int,boolean)>
Start to analyze method: <org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1: void run()>
Unit: goto [?= return] AT LINE 315 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractSchedulerPlanFollower: void synchronizePlan(org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,boolean)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.ContainerAllocation doAllocation(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.ContainerAllocation,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyTokenSecretManager: byte[] retrievePassword(org.apache.hadoop.yarn.security.AMRMTokenIdentifier)>
Start to analyze method: <org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods: java.net.URI redirectURI(org.apache.hadoop.hdfs.server.federation.router.Router,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,long,java.lang.String,org.apache.hadoop.hdfs.web.resources.Param[])>
Start to analyze method: <org.apache.hadoop.ipc.Server$Connection: void authorizeConnection()>
Start to analyze method: <org.apache.hadoop.io.TestArrayFile: void writeTest(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.RandomDatum[],java.lang.String)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.LOOKUP3Response lookup(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void removeReservationState(java.lang.String,java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void cleanupKeysWithPrefix(java.lang.String)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.MKDIR3Response mkdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer: void <init>(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,java.lang.String)>
Unit: if targetStorageTypes != null goto $stack38 = staticinvoke <java.util.Arrays: java.util.List asList(java.lang.Object[])>(targetStorageTypes) AT LINE 2378 is not found in our analysis.
Unit: goto [?= $stack39 = virtualinvoke $stack37.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>($stack38)] AT LINE 2395 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor: java.lang.String[] getPrivilegedOperationExecutionCommand(java.util.List,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation)>
Start to analyze method: <org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair doSaslHandshake(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,java.util.Map,javax.security.auth.callback.CallbackHandler)>
Unit: if cipherOption != null goto $stack41 = <org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.slf4j.Logger LOG> AT LINE 387 is not found in our analysis.
Unit: cipherSuites = virtualinvoke conf.<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String)>("dfs.encrypt.data.transfer.cipher.suites") AT LINE 389 is not found in our analysis.
Unit: if cipherSuites == null goto (branch) AT LINE 391 is not found in our analysis.
Unit: $stack47 = virtualinvoke cipherSuites.<java.lang.String: boolean isEmpty()>() AT LINE 411 is not found in our analysis.
Unit: if $stack47 != 0 goto (branch) AT LINE 411 is not found in our analysis.
Unit: goto [?= $stack26 = staticinvoke <org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: org.apache.hadoop.crypto.CipherOption wrap(org.apache.hadoop.crypto.CipherOption,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant)>(cipherOption, sasl)] AT LINE 397 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.util.ApplicationClassLoader: java.net.URL getResource(java.lang.String)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx$Dumper: void run()>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: void nodeUpdate(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode)>
Unit: if schedulerNode != null goto $stack23 = virtualinvoke schedulerNode.<org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: org.apache.hadoop.yarn.api.records.Resource getUnallocatedResource()>() AT LINE 1118 is not found in our analysis.
Unit: goto [?= $stack24 = virtualinvoke $stack22.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>($stack23)] AT LINE 1123 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl$ResourceRequestInfo remove(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,org.apache.hadoop.yarn.api.records.Resource)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSImage: boolean recoverTransitionRead(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)>
Unit: $stack116 = staticinvoke <com.google.common.base.Joiner: com.google.common.base.Joiner on(java.lang.String)>("\n  ") AT LINE 229 is not found in our analysis.
Unit: $stack117 = virtualinvoke $stack116.<com.google.common.base.Joiner: com.google.common.base.Joiner$MapJoiner withKeyValueSeparator(java.lang.String)>(": ") AT LINE 229 is not found in our analysis.
Unit: $stack118 = virtualinvoke $stack117.<com.google.common.base.Joiner$MapJoiner: java.lang.String join(java.util.Map)>(dataDirStates) AT LINE 229 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.ha.ActiveStandbyElector: void createConnection()>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: java.util.List getContainerStatuses()>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl: void updateCGroupParam(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController,java.lang.String,java.lang.String,java.lang.String)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache: void scan(long)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppAttemptTransition: org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMStateStoreState transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent)>
Start to analyze method: <org.apache.hadoop.yarn.webapp.GenericExceptionHandler: javax.ws.rs.core.Response toResponse(java.lang.Exception)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$1: void updateTimelineCollectorData(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: void removeReservationState(java.lang.String,java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: void recoverTrackerResources(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTracker,org.apache.hadoop.yarn.server.nodemanager.recovery.NMStateStoreService$LocalResourceTrackerState)>
Start to analyze method: <org.apache.hadoop.mapred.CleanupQueue: boolean deletePath(org.apache.hadoop.mapred.CleanupQueue$PathDeletionContext)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.Nfs3Utils: void writeChannel(org.jboss.netty.channel.Channel,org.apache.hadoop.oncrpc.XDR,int)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void removeContainerPaused(org.apache.hadoop.yarn.api.records.ContainerId)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler: void updateCgroup(java.lang.String,java.lang.String,java.lang.String,java.lang.String)>
Start to analyze method: <org.apache.hadoop.nfs.NfsExports$CIDRMatch: boolean isIncluded(java.lang.String,java.lang.String)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync: void enqueueEdit(org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync$Edit)>
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter: org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse putEntities(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.records.timeline.TimelineEntityGroupId,org.apache.hadoop.yarn.api.records.timeline.TimelineEntity[])>
Start to analyze method: <org.apache.hadoop.ipc.Server$RpcCall: java.lang.Void run()>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController: java.util.Map readStats()>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.JobHistory: java.util.Map getAllJobs(org.apache.hadoop.yarn.api.records.ApplicationId)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods: void init(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.HttpOpParam,org.apache.hadoop.hdfs.web.resources.Param[])>
Start to analyze method: <org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue: org.apache.hadoop.io.SequenceFile$Sorter$RawKeyValueIterator merge()>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.COMMIT3Response commit(org.apache.hadoop.oncrpc.XDR,org.jboss.netty.channel.Channel,int,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: void doSingleWrite(org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSNamesystem: void startActiveServices()>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FileJournalManager: void addStreamsToCollectionFromFiles(java.util.Collection,java.util.Collection,long,long,boolean)>
Start to analyze method: <org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor: void processXml()>
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: long removeLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: java.util.List chooseReplicasToDelete(java.util.Collection,int,java.util.List,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
Unit: goto [?= return excessReplicas] AT LINE 1125 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void <init>(org.apache.hadoop.conf.Configuration)>
Start to analyze method: <org.apache.hadoop.mapred.ShuffleHandler$Shuffle$3: void onRemoval(com.google.common.cache.RemovalNotification)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager: void removeStoredToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier)>
Start to analyze method: <org.apache.hadoop.yarn.util.FSDownload: void changePermissions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: org.apache.hadoop.hdfs.server.datanode.BlockReceiver$Packet waitForAckHead(long)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void loadRMAppStateFromAppNode(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState,java.lang.String,java.lang.String)>
Start to analyze method: <org.apache.hadoop.registry.cli.RegistryCli: java.lang.String analyzeException(java.lang.String,java.lang.Exception,java.util.List)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.QueuePriorityContainerCandidateSelector: java.util.Map selectCandidates(java.util.Map,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler: boolean deleteCgroup(java.lang.String)>
Start to analyze method: <org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair getEncryptedStreams(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream)>
Start to analyze method: <org.apache.hadoop.io.SequenceFile$Sorter: int sortPass(boolean)>
Start to analyze method: <org.apache.hadoop.net.unix.DomainSocketWatcher: void close()>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: boolean streamCleanup(long,long)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: void <init>(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.RMContext,org.apache.hadoop.yarn.server.resourcemanager.scheduler.YarnScheduler,org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService,org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp,org.apache.hadoop.yarn.server.resourcemanager.blacklist.BlacklistManager)>
Start to analyze method: <org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker: com.google.protobuf.Message invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])>
Start to analyze method: <org.apache.hadoop.ipc.Server: void wrapWithSasl(org.apache.hadoop.ipc.Server$RpcCall)>
Start to analyze method: <org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore: boolean deleteNextEntity(java.lang.String,byte[],org.apache.hadoop.yarn.server.utils.LeveldbIterator,org.apache.hadoop.yarn.server.utils.LeveldbIterator,boolean)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService: void storeTokenMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition: void setup(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl)>
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.SharedCacheClientImpl: void serviceStart()>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.PlacementSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)>
Start to analyze method: <org.apache.hadoop.registry.client.impl.zk.CuratorService: byte[] zkRead(java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue: org.apache.hadoop.yarn.api.records.Resource assignContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode)>
Unit: goto [?= return assigned] AT LINE 349 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: void recoverApplication(org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto)>
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$EntityLogFD: void writeEntities(java.util.List)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.DataXceiver: void <init>(org.apache.hadoop.hdfs.net.Peer,org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.datanode.DataXceiverServer)>
Start to analyze method: <org.apache.hadoop.hdfs.server.balancer.Dispatcher: boolean shouldIgnore(org.apache.hadoop.hdfs.protocol.DatanodeInfo)>
Start to analyze method: <org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair doSaslHandshake(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,java.lang.String,java.util.Map,javax.security.auth.callback.CallbackHandler)>
Unit: if cipherOption != null goto $stack38 = <org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: org.slf4j.Logger LOG> AT LINE 487 is not found in our analysis.
Unit: if cipherSuites == null goto (branch) AT LINE 489 is not found in our analysis.
Unit: $stack43 = virtualinvoke cipherSuites.<java.lang.String: boolean isEmpty()>() AT LINE 504 is not found in our analysis.
Unit: if $stack43 != 0 goto (branch) AT LINE 504 is not found in our analysis.
Unit: goto [?= (branch)] AT LINE 491 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.net.NetworkTopologyWithNodeGroup: void add(org.apache.hadoop.net.Node)>
Start to analyze method: <org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove: boolean markMovedIfGoodBlock(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock,org.apache.hadoop.fs.StorageType)>
Start to analyze method: <org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec: void setConf(org.apache.hadoop.conf.Configuration)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void addOrUpdateReservationState(org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto,java.lang.String,java.lang.String,org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction,boolean)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirConcatOp: org.apache.hadoop.fs.FileStatus concat(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,java.lang.String[],boolean)>
Start to analyze method: <org.apache.hadoop.io.TestArrayFile: org.apache.hadoop.io.RandomDatum[] generate(int)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler: void addApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,boolean,boolean)>
Unit: goto [?= return] AT LINE 436 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.WriteManager: int commitBeforeRead(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.FileHandle,long)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testFinalizedRbwReplicas()>
Start to analyze method: <org.apache.hadoop.io.retry.RetryUtils: org.apache.hadoop.io.retry.RetryPolicy getDefaultRetryPolicy(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String,java.lang.String)>
Start to analyze method: <org.apache.hadoop.mapred.CleanupQueue$PathCleanupThread: void run()>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: void storeApplicationAttemptStateInternal(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationAttemptStateData)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController: boolean checkIfAlreadyBootstrapped(java.lang.String)>
Unit: goto [?= (branch)] AT LINE 226 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.ipc.Client$Connection: void setupConnection(org.apache.hadoop.security.UserGroupInformation)>
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.net.Node chooseTarget(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet,boolean)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor: void startLocalizer(org.apache.hadoop.yarn.server.nodemanager.executor.LocalizerStartContext)>
Start to analyze method: <org.apache.hadoop.ipc.DecayRpcScheduler: void addResponseTime(java.lang.String,int,int,int)>
Start to analyze method: <org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore: long evictOldStartTimes(long)>
Start to analyze method: <org.apache.hadoop.nfs.NfsExports: void <init>(int,long,java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: void init()>
Unit: $stack22 = this.<org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandler resourceHandlerChain> AT LINE 309 is not found in our analysis.
Unit: if $stack22 == null goto $stack26 = 0 AT LINE 309 is not found in our analysis.
Unit: goto [?= $stack23 = virtualinvoke $stack21.<java.lang.StringBuilder: java.lang.StringBuilder append(boolean)>($stack26)] AT LINE 312 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.security.ContainerTokenSelector: org.apache.hadoop.security.token.Token selectToken(org.apache.hadoop.io.Text,java.util.Collection)>
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo chooseRandom(int,java.lang.String,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)>
Unit: if builder == null goto (branch) AT LINE 808 is not found in our analysis.
Unit: virtualinvoke builder.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("\n]") AT LINE 802 is not found in our analysis.
Unit: if builder == null goto $stack26 = <org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: java.lang.ThreadLocal CHOOSE_RANDOM_REASONS> AT LINE 829 is not found in our analysis.
Unit: includeType#42 = virtualinvoke builder.<java.lang.StringBuilder: java.lang.String toString()>() AT LINE 812 is not found in our analysis.
Unit: if badTarget == 0 goto $stack35 = virtualinvoke includeType#42.<java.lang.String: int length()>() AT LINE 813 is not found in our analysis.
Unit: virtualinvoke builder.<java.lang.StringBuilder: void setLength(int)>(0) AT LINE 814 is not found in our analysis.
Unit: goto [?= $stack26 = <org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: java.lang.ThreadLocal CHOOSE_RANDOM_REASONS>] AT LINE 814 is not found in our analysis.
Unit: $stack35 = virtualinvoke includeType#42.<java.lang.String: int length()>() AT LINE 816 is not found in our analysis.
Unit: if $stack35 <= 1 goto includeType#42 = "" AT LINE 816 is not found in our analysis.
Unit: includeType#42 = "" AT LINE 821 is not found in our analysis.
Unit: if builder == null goto i$#13 = null AT LINE 773 is not found in our analysis.
Unit: $stack93 = virtualinvoke builder.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("\nNode ") AT LINE 766 is not found in our analysis.
Unit: $stack94 = staticinvoke <org.apache.hadoop.net.NodeBase: java.lang.String getPath(org.apache.hadoop.net.Node)>(chosenNode) AT LINE 766 is not found in our analysis.
Unit: $stack95 = virtualinvoke $stack93.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($stack94) AT LINE 766 is not found in our analysis.
Unit: virtualinvoke $stack95.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>(" [") AT LINE 766 is not found in our analysis.
Unit: $stack105 = <org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault: java.lang.ThreadLocal debugLoggingBuilder> AT LINE 736 is not found in our analysis.
Unit: $stack106 = virtualinvoke $stack105.<java.lang.ThreadLocal: java.lang.Object get()>() AT LINE 736 is not found in our analysis.
Unit: builder = (java.lang.StringBuilder) $stack106 AT LINE 736 is not found in our analysis.
Unit: virtualinvoke builder.<java.lang.StringBuilder: void setLength(int)>(0) AT LINE 737 is not found in our analysis.
Unit: virtualinvoke builder.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("[") AT LINE 738 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.BlockReportTestBase: void blockReport_02()>
Start to analyze method: <org.apache.hadoop.hdfs.tools.GetGroups: void setConf(org.apache.hadoop.conf.Configuration)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: void handle(org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void checkRemoveParentZnode(java.lang.String,int)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: void containerCompleted(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType)>
Start to analyze method: <org.apache.hadoop.ipc.Client$Connection: void <init>(org.apache.hadoop.ipc.Client,org.apache.hadoop.ipc.Client$ConnectionId,int)>
Start to analyze method: <org.apache.hadoop.security.LdapGroupsMapping: void setConf(org.apache.hadoop.conf.Configuration)>
Start to analyze method: <org.apache.hadoop.registry.server.services.RegistryAdminService: void verifyRealmValidity()>
Start to analyze method: <org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore: org.apache.hadoop.yarn.server.federation.store.records.GetApplicationHomeSubClusterResponse getApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.GetApplicationHomeSubClusterRequest)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl addAttempt(org.apache.hadoop.mapreduce.v2.api.records.Avataar)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor: org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor$Registrations registerWithNewSubClusters(java.util.Set)>
Start to analyze method: <org.apache.hadoop.ha.ActiveStandbyElector: void processWatchEvent(org.apache.zookeeper.ZooKeeper,org.apache.zookeeper.WatchedEvent)>
Start to analyze method: <org.apache.hadoop.security.SaslRpcServer: javax.security.sasl.SaslServer create(org.apache.hadoop.ipc.Server$Connection,java.util.Map,org.apache.hadoop.security.token.SecretManager)>
Start to analyze method: <org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source: void dispatchBlocks(long)>
Start to analyze method: <org.apache.hadoop.io.TestArrayFile: void readTest(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.RandomDatum[],java.lang.String,org.apache.hadoop.conf.Configuration)>
Start to analyze method: <org.apache.hadoop.registry.client.impl.zk.RegistrySecurity: org.apache.zookeeper.data.ACL createACLForUser(org.apache.hadoop.security.UserGroupInformation,int)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue: boolean accept(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: org.apache.hadoop.yarn.api.records.Resource computeUserLimitAndSetHeadroom(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.api.records.Resource)>
Start to analyze method: <org.apache.hadoop.security.authentication.client.KerberosAuthenticator: void doSpnegoSequence(org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: void rename2(java.lang.String,java.lang.String,org.apache.hadoop.fs.Options$Rename[])>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten append(java.lang.String,org.apache.hadoop.hdfs.server.datanode.FinalizedReplica,long,long)>
Start to analyze method: <org.apache.hadoop.hdfs.util.LightWeightHashSet: void <init>(int,float,float)>
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: boolean checkLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,long,long)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.REMOVE3Response remove(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
Start to analyze method: <org.apache.hadoop.io.file.tfile.Compression$Algorithm: org.apache.hadoop.io.compress.Compressor getCompressor()>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService: void storeToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier,java.lang.Long)>
Start to analyze method: <org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore: org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterInfoResponse getSubCluster(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterInfoRequest)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: void handleInternal(org.jboss.netty.channel.ChannelHandlerContext,org.apache.hadoop.oncrpc.RpcInfo)>
Unit: $stack136 = <org.apache.hadoop.nfs.nfs3.Nfs3Constant$NFSPROC3: org.apache.hadoop.nfs.nfs3.Nfs3Constant$NFSPROC3 READ> AT LINE 2267 is not found in our analysis.
Unit: if nfsproc3 != $stack136 goto $stack134 = <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.hdfs.nfs.nfs3.Nfs3Metrics metrics> AT LINE 2267 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: org.apache.hadoop.yarn.api.records.Resource getUserAMResourceLimitPerPartition(java.lang.String,java.lang.String)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.FSINFO3Response fsinfo(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher: void publishLocalizationEvent(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEvent)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: void alterWriteRequest(org.apache.hadoop.nfs.nfs3.request.WRITE3Request,long)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue: void removeChildQueue(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.scheduler.DistributedScheduler: org.apache.hadoop.yarn.server.api.protocolrecords.DistributedSchedulingAllocateResponse allocateForDistributedScheduling(org.apache.hadoop.yarn.server.api.protocolrecords.DistributedSchedulingAllocateRequest)>
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable: org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl$ResourceRequestInfo decResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionTypeRequest,org.apache.hadoop.yarn.api.records.Resource,java.lang.Object)>
Start to analyze method: <org.apache.hadoop.mapred.ShuffleHandler$Shuffle$1: org.apache.hadoop.mapred.ShuffleHandler$AttemptPathInfo load(org.apache.hadoop.mapred.ShuffleHandler$AttemptPathIdentifier)>
Start to analyze method: <org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule: boolean logout()>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager: void storeNewMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor: org.apache.hadoop.yarn.server.api.protocolrecords.DistributedSchedulingAllocateResponse allocateForDistributedScheduling(org.apache.hadoop.yarn.server.api.protocolrecords.DistributedSchedulingAllocateRequest)>
Start to analyze method: <org.apache.hadoop.registry.client.impl.zk.CuratorService: org.apache.curator.framework.CuratorFramework createCurator()>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.DockerContainerExecutor: int launchContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.FileDeletionTask: void run()>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: boolean signalContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerSignalContext)>
Start to analyze method: <org.apache.hadoop.hdfs.net.DFSNetworkTopology: org.apache.hadoop.net.Node chooseRandomWithStorageTypeTwoTrial(java.lang.String,java.util.Collection,org.apache.hadoop.fs.StorageType)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService: void execute(java.lang.Runnable)>
Start to analyze method: <org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer: void checkOperation(org.apache.hadoop.hdfs.server.namenode.NameNode$OperationCategory)>
Start to analyze method: <org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater: void reinit(org.apache.hadoop.conf.Configuration)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand blockReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,org.apache.hadoop.hdfs.server.protocol.StorageBlockReport[],org.apache.hadoop.hdfs.server.protocol.BlockReportContext)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue: void updateDemand()>
Start to analyze method: <org.apache.hadoop.security.UserGroupInformation: void logUserInfo(org.slf4j.Logger,java.lang.String,org.apache.hadoop.security.UserGroupInformation)>
Unit: $stack11 = virtualinvoke ugi.<org.apache.hadoop.security.UserGroupInformation: java.util.Collection getTokens()>() AT LINE 1935 is not found in our analysis.
Unit: i$ = interfaceinvoke $stack11.<java.util.Collection: java.util.Iterator iterator()>() AT LINE 1935 is not found in our analysis.
Unit: $stack13 = interfaceinvoke i$.<java.util.Iterator: boolean hasNext()>() AT LINE 1935 is not found in our analysis.
Unit: if $stack13 == 0 goto return AT LINE 1935 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.utils.YarnServerSecurityUtils: org.apache.hadoop.security.Credentials parseCredentials(org.apache.hadoop.yarn.api.records.ContainerLaunchContext)>
Unit: $stack14 = virtualinvoke credentials.<org.apache.hadoop.security.Credentials: java.util.Collection getAllTokens()>() AT LINE 155 is not found in our analysis.
Unit: i$ = interfaceinvoke $stack14.<java.util.Collection: java.util.Iterator iterator()>() AT LINE 155 is not found in our analysis.
Unit: $stack16 = interfaceinvoke i$.<java.util.Iterator: boolean hasNext()>() AT LINE 155 is not found in our analysis.
Unit: if $stack16 == 0 goto return credentials AT LINE 155 is not found in our analysis.
Unit: $stack17 = interfaceinvoke i$.<java.util.Iterator: java.lang.Object next()>() AT LINE 161 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.BlockReceiver: int receivePacket()>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: org.apache.hadoop.yarn.api.records.Resource computeUserLimit(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,boolean)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd: org.apache.hadoop.oncrpc.XDR nullOp(org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: boolean reservationExceedsThreshold(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType)>
Start to analyze method: <org.apache.hadoop.ha.ActiveStandbyElector: void terminateConnection()>
Start to analyze method: <org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager: void removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp: org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp$FileState analyzeFileState(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,long,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.LocatedBlock[])>
Unit: goto [?= $stack25 = new org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp$FileState] AT LINE 557 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void registerSlot(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,boolean)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: void reserveResource(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)>
Unit: goto [?= virtualinvoke this.<org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode: void setReservedContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)>(container)] AT LINE 85 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime: org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation buildLaunchOp(org.apache.hadoop.yarn.server.nodemanager.containermanager.runtime.ContainerRuntimeContext,java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerRunCommand)>
Start to analyze method: <org.apache.hadoop.service.AbstractService: org.apache.hadoop.service.Service$STATE enterState(org.apache.hadoop.service.Service$STATE)>
Start to analyze method: <org.apache.hadoop.hdfs.DFSUtil: java.lang.String[] getSuffixIDs(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.DFSUtil$AddressMatcher)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl: void deleteCGroup(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController,java.lang.String)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.BlockReportTestBase: java.util.ArrayList locatedToBlocks(java.util.List,java.util.List)>
Unit: goto [?= i = i + 1] AT LINE 830 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void wipeDatanode(org.apache.hadoop.hdfs.protocol.DatanodeID)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo: void delete()>
Start to analyze method: <org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL: java.net.HttpURLConnection openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)>
Unit: $stack52 = virtualinvoke creds.<org.apache.hadoop.security.Credentials: java.util.Collection getAllTokens()>() AT LINE 299 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.timelineservice.RMTimelineCollectorManager: void doPostPut(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollector)>
Start to analyze method: <org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider: java.io.OutputStream getOutputStreamForKeystore()>
Start to analyze method: <org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void handleEvent(org.apache.hadoop.mapreduce.jobhistory.JobHistoryEvent)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt: void showRequests()>
Unit: $stack7 = this.<org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt: java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock readLock> AT LINE 622 is not found in our analysis.
Unit: virtualinvoke $stack7.<java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock: void lock()>() AT LINE 622 is not found in our analysis.
Unit: $stack8 = virtualinvoke this.<org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt: java.util.Collection getSchedulerKeys()>() AT LINE 623 is not found in our analysis.
Unit: i$ = interfaceinvoke $stack8.<java.util.Collection: java.util.Iterator iterator()>() AT LINE 623 is not found in our analysis.
Unit: $stack10 = interfaceinvoke i$.<java.util.Iterator: boolean hasNext()>() AT LINE 623 is not found in our analysis.
Unit: if $stack10 == 0 goto $stack11 = this.<org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt: java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock readLock> AT LINE 623 is not found in our analysis.
Unit: $stack12 = interfaceinvoke i$.<java.util.Iterator: java.lang.Object next()>() AT LINE 637 is not found in our analysis.
Unit: schedulerKey = (org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey) $stack12 AT LINE 637 is not found in our analysis.
Unit: ps = virtualinvoke this.<org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt: org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.SchedulingPlacementSet getSchedulingPlacementSet(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey)>(schedulerKey) AT LINE 624 is not found in our analysis.
Unit: if ps == null goto (branch) AT LINE 625 is not found in our analysis.
Unit: $stack15 = interfaceinvoke ps.<org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.SchedulingPlacementSet: int getOutstandingAsksCount(java.lang.String)>("*") AT LINE 632 is not found in our analysis.
Unit: if $stack15 <= 0 goto (branch) AT LINE 632 is not found in our analysis.
Unit: interfaceinvoke ps.<org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.SchedulingPlacementSet: void showRequests()>() AT LINE 630 is not found in our analysis.
Unit: $stack11 = this.<org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt: java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock readLock> AT LINE 634 is not found in our analysis.
Unit: virtualinvoke $stack11.<java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock: void unlock()>() AT LINE 634 is not found in our analysis.
Unit: goto [?= return] AT LINE 635 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: byte[] createPassword(org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier)>
Start to analyze method: <org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider: void flush()>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testNotMatchedReplicaID()>
Start to analyze method: <org.apache.hadoop.service.AbstractService: void noteFailure(java.lang.Exception)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: boolean checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.QueueACL,java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: void cleanupContainer()>
Unit: if result == 0 goto $stack75 = "failed" AT LINE 695 is not found in our analysis.
Unit: goto [?= $stack71 = virtualinvoke $stack70.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($stack75)] AT LINE 699 is not found in our analysis.
Unit: $stack102 = this.<org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: org.apache.hadoop.fs.Path pidFilePath> AT LINE 661 is not found in our analysis.
Unit: if $stack102 == null goto $stack108 = "null" AT LINE 661 is not found in our analysis.
Unit: goto [?= $stack103 = virtualinvoke $stack101.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($stack108)] AT LINE 672 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.ipc.DecayRpcScheduler: boolean shouldBackOff(org.apache.hadoop.ipc.Schedulable)>
Unit: i = 0 AT LINE 577 is not found in our analysis.
Unit: $stack31 = this.<org.apache.hadoop.ipc.DecayRpcScheduler: int numLevels> AT LINE 577 is not found in our analysis.
Unit: if i >= $stack31 goto responseTimes#4 = 0 AT LINE 577 is not found in our analysis.
Unit: i = i + 1 AT LINE 577 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirectory: byte[][] constructRemainingPath(byte[][],byte[][],int)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx: void trimWrite(int)>
Start to analyze method: <org.apache.hadoop.net.unix.DomainSocketWatcher$2: void run()>
Start to analyze method: <org.apache.hadoop.io.compress.CodecPool: org.apache.hadoop.io.compress.Decompressor getDecompressor(org.apache.hadoop.io.compress.CompressionCodec)>
Start to analyze method: <org.apache.hadoop.ipc.Server$ConnectionManager: org.apache.hadoop.ipc.Server$Connection register(java.nio.channels.SocketChannel)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor: java.lang.String executeDockerCommand(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommand,java.lang.String,java.util.Map,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor,boolean)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService$ApplicationEventHandler: void handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent)>
Unit: goto [?= return] AT LINE 760 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: org.apache.hadoop.yarn.api.records.Resource assignContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode,boolean)>
Start to analyze method: <org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: void serviceStop()>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.BlockSender: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,boolean,org.apache.hadoop.hdfs.server.datanode.DataNode,java.lang.String,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: void updateMetricsForAllocatedContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.api.records.Container)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService: void removeToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: long applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: void preallocate()>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSImage: void renameImageFileInDir(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,long,boolean)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.ContainerAllocation preCheckForPlacementSet(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.PlacementSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey)>
Start to analyze method: <org.apache.hadoop.hdfs.HAUtilClient: void cloneDelegationTokenForLogicalUri(org.apache.hadoop.security.UserGroupInformation,java.net.URI,java.util.Collection)>
Start to analyze method: <org.apache.hadoop.ha.ActiveStandbyElector: void becomeStandby()>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: void completedContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: void tryCommit(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator: boolean shouldAllocOrReserveNewContainer(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.Resource)>
Start to analyze method: <org.apache.hadoop.net.unix.DomainSocketWatcher: boolean sendCallback(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp: boolean unprotectedRemoveBlock(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INodeFile,org.apache.hadoop.hdfs.protocol.Block)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: void setattrInternal(org.apache.hadoop.hdfs.DFSClient,java.lang.String,org.apache.hadoop.nfs.nfs3.request.SetAttr3,boolean)>
Start to analyze method: <org.apache.hadoop.hdfs.server.common.Storage: void nativeCopyFileUnbuffered(java.io.File,java.io.File,boolean)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp: org.apache.hadoop.fs.FileEncryptionInfo getFileEncryptionInfo(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: void releaseResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx$Dumper: void dump()>
Start to analyze method: <org.apache.hadoop.security.SaslRpcClient$WrappedOutputStream: void write(byte[],int,int)>
Start to analyze method: <org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter: void outputINodes(java.io.InputStream)>
Unit: $stack38 = i % 100000 AT LINE 611 is not found in our analysis.
Unit: if $stack38 != 0 goto i = i + 1 AT LINE 611 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.ipc.Server$ConnectionManager$1: void run()>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.ActiveUsersManager: void deactivateApplication(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp: org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp$RenameResult renameTo(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INodesInPath,boolean)>
Unit: $stack28 = virtualinvoke dstIIP.<org.apache.hadoop.hdfs.server.namenode.INodesInPath: java.lang.String getPath()>() AT LINE 481 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.DataXceiver: void replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.fs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo)>
Start to analyze method: <org.apache.hadoop.hdfs.tools.DelegationTokenFetcher$1: java.lang.Object run()>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.AllocationBasedResourceUtilizationTracker: boolean hasResourcesAvailable(long,long,int)>
Start to analyze method: <org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$NMCallbackHandler: void onContainerStarted(org.apache.hadoop.yarn.api.records.ContainerId,java.util.Map)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline: org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams createStreams(boolean,org.apache.hadoop.util.DataChecksum)>
Start to analyze method: <org.apache.hadoop.mapred.FileInputFormat: org.apache.hadoop.fs.FileStatus[] listStatus(org.apache.hadoop.mapred.JobConf)>
Start to analyze method: <org.apache.hadoop.ipc.ClientCache: org.apache.hadoop.ipc.Client getClient(org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,java.lang.Class)>
Start to analyze method: <org.apache.hadoop.ipc.Server$Connection: void unwrapPacketAndProcessRpcs(byte[])>
Start to analyze method: <org.apache.hadoop.registry.client.impl.zk.RegistrySecurity: void initSecurity()>
Unit: goto [?= $stack44 = this.<org.apache.hadoop.registry.client.impl.zk.RegistrySecurity: java.util.List systemACLs>] AT LINE 288 is not found in our analysis.
Unit: goto [?= $stack44 = this.<org.apache.hadoop.registry.client.impl.zk.RegistrySecurity: java.util.List systemACLs>] AT LINE 267 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: void recover()>
Start to analyze method: <org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)>
Start to analyze method: <org.apache.hadoop.fs.RawLocalFileSystem: boolean rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.IntraQueueCandidatesSelector: void preemptFromLeastStarvedApp(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,java.util.Map,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,java.util.Map,java.util.Map)>
Unit: goto [?= return] AT LINE 260 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: boolean accept(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: org.apache.hadoop.yarn.api.records.Resource assignContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.PendingAsk,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType,boolean,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey)>
Start to analyze method: <org.apache.hadoop.registry.server.services.RegistryAdminService$AsyncPurge: java.lang.Integer call()>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.WriteManager: void handleWrite(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.request.WRITE3Request,org.jboss.netty.channel.Channel,int,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes)>
Start to analyze method: <org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void unregisterSlot(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId)>
Start to analyze method: <org.apache.hadoop.hdfs.client.impl.BlockReaderRemote2: int read(byte[],int,int)>
Unit: goto [?= randomId = $stack50] AT LINE 145 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.mapred.FileInputFormat: org.apache.hadoop.mapred.InputSplit[] getSplits(org.apache.hadoop.mapred.JobConf,int)>
Unit: $stack78 = virtualinvoke file.<org.apache.hadoop.fs.FileStatus: long getBlockSize()>() AT LINE 374 is not found in our analysis.
Unit: $stack79 = staticinvoke <java.lang.Math: long min(long,long)>($stack78, i$#6) AT LINE 374 is not found in our analysis.
Unit: $stack80 = length cmp $stack79 AT LINE 374 is not found in our analysis.
Unit: if $stack80 <= 0 goto blockSize#29 = specialinvoke this.<org.apache.hadoop.mapred.FileInputFormat: java.lang.String[][] getSplitHostsAndCachedHosts(org.apache.hadoop.fs.BlockLocation[],long,long,org.apache.hadoop.net.NetworkTopology)>(blkLocations, 0L, length, clusterMap) AT LINE 374 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp: org.apache.hadoop.hdfs.protocol.HdfsFileStatus startFile(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.fs.permission.PermissionStatus,java.lang.String,java.lang.String,java.util.EnumSet,boolean,short,long,org.apache.hadoop.fs.FileEncryptionInfo,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,boolean)>
Start to analyze method: <org.apache.hadoop.security.SecurityUtil: void setTokenServiceUseIp(boolean)>
Unit: tmp$528545547 = new java.lang.StringBuilder AT LINE 121 is not found in our analysis.
Unit: specialinvoke tmp$528545547.<java.lang.StringBuilder: void <init>()>() AT LINE 121 is not found in our analysis.
Unit: $stack8 = virtualinvoke tmp$528545547.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("Setting hadoop.security.token.service.use_ip to ") AT LINE 121 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl: org.apache.hadoop.mapreduce.task.reduce.MapHost getHost()>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.protocol.LocatedBlock getAdditionalDatanode(java.lang.String,long,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],java.lang.String[],org.apache.hadoop.hdfs.protocol.DatanodeInfo[],int,java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController: java.lang.String readState()>
Start to analyze method: <org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler: void handle(javax.security.auth.callback.Callback[])>
Unit: $stack15 = this.<org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler: org.apache.hadoop.security.token.SecretManager secretManager> AT LINE 325 is not found in our analysis.
Unit: $stack16 = staticinvoke <org.apache.hadoop.security.SaslRpcServer: org.apache.hadoop.security.token.TokenIdentifier getIdentifier(java.lang.String,org.apache.hadoop.security.token.SecretManager)>(len$#15, $stack15) AT LINE 325 is not found in our analysis.
Unit: i$#11 = virtualinvoke $stack16.<org.apache.hadoop.security.token.TokenIdentifier: org.apache.hadoop.security.UserGroupInformation getUser()>() AT LINE 325 is not found in our analysis.
Unit: if i$#11 != null goto $stack18 = virtualinvoke i$#11.<org.apache.hadoop.security.UserGroupInformation: java.lang.String getUserName()>() AT LINE 327 is not found in our analysis.
Unit: goto [?= callback#18 = $stack18] AT LINE 334 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType getAllowedLocalityLevelByTime(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,long,long,long)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner: org.apache.hadoop.security.Credentials getSystemCredentialsSentFromRM(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizerContext)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.FifoIntraQueuePreemptionPlugin: java.util.Map getResourceDemandFromAppsPerQueue(java.lang.String,java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void removeRMDelegationTokenState(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue: void setFairShare(org.apache.hadoop.yarn.api.records.Resource)>
Start to analyze method: <org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo: void shutDownTimer()>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: void markContainerForKillable(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)>
Start to analyze method: <org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$NameSectionProcessor: void process()>
Start to analyze method: <org.apache.hadoop.service.AbstractService: void stop()>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler: void addApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String,boolean)>
Unit: goto [?= return] AT LINE 404 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.client.impl.LeaseRenewer: void run(int)>
Unit: $stack21 = this.<org.apache.hadoop.hdfs.client.impl.LeaseRenewer: int currentId> AT LINE 446 is not found in our analysis.
Unit: if id == $stack21 goto $stack23 = <org.apache.hadoop.hdfs.client.impl.LeaseRenewer: org.slf4j.Logger LOG> AT LINE 446 is not found in our analysis.
Unit: goto [?= exitmonitor ie#11] AT LINE 447 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMDistributedNodeLabelsHandler: java.util.Set getNodeLabelsForHeartbeat()>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$PublicLocalizer: void addResource(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerResourceRequestEvent)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry: void removeShm(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: void addDirectoryToSerialNumberIndex(org.apache.hadoop.fs.Path)>
Start to analyze method: <org.apache.hadoop.registry.client.impl.zk.CuratorService: void zkUpdate(java.lang.String,byte[])>
Start to analyze method: <org.apache.hadoop.io.compress.bzip2.Bzip2Compressor: void reinit(org.apache.hadoop.conf.Configuration)>
Start to analyze method: <org.apache.hadoop.hdfs.web.HftpFileSystem: java.net.URL getNamenodeURL(java.lang.String,java.lang.String)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService: int loadTokenMasterKeys(org.apache.hadoop.mapreduce.v2.hs.HistoryServerStateStoreService$HistoryServerState)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue: boolean fitsInMaxShare(org.apache.hadoop.yarn.api.records.Resource)>
Start to analyze method: <org.apache.hadoop.io.SequenceFile$Reader: void getCurrentValue(org.apache.hadoop.io.Writable)>
Start to analyze method: <org.apache.hadoop.oncrpc.RpcProgram: void messageReceived(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.MessageEvent)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void removeApp(java.lang.String,boolean,java.util.Set)>
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: long requestLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
Unit: $stack38 = this.<org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData pendingHead> AT LINE 250 is not found in our analysis.
Unit: cur = $stack38.<org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData: org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData next> AT LINE 250 is not found in our analysis.
Unit: $stack39 = this.<org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData pendingHead> AT LINE 250 is not found in our analysis.
Unit: if cur == $stack39 goto $stack41 = <org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: org.slf4j.Logger LOG> AT LINE 250 is not found in our analysis.
Unit: cur = cur.<org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData: org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData next> AT LINE 251 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: void handle(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent)>
Start to analyze method: <org.apache.hadoop.ipc.WritableRpcEngine$Invoker: java.lang.Object invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: void completedContainerInternal(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager: void storeNewToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier,long)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.NNThroughputBenchmark$TinyDatanode: void sendHeartbeat()>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: boolean rename(java.lang.String,java.lang.String)>
Start to analyze method: <org.apache.hadoop.util.FileBasedIPList: java.lang.String[] readLines(java.lang.String)>
Start to analyze method: <org.apache.hadoop.io.retry.LossyRetryInvocationHandler: java.lang.Object invokeMethod(java.lang.reflect.Method,java.lang.Object[])>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler: void exceptionCaught(io.netty.channel.ChannelHandlerContext,java.lang.Throwable)>
Start to analyze method: <org.apache.hadoop.io.SequenceFile$Sorter: int mergePass(org.apache.hadoop.fs.Path)>
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.UnderReplicatedBlocks: void update(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int,int,int,int,int)>
Start to analyze method: <org.apache.hadoop.mapred.MapTask$MapOutputBuffer: void mergeParts()>
Start to analyze method: <org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader: org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader load(java.lang.String)>
Unit: $stack53 = virtualinvoke s.<org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section: java.lang.String getName()>() AT LINE 154 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.DataStreamer$LastExceptionInStreamer: void check(boolean)>
Unit: $stack7 = new java.lang.Throwable AT LINE 312 is not found in our analysis.
Unit: $stack8 = this.<org.apache.hadoop.hdfs.DataStreamer$LastExceptionInStreamer: java.io.IOException thrown> AT LINE 312 is not found in our analysis.
Unit: specialinvoke $stack7.<java.lang.Throwable: void <init>(java.lang.Throwable)>($stack8) AT LINE 312 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$UserLogDir: void scanIfNeeded(org.apache.hadoop.fs.FileStatus)>
Start to analyze method: <org.apache.hadoop.ipc.Server$Connection: org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto processSaslToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)>
Start to analyze method: <org.apache.hadoop.security.SaslRpcClient: void sendSaslMessage(java.io.OutputStream,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerAppUtils: boolean isPlaceBlacklisted(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.commons.logging.Log)>
Start to analyze method: <org.apache.hadoop.security.SaslRpcClient: javax.security.sasl.SaslClient createSaslClient(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)>
Unit: goto [?= token#3 = virtualinvoke method.<org.apache.hadoop.security.SaslRpcServer$AuthMethod: java.lang.String getMechanismName()>()] AT LINE 236 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData: org.apache.hadoop.yarn.api.ContainerManagementProtocol newProxy(org.apache.hadoop.yarn.ipc.YarnRPC,java.lang.String,org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.Token)>
Start to analyze method: <org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker: com.google.protobuf.Message getReturnMessage(java.lang.reflect.Method,org.apache.hadoop.ipc.RpcWritable$Buffer)>
Start to analyze method: <org.apache.hadoop.ipc.Server$Responder: boolean processResponse(java.util.LinkedList,boolean)>
Unit: goto [?= error = 0] AT LINE 1482 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void removeDatanode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager: void removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)>
Start to analyze method: <org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: javax.management.AttributeList getAttributes(java.lang.String[])>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender: void sendLifelineIfDue()>
Start to analyze method: <org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: void recoverUnfinalizedSegments()>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: boolean mkdirs(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,boolean)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: void releaseContainer(org.apache.hadoop.yarn.api.records.ContainerId,boolean)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy: void containerBasedPreemptOrKill(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,org.apache.hadoop.yarn.api.records.Resource)>
Unit: $stack66 = interfaceinvoke i$.<java.util.Map: java.util.Collection values()>() AT LINE 462 is not found in our analysis.
Unit: i$ = interfaceinvoke $stack66.<java.util.Collection: java.util.Iterator iterator()>() AT LINE 462 is not found in our analysis.
Unit: $stack68 = interfaceinvoke i$.<java.util.Iterator: boolean hasNext()>() AT LINE 462 is not found in our analysis.
Unit: if $stack68 == 0 goto $stack70 = <org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy: org.apache.commons.logging.Log LOG> AT LINE 462 is not found in our analysis.
Unit: $stack75 = interfaceinvoke i$.<java.util.Iterator: java.lang.Object next()>() AT LINE 469 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher: void putEntity(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.api.records.ApplicationId)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager: void updateActiveUsersResourceUsage(java.lang.String)>
Start to analyze method: <org.apache.hadoop.hdfs.server.balancer.NameNodeConnector: boolean shouldContinue(long)>
Unit: $stack26 = this.<org.apache.hadoop.hdfs.server.balancer.NameNodeConnector: int maxNotChangedIterations> AT LINE 206 is not found in our analysis.
Unit: if $stack26 < 0 goto $stack31 = "Infinite" AT LINE 206 is not found in our analysis.
Unit: goto [?= $stack27 = virtualinvoke $stack25.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>($stack31)] AT LINE 211 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: void handleLifeline(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,org.apache.hadoop.hdfs.server.protocol.StorageReport[],java.lang.String,long,long,int,int,int,org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary)>
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.TimelineWriter: com.sun.jersey.api.client.ClientResponse doPosting(java.lang.Object,java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: void signalContainer(org.apache.hadoop.yarn.api.records.SignalContainerCommand)>
Unit: $stack99 = this.<org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: org.apache.hadoop.fs.Path pidFilePath> AT LINE 751 is not found in our analysis.
Unit: if $stack99 == null goto $stack105 = "null" AT LINE 751 is not found in our analysis.
Unit: goto [?= $stack100 = virtualinvoke $stack98.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($stack105)] AT LINE 760 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.NFS3Response nullProcedure()>
Start to analyze method: <org.apache.hadoop.nfs.NfsExports$RegexMatch: boolean isIncluded(java.lang.String,java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment assignContainersToChildQueues(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.PlacementSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)>
Start to analyze method: <org.apache.hadoop.security.UserGroupInformation: void logPrivilegedAction(javax.security.auth.Subject,java.lang.Object)>
Unit: $stack6 = new java.lang.Throwable AT LINE 1919 is not found in our analysis.
Unit: specialinvoke $stack6.<java.lang.Throwable: void <init>()>() AT LINE 1919 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.registry.client.impl.zk.CuratorService: boolean zkMkPath(java.lang.String,org.apache.zookeeper.CreateMode,boolean,java.util.List)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ReservedContainerCandidatesSelector: org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ReservedContainerCandidatesSelector$NodeForPreemption getPreemptionCandidatesOnNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.api.records.Resource,java.util.Map,java.util.Map,org.apache.hadoop.yarn.api.records.Resource,boolean)>
Start to analyze method: <org.apache.hadoop.security.authentication.server.AuthenticationFilter: void doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: java.util.Map loadStartedResources(org.apache.hadoop.yarn.server.utils.LeveldbIterator,java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.TimelineWriter: com.sun.jersey.api.client.ClientResponse doPostingObject(java.lang.Object,java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager: void storeNewToken(org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier,long)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: void killReservedContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)>
Start to analyze method: <org.apache.hadoop.ipc.Client$Connection: void setupIOstreams(java.util.concurrent.atomic.AtomicBoolean)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter: boolean saveNextReplica()>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.DataNode: void checkReadAccess(org.apache.hadoop.hdfs.protocol.ExtendedBlock)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: java.io.File[] copyBlockFiles(java.io.File,java.io.File,java.io.File,java.io.File,boolean,int,org.apache.hadoop.conf.Configuration)>
Unit: if calculateChecksum == 0 goto $stack12 = <org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: org.slf4j.Logger LOG> AT LINE 967 is not found in our analysis.
Unit: goto [?= $stack10 = newarray (java.io.File)[2]] AT LINE 968 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor: void readVersion()>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.DeletionService: void delete(org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.DeletionTask)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: void scanIntermediateDirectory(org.apache.hadoop.fs.Path)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: java.lang.String getContainerPid(org.apache.hadoop.fs.Path)>
Unit: goto [?= return processId] AT LINE 940 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: void addApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String,boolean)>
Unit: goto [?= $stack73 = this.<org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock writeLock>] AT LINE 494 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.security.UserGroupInformation: void loginUserFromSubject(javax.security.auth.Subject)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.BlockReportTestBase: void waitForTempReplica(org.apache.hadoop.hdfs.protocol.Block,int)>
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: void addProxyToCache(java.lang.String,org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: void allocateResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)>
Start to analyze method: <org.apache.hadoop.ipc.Client$Connection: void handleConnectionFailure(int,java.io.IOException)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.PlacementSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)>
Unit: virtualinvoke this.<org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: void showRequests()>() AT LINE 862 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService: int loadTokens(org.apache.hadoop.mapreduce.v2.hs.HistoryServerStateStoreService$HistoryServerState)>
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$AttemptDirCache: org.apache.hadoop.fs.Path createApplicationDir(org.apache.hadoop.yarn.api.records.ApplicationId)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: void setChildQueues(java.util.Collection)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSNamesystem: org.apache.hadoop.hdfs.protocol.HdfsFileStatus startFileInt(java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,java.lang.String,java.lang.String,java.util.EnumSet,boolean,short,long,org.apache.hadoop.crypto.CryptoProtocolVersion[],boolean)>
Start to analyze method: <org.apache.hadoop.service.CompositeService: void serviceStart()>
Start to analyze method: <org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet: void proxyLink(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.net.URI,javax.servlet.http.Cookie,java.lang.String,org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet$HTTP)>
Start to analyze method: <org.apache.hadoop.security.token.Token$PrivateToken: void <init>(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)>
Start to analyze method: <org.apache.hadoop.security.SaslRpcServer$SaslGssCallbackHandler: void handle(javax.security.auth.callback.Callback[])>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService: void fenceOldActive(byte[])>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue: boolean assignContainerPreCheck(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: void storeRMDTMasterKeyState(org.apache.hadoop.security.token.delegation.DelegationKey)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.web.webhdfs.ExceptionHandler: io.netty.handler.codec.http.DefaultFullHttpResponse exceptionCaught(java.lang.Throwable)>
Start to analyze method: <org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter: void doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)>
Start to analyze method: <org.apache.hadoop.mapreduce.task.reduce.Fetcher: void copyFromHost(org.apache.hadoop.mapreduce.task.reduce.MapHost)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalCacheCleaner$LocalCacheCleanerStats handleCacheCleanup()>
Unit: goto [?= $stack17 = this.<org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: org.apache.hadoop.yarn.server.nodemanager.metrics.NodeManagerMetrics metrics>] AT LINE 538 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods: java.net.URI redirectURI(javax.ws.rs.core.Response$ResponseBuilder,org.apache.hadoop.hdfs.server.namenode.NameNode,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,long,long,java.lang.String,org.apache.hadoop.hdfs.web.resources.Param[])>
Start to analyze method: <org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: void mergePaths(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)>
Start to analyze method: <org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: void unref(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: java.util.List getResources()>
Unit: $stack51 = virtualinvoke headRoom.<org.apache.hadoop.yarn.api.records.Resource: boolean equals(java.lang.Object)>(e#7) AT LINE 855 is not found in our analysis.
Unit: if $stack51 != 0 goto $stack23 = <org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: org.apache.commons.logging.Log LOG> AT LINE 855 is not found in our analysis.
Unit: nmToken#18 = interfaceinvoke diagMsg#8.<java.util.List: java.util.Iterator iterator()>() AT LINE 853 is not found in our analysis.
Unit: $stack35 = interfaceinvoke nmToken#18.<java.util.Iterator: boolean hasNext()>() AT LINE 853 is not found in our analysis.
Unit: if $stack35 == 0 goto virtualinvoke this.<org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: void computeIgnoreBlacklisting()>() AT LINE 853 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: void processOverWrite(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.request.WRITE3Request,org.jboss.netty.channel.Channel,int,org.apache.hadoop.security.IdMappingServiceProvider)>
Start to analyze method: <org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule: boolean login()>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp: void persistNewBlock(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo: void moveToDone()>
Start to analyze method: <org.apache.hadoop.hdfs.TestPipelines: void pipeline_01()>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer: org.apache.hadoop.hdfs.server.protocol.DatanodeCommand cacheReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,java.util.List)>
Start to analyze method: <org.apache.hadoop.io.retry.RetryInvocationHandler: org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo handleException(java.lang.reflect.Method,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.io.retry.RetryInvocationHandler$Counters,long,java.lang.Exception)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp: boolean deleteAllowed(org.apache.hadoop.hdfs.server.namenode.INodesInPath)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.FifoIntraQueuePreemptionPlugin: java.util.PriorityQueue createTempAppForResCalculation(org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TempQueuePerPartition,java.util.Collection,org.apache.hadoop.yarn.api.records.Resource,java.util.Map)>
Start to analyze method: <org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore: org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse put(org.apache.hadoop.yarn.api.records.timeline.TimelineEntities)>
Start to analyze method: <org.apache.hadoop.metrics2.lib.MutableMetricsFactory: org.apache.hadoop.metrics2.lib.MutableMetric newForField(java.lang.reflect.Field,org.apache.hadoop.metrics2.'annotation'.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.FSDirSymlinkOp: org.apache.hadoop.fs.FileStatus createSymlinkInt(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,boolean)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void storeContainerCompleted(org.apache.hadoop.yarn.api.records.ContainerId,int)>
Start to analyze method: <org.apache.hadoop.ha.ActiveStandbyElector: void joinElection(byte[])>
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void removeBlocksAndUpdateSafemodeTotal(org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.NNThroughputBenchmark$TinyDatanode: boolean addBlock(org.apache.hadoop.hdfs.protocol.Block)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue: boolean canAssignToThisQueue(org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)>
Start to analyze method: <org.apache.hadoop.oncrpc.RpcCall: void <init>(int,org.apache.hadoop.oncrpc.RpcMessage$Type,int,int,int,int,org.apache.hadoop.oncrpc.security.Credentials,org.apache.hadoop.oncrpc.security.Verifier)>
Start to analyze method: <org.apache.hadoop.registry.client.impl.zk.CuratorService: void serviceInit(org.apache.hadoop.conf.Configuration)>
Start to analyze method: <org.apache.hadoop.hdfs.tools.DFSHAAdmin: org.apache.hadoop.conf.Configuration addSecurityConfiguration(org.apache.hadoop.conf.Configuration)>
Start to analyze method: <org.apache.hadoop.io.compress.CodecPool: org.apache.hadoop.io.compress.Compressor getCompressor(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl: void logLineFromTasksFile(java.io.File)>
Unit: $stack10 = new java.io.BufferedReader AT LINE 482 is not found in our analysis.
Unit: $stack11 = new java.io.InputStreamReader AT LINE 482 is not found in our analysis.
Unit: $stack12 = new java.io.FileInputStream AT LINE 482 is not found in our analysis.
Unit: $stack13 = new java.lang.StringBuilder AT LINE 482 is not found in our analysis.
Unit: specialinvoke $stack13.<java.lang.StringBuilder: void <init>()>() AT LINE 482 is not found in our analysis.
Unit: $stack14 = virtualinvoke $stack13.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.Object)>(cgf) AT LINE 482 is not found in our analysis.
Unit: $stack15 = virtualinvoke $stack14.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>("/tasks") AT LINE 482 is not found in our analysis.
Unit: $stack16 = virtualinvoke $stack15.<java.lang.StringBuilder: java.lang.String toString()>() AT LINE 482 is not found in our analysis.
Unit: specialinvoke $stack12.<java.io.FileInputStream: void <init>(java.lang.String)>($stack16) AT LINE 482 is not found in our analysis.
Unit: specialinvoke $stack11.<java.io.InputStreamReader: void <init>(java.io.InputStream,java.lang.String)>($stack12, "UTF-8") AT LINE 482 is not found in our analysis.
Unit: specialinvoke $stack10.<java.io.BufferedReader: void <init>(java.io.Reader)>($stack11) AT LINE 482 is not found in our analysis.
Unit: inl = $stack10 AT LINE 482 is not found in our analysis.
Unit: l4 = null AT LINE 482 is not found in our analysis.
Unit: str = virtualinvoke inl.<java.io.BufferedReader: java.lang.String readLine()>() AT LINE 485 is not found in our analysis.
Unit: if str == null goto (branch) AT LINE 486 is not found in our analysis.
Unit: if inl == null goto (branch) AT LINE 489 is not found in our analysis.
Unit: if l4 == null goto virtualinvoke inl.<java.io.BufferedReader: void close()>() AT LINE 493 is not found in our analysis.
Unit: virtualinvoke inl.<java.io.BufferedReader: void close()>() AT LINE 491 is not found in our analysis.
Unit: goto [?= (branch)] AT LINE 491 is not found in our analysis.
Unit: virtualinvoke inl.<java.io.BufferedReader: void close()>() AT LINE 493 is not found in our analysis.
Unit: goto [?= (branch)] AT LINE 493 is not found in our analysis.
Unit: goto [?= return] AT LINE 491 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: void addApplicationOnRecovery(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Priority)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.SYMLINK3Response symlink(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: void invalidate(java.lang.String,org.apache.hadoop.hdfs.protocol.Block[],boolean)>
Start to analyze method: <org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30: void emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)>
Start to analyze method: <org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder: org.apache.hadoop.io.FastByteComparisons$Comparer getBestComparer()>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: void receivedNewWriteInternal(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.request.WRITE3Request,org.jboss.netty.channel.Channel,int,org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService,org.apache.hadoop.security.IdMappingServiceProvider)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.NodeManager: void reregisterCollectors()>
All overhead in <org.apache.hadoop.security.UserGroupInformation: void logAllUserInfo(org.slf4j.Logger,org.apache.hadoop.security.UserGroupInformation)> are not found in our analysis!
Start to analyze method: <org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider: void initFileSystem(java.net.URI)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.TransferFsImage: void downloadEditsToStorage(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog,org.apache.hadoop.hdfs.server.namenode.NNStorage)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: boolean commonCheckContainerAllocation(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ContainerAllocationProposal,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.SchedulerContainer)>
Start to analyze method: <org.apache.hadoop.hdfs.qjournal.server.Journal: void journal(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,long,int,byte[])>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.BlockReportTestBase: void printStats()>
Start to analyze method: <org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker: org.apache.hadoop.io.Writable call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)>
Unit: if 1 == 0 goto $stack123 = "" AT LINE 522 is not found in our analysis.
Unit: goto [?= $stack107 = virtualinvoke $stack106.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($stack123)] AT LINE 531 is not found in our analysis.
Unit: if null == null goto $stack113 = <org.apache.hadoop.ipc.Server: org.slf4j.Logger LOG> AT LINE 526 is not found in our analysis.
Unit: if 0 == 0 goto $stack81 = "" AT LINE 522 is not found in our analysis.
Unit: goto [?= $stack65 = virtualinvoke $stack64.<java.lang.StringBuilder: java.lang.StringBuilder append(java.lang.String)>($stack81)] AT LINE 531 is not found in our analysis.
Unit: if null == null goto $stack71 = <org.apache.hadoop.ipc.Server: org.slf4j.Logger LOG> AT LINE 526 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks$PendingReplicationMonitor: void pendingReplicationCheck()>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: boolean moveReservation(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests: void addMap(org.apache.hadoop.mapreduce.v2.app.rm.ContainerRequestEvent)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.NodeManager: org.apache.hadoop.yarn.server.nodemanager.nodelabels.NodeLabelsProvider createNodeLabelsProvider(org.apache.hadoop.conf.Configuration)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: void removeApplicationAttemptInternal(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.BlockReceiver: void <init>(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.fs.StorageType,java.io.DataInputStream,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,long,long,long,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean,boolean)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.ACCESS3Response access(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx$COMMIT_STATUS checkCommitInternal(long,org.jboss.netty.channel.Channel,int,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes,boolean)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: void handleContainerExitCode(int,org.apache.hadoop.fs.Path)>
Start to analyze method: <org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl: void decResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionTypeRequest,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.client.api.AMRMClient$ContainerRequest)>
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.PendingReplicationBlocks: boolean decrement(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)>
Start to analyze method: <org.apache.hadoop.ipc.Server$Responder: void doRunLoop()>
Start to analyze method: <org.apache.hadoop.mapred.Task: void reportNextRecordRange(org.apache.hadoop.mapred.TaskUmbilicalProtocol,long)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.BackupImage: void journal(long,int,byte[])>
Start to analyze method: <org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: void processQueuedMessages(java.lang.Iterable)>
Start to analyze method: <org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$JobListCache: org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo addIfAbsent(org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo)>
Start to analyze method: <org.apache.hadoop.io.SequenceFile$Reader: java.lang.Object getCurrentValue(java.lang.Object)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx addWritesToCache(org.apache.hadoop.nfs.nfs3.request.WRITE3Request,org.jboss.netty.channel.Channel,int)>
Unit: goto [?= return writeCtx] AT LINE 604 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt: org.apache.hadoop.yarn.api.records.Resource assignContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode)>
Start to analyze method: <org.apache.hadoop.ipc.Server$Connection: void processOneRpc(java.nio.ByteBuffer)>
Start to analyze method: <org.apache.hadoop.hdfs.server.namenode.NameNode: void initializeGenericKeys(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: int loadRMDTSecretManagerKeys(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.NodesListManager: void printConfiguredHosts()>
Unit: return AT LINE 186 is not found in our analysis.
Unit: $stack17 = this.<org.apache.hadoop.yarn.server.resourcemanager.NodesListManager: org.apache.hadoop.util.HostsFileReader hostsReader> AT LINE 195 is not found in our analysis.
Unit: hostDetails = virtualinvoke $stack17.<org.apache.hadoop.util.HostsFileReader: org.apache.hadoop.util.HostsFileReader$HostDetails getHostDetails()>() AT LINE 195 is not found in our analysis.
Unit: $stack19 = virtualinvoke hostDetails.<org.apache.hadoop.util.HostsFileReader$HostDetails: java.util.Set getIncludedHosts()>() AT LINE 196 is not found in our analysis.
Unit: i$ = interfaceinvoke $stack19.<java.util.Set: java.util.Iterator iterator()>() AT LINE 196 is not found in our analysis.
Unit: $stack21 = interfaceinvoke i$.<java.util.Iterator: boolean hasNext()>() AT LINE 196 is not found in our analysis.
Unit: if $stack21 == 0 goto $stack22 = virtualinvoke hostDetails.<org.apache.hadoop.util.HostsFileReader$HostDetails: java.util.Set getExcludedHosts()>() AT LINE 196 is not found in our analysis.
Unit: $stack22 = virtualinvoke hostDetails.<org.apache.hadoop.util.HostsFileReader$HostDetails: java.util.Set getExcludedHosts()>() AT LINE 199 is not found in our analysis.
Unit: i$ = interfaceinvoke $stack22.<java.util.Set: java.util.Iterator iterator()>() AT LINE 199 is not found in our analysis.
Unit: $stack24 = interfaceinvoke i$.<java.util.Iterator: boolean hasNext()>() AT LINE 199 is not found in our analysis.
Unit: if $stack24 == 0 goto return AT LINE 199 is not found in our analysis.
Unit: return AT LINE 202 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: int demoteOldEvictableMmaped(long)>
Unit: if needMoreSpace == 0 goto $stack47 = "because it\'s too old" AT LINE 510 is not found in our analysis.
Unit: goto [?= rationale = $stack47] AT LINE 520 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.security.client.TimelineDelegationTokenSelector: org.apache.hadoop.security.token.Token selectToken(org.apache.hadoop.io.Text,java.util.Collection)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.BlockSender: long doSendBlock(java.io.DataOutputStream,java.io.OutputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler)>
Unit: goto [?= startTime = $stack90] AT LINE 784 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt: org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer allocate(org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.Container)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder: void run()>
Unit: goto [?= lastPacketInBlock = pkt.<org.apache.hadoop.hdfs.server.datanode.BlockReceiver$Packet: boolean lastPacketInBlock>] AT LINE 1421 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.security.BaseNMTokenSecretManager: byte[] createPassword(org.apache.hadoop.yarn.security.NMTokenIdentifier)>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport getAppResourceUsageReport(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)>
Start to analyze method: <org.apache.hadoop.yarn.client.api.async.AMRMClientAsync: void waitFor(com.google.common.base.Supplier,int,int)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.READLINK3Response readlink(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
Start to analyze method: <org.apache.hadoop.registry.server.services.MicroZookeeperService: void serviceStart()>
Unit: $stack37 = new java.io.StringWriter AT LINE 244 is not found in our analysis.
Unit: specialinvoke $stack37.<java.io.StringWriter: void <init>()>() AT LINE 244 is not found in our analysis.
Unit: sw = $stack37 AT LINE 244 is not found in our analysis.
Unit: $stack38 = new java.io.PrintWriter AT LINE 245 is not found in our analysis.
Unit: specialinvoke $stack38.<java.io.PrintWriter: void <init>(java.io.Writer)>(sw) AT LINE 245 is not found in our analysis.
Unit: pw = $stack38 AT LINE 245 is not found in our analysis.
Unit: virtualinvoke zkServer.<org.apache.zookeeper.server.ZooKeeperServer: void dumpConf(java.io.PrintWriter)>(pw) AT LINE 246 is not found in our analysis.
Unit: virtualinvoke pw.<java.io.PrintWriter: void flush()>() AT LINE 247 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testErrorReplicas()>
Start to analyze method: <org.apache.hadoop.security.UserGroupInformation: void reloginFromKeytab()>
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: void markContainerForNonKillable(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)>
Start to analyze method: <org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService: void storeContainer(org.apache.hadoop.yarn.api.records.ContainerId,int,long,org.apache.hadoop.yarn.api.protocolrecords.StartContainerRequest)>
Start to analyze method: <org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery: void testNoReplicaUnderRecovery()>
Start to analyze method: <org.apache.hadoop.service.CompositeService: void stop(int,boolean)>
Start to analyze method: <org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor: void beforeExecute(java.lang.Thread,java.lang.Runnable)>
Start to analyze method: <org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3: org.apache.hadoop.nfs.nfs3.response.GETATTR3Response getattr(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress)>
Start to analyze method: <org.apache.hadoop.io.nativeio.NativeIO$POSIX: java.lang.String getName(org.apache.hadoop.io.nativeio.NativeIO$POSIX$IdCache,int)>
Unit: $stack19 = <org.apache.hadoop.io.nativeio.NativeIO$POSIX$IdCache: org.apache.hadoop.io.nativeio.NativeIO$POSIX$IdCache USER> AT LINE 442 is not found in our analysis.
Unit: if domain != $stack19 goto $stack36 = "GroupName" AT LINE 442 is not found in our analysis.
Unit: goto [?= type = $stack36] AT LINE 449 is not found in our analysis.
Start to analyze method: <org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore: void removeRMDTMasterKeyState(org.apache.hadoop.security.token.delegation.DelegationKey)>
Start to analyze method: <org.apache.hadoop.hdfs.web.HftpFileSystem$2: org.apache.hadoop.security.token.Token run()>
Total units in benchmark: 12866
Matched units in our result: 12302