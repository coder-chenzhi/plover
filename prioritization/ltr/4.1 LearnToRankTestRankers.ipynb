{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankLibJar = r\"E:\\IDEAWorkspace\\plover\\prioritization\\ltr\\RankLib-2.15.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"Cassandra\"\n",
    "dataFolds = r\"E:\\IDEAWorkspace\\plover\\prioritization\\ltr\\\\\" + project + r\"Data\\folds5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = {\n",
    "    \"0\" : \"MART\", \n",
    "    \"1\" : \"RankNet\", \n",
    "    \"2\" : \"RankBoost\", \n",
    "    \"7\" : \"ListNet\", \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: MethodCalls\n",
    "# 2: PotentialMethodCall\n",
    "# 3: Instructions\n",
    "# 4: PotentialInstruction\n",
    "# 5: ExecutionFrequency\n",
    "# 6: MethodFrequency\n",
    "features = {\n",
    "#     \"1\" : \"MethodCalls\", \n",
    "#     \"2\" : \"PotentialMethodCall\",\n",
    "#     \"3\" : \"Instructions\", \n",
    "#     \"4\" : \"PotentialInstruction\",\n",
    "    \"5\" : \"ExecutionFrequency\",\n",
    "#     \"6\" : \"MethodFrequency\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutoff of metrics\n",
    "cutoffs = [\"5\", \"10\", \"15\", \"20\", \"25\", \"30\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics for test\n",
    "testMetrics = [\"P\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\"java\", \"-jar\", rankLibJar, \"-train\", \"train.data\", \"-test\", \"test.data\", \"-feature\", (\",\").join(features.keys()), \n",
    "       \"-ranker\", \"0\", \"-metric2t\", \"NDCG@\"+cutoffs[0], \"-metric2T\", \"P@\"+cutoffs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result = subprocess.check_output(cmd, timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = show_result.decode(encoding=\"utf-8\", errors=\"ignore\").splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_perf = float(result[-3].split()[-1])\n",
    "train_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perf = float(result[-1].split()[-1])\n",
    "test_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNames = os.listdir(dataFolds)\n",
    "\n",
    "for algo_id in algos:\n",
    "#     print(\"Use ranker \" + algos[algo_id] + \" for test.\")\n",
    "    \n",
    "    for metric in testMetrics:\n",
    "#         print(\"Use metric=\" + metric + \" for test.\")\n",
    "        \n",
    "        for cutoff in cutoffs:\n",
    "#             print(\"Use cutoff=\" + cutoff + \" for test.\")\n",
    "            \n",
    "            test_perfs = []\n",
    "            for testFileName in fileNames:\n",
    "#                 print(\"Use \" + testFileName + \" for test.\")\n",
    "                # write test file\n",
    "                with open(\"test.data\", \"w\") as outfile:\n",
    "                    with open(os.path.join(dataFolds, testFileName), \"r\") as testFile:\n",
    "                        outfile.write(testFile.read())\n",
    "\n",
    "                # write train file\n",
    "                with open(\"train.data\", \"w\") as outfile:\n",
    "                    for trainFileName in fileNames:\n",
    "                        if trainFileName != testFileName:\n",
    "                            with open(os.path.join(dataFolds, trainFileName), \"r\") as trainFile:\n",
    "                                outfile.write(trainFile.read())\n",
    "\n",
    "                try:\n",
    "                    # invoke \n",
    "                    cmd = [\"java\", \"-jar\", rankLibJar, \"-train\", \"train.data\", \"-test\", \"test.data\", \n",
    "                           \"-feature\", (\",\").join(features.keys()), \"-ranker\", algo_id, \n",
    "                           \"-metric2t\", \"NDCG@\"+cutoff, \"-metric2T\", metric+\"@\"+cutoff]\n",
    "#                            \"-save\", \"Models/\"+project+\"_\"+algos[algo_id]+\"_@\"+cutoff+\"_Test\"+testFileName]\n",
    "#                     print(\" \".join(cmd))\n",
    "                    show_result = subprocess.check_output(cmd)\n",
    "                    result = show_result.decode(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "                    train_perf = float(result[-3].split()[-1])\n",
    "                    test_perf = float(result[-1].split()[-1])\n",
    "                    test_perfs.append(test_perf)\n",
    "#                     print(result[-3])\n",
    "#                     print(result[-1])\n",
    "                except subprocess.CalledProcessError as call_err:\n",
    "                    print(call_err.output.decode(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "                    \n",
    "#             print(\"Average \" + metric+\"@\"+cutoff + \" for \" + algos[algo_id] + \" on test is \" + str(sum(test_perfs)/len(test_perfs)))\n",
    "            print(str(sum(test_perfs)/len(test_perfs)), end=\"\\t\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
